{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from reservoirpy.nodes import Reservoir, Ridge, ESN\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_SIZE = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 8\n",
    "DROPOUT = 0.1\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 30\n",
    "PATIENCE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    partOfData = 1\n",
    "    X_genres = pd.read_csv(\"../data/train/input_genres_tags_data.csv\")\n",
    "    X_instruments = pd.read_csv(\"../data/train/input_instruments_tags_data.csv\")\n",
    "    X_moods = pd.read_csv(\"../data/train/input_moods_tags_data.csv\")\n",
    "    X_genres_categories = pd.read_csv(\"../data/train/input_genres_categories_data.csv\")\n",
    "    X_instruments_categories = pd.read_csv(\n",
    "        \"../data/train/input_instruments_categories_data.csv\"\n",
    "    )\n",
    "    X_moods_categories = pd.read_csv(\"../data/train/input_moods_categories_data.csv\")\n",
    "\n",
    "    y_genres = pd.read_csv(\"../data/train/output_genres_tags_data.csv\")\n",
    "    y_instruments = pd.read_csv(\"../data/train/output_instruments_tags_data.csv\")\n",
    "    y_moods = pd.read_csv(\"../data/train/output_moods_tags_data.csv\")\n",
    "\n",
    "    # On peut garder seulement une partie des données\n",
    "    X_genres = X_genres[: int(partOfData * len(X_genres))]\n",
    "    X_instruments = X_instruments[: int(partOfData * len(X_instruments))]\n",
    "    X_moods = X_moods[: int(partOfData * len(X_moods))]\n",
    "    y_genres = y_genres[: int(partOfData * len(y_genres))]\n",
    "    y_instruments = y_instruments[: int(partOfData * len(y_instruments))]\n",
    "    y_moods = y_moods[: int(partOfData * len(y_moods))]\n",
    "    X_genres_categories = X_genres_categories[\n",
    "        : int(partOfData * len(X_genres_categories))\n",
    "    ]\n",
    "    X_instruments_categories = X_instruments_categories[\n",
    "        : int(partOfData * len(X_instruments_categories))\n",
    "    ]\n",
    "    X_moods_categories = X_moods_categories[: int(partOfData * len(X_moods_categories))]\n",
    "\n",
    "    return (\n",
    "        X_genres,\n",
    "        X_instruments,\n",
    "        X_moods,\n",
    "        X_genres_categories,\n",
    "        X_instruments_categories,\n",
    "        X_moods_categories,\n",
    "    ), (y_genres, y_instruments, y_moods)\n",
    "\n",
    "\n",
    "# Ensure the input data is in the correct format\n",
    "def reshape_input(X):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        return X.values.reshape(-1, 1, X.shape[1])  # Handles pandas DataFrame\n",
    "    elif isinstance(X, np.ndarray):\n",
    "        return X.reshape(-1, 1, X.shape[1])  # Handles numpy ndarray\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a pandas DataFrame or a numpy ndarray\")\n",
    "\n",
    "\n",
    "def format_predictions(predictions):\n",
    "    # Convert the list to a NumPy array\n",
    "    predictions_array = np.array(predictions)\n",
    "\n",
    "    # Reshape the array to 2-dimensional\n",
    "    predictions_reshaped = predictions_array.reshape(-1, predictions_array.shape[-1])\n",
    "\n",
    "    return predictions_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données\n",
    "(\n",
    "    (\n",
    "        X_genres,\n",
    "        X_instruments,\n",
    "        X_moods,\n",
    "        X_genres_categories,\n",
    "        X_instruments_categories,\n",
    "        X_moods_categories,\n",
    "    ),\n",
    "    (y_genres, y_instruments, y_moods),\n",
    ") = load_data()\n",
    "\n",
    "# Train-test split\n",
    "X_genres_train, X_genres_test, y_genres_train, y_genres_test = train_test_split(\n",
    "    X_genres, y_genres, test_size=0.2, random_state=42\n",
    ")\n",
    "X_instruments_train, X_instruments_test, y_instruments_train, y_instruments_test = (\n",
    "    train_test_split(X_instruments, y_instruments, test_size=0.2, random_state=42)\n",
    ")\n",
    "X_moods_train, X_moods_test, y_moods_train, y_moods_test = train_test_split(\n",
    "    X_moods, y_moods, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train-test split for categories\n",
    "X_genres_categories_train, X_genres_categories_test = train_test_split(\n",
    "    X_genres_categories, test_size=0.2, random_state=42\n",
    ")\n",
    "X_instruments_categories_train, X_instruments_categories_test = train_test_split(\n",
    "    X_instruments_categories, test_size=0.2, random_state=42\n",
    ")\n",
    "X_moods_categories_train, X_moods_categories_test = train_test_split(\n",
    "    X_moods_categories, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données\n",
    "X_genres_train = X_genres_train.drop(columns=[\"ChallengeID\"])\n",
    "X_instruments_train = X_instruments_train.drop(columns=[\"ChallengeID\"])\n",
    "X_moods_train = X_moods_train.drop(columns=[\"ChallengeID\"])\n",
    "y_genres_train = y_genres_train.drop(columns=[\"ChallengeID\"])\n",
    "y_instruments_train = y_instruments_train.drop(columns=[\"ChallengeID\"])\n",
    "y_moods_train = y_moods_train.drop(columns=[\"ChallengeID\"])\n",
    "X_genres_categories_train = X_genres_categories_train.drop(columns=[\"ChallengeID\"])\n",
    "X_instruments_categories_train = X_instruments_categories_train.drop(\n",
    "    columns=[\"ChallengeID\"]\n",
    ")\n",
    "X_moods_categories_train = X_moods_categories_train.drop(columns=[\"ChallengeID\"])\n",
    "\n",
    "X_genres_test = X_genres_test.drop(columns=[\"ChallengeID\"])\n",
    "X_instruments_test = X_instruments_test.drop(columns=[\"ChallengeID\"])\n",
    "X_moods_test = X_moods_test.drop(columns=[\"ChallengeID\"])\n",
    "y_genres_test = y_genres_test.drop(columns=[\"ChallengeID\"])\n",
    "y_instruments_test = y_instruments_test.drop(columns=[\"ChallengeID\"])\n",
    "y_moods_test = y_moods_test.drop(columns=[\"ChallengeID\"])\n",
    "X_genres_categories_test = X_genres_categories_test.drop(columns=[\"ChallengeID\"])\n",
    "X_instruments_categories_test = X_instruments_categories_test.drop(\n",
    "    columns=[\"ChallengeID\"]\n",
    ")\n",
    "X_moods_categories_test = X_moods_categories_test.drop(columns=[\"ChallengeID\"])\n",
    "\n",
    "\n",
    "X_train = np.concatenate(\n",
    "    [\n",
    "        X_genres_train,\n",
    "        X_instruments_train,\n",
    "        X_moods_train,\n",
    "        X_genres_categories_train,\n",
    "        X_instruments_categories_train,\n",
    "        X_moods_categories_train,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "X_test = np.concatenate(\n",
    "    [\n",
    "        X_genres_test,\n",
    "        X_instruments_test,\n",
    "        X_moods_test,\n",
    "        X_genres_categories_test,\n",
    "        X_instruments_categories_test,\n",
    "        X_moods_categories_test,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "y_train = np.concatenate([y_genres_train, y_instruments_train, y_moods_train], axis=1)\n",
    "y_test = np.concatenate([y_genres_test, y_instruments_test, y_moods_test], axis=1)\n",
    "\n",
    "# Convertir les données en tensors PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(DEVICE)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(DEVICE)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# Générer les données croisées pour les interactions\n",
    "X_genres_instruments_train = np.concatenate(\n",
    "    [X_genres_train, X_instruments_train], axis=1\n",
    ")\n",
    "X_genres_moods_train = np.concatenate([X_genres_train, X_moods_train], axis=1)\n",
    "X_instruments_moods_train = np.concatenate([X_instruments_train, X_moods_train], axis=1)\n",
    "y_genres_instruments_train = np.concatenate(\n",
    "    [y_genres_train, y_instruments_train], axis=1\n",
    ")\n",
    "y_genres_moods_train = np.concatenate([y_genres_train, y_moods_train], axis=1)\n",
    "y_instruments_moods_train = np.concatenate([y_instruments_train, y_moods_train], axis=1)\n",
    "\n",
    "X_genres_instruments_test = np.concatenate([X_genres_test, X_instruments_test], axis=1)\n",
    "X_genres_moods_test = np.concatenate([X_genres_test, X_moods_test], axis=1)\n",
    "X_instruments_moods_test = np.concatenate([X_instruments_test, X_moods_test], axis=1)\n",
    "y_genres_instruments_test = np.concatenate([y_genres_test, y_instruments_test], axis=1)\n",
    "y_genres_moods_test = np.concatenate([y_genres_test, y_moods_test], axis=1)\n",
    "y_instruments_moods_test = np.concatenate([y_instruments_test, y_moods_test], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all outputs (individual and cross) into a dictionary\n",
    "X_train_final = {\n",
    "    \"X_genres\": X_genres_train,\n",
    "    \"X_instruments\": X_instruments_train,\n",
    "    \"X_moods\": X_moods_train,\n",
    "    \"X_genres_categories\": X_genres_categories_train,\n",
    "    \"X_instruments_categories\": X_instruments_categories_train,\n",
    "    \"X_moods_categories\": X_moods_categories_train,\n",
    "}\n",
    "\n",
    "X_test_final = {\n",
    "    \"X_genres\": X_genres_test,\n",
    "    \"X_instruments\": X_instruments_test,\n",
    "    \"X_moods\": X_moods_test,\n",
    "    \"X_genres_categories\": X_genres_categories_test,\n",
    "    \"X_instruments_categories\": X_instruments_categories_test,\n",
    "    \"X_moods_categories\": X_moods_categories_test,\n",
    "}\n",
    "\n",
    "# Define input sizes for the embeddings\n",
    "input_size_dict = {\n",
    "    \"X_genres\": X_genres_train.shape[1],\n",
    "    \"X_instruments\": X_instruments_train.shape[1],\n",
    "    \"X_moods\": X_moods_train.shape[1],\n",
    "    \"X_genres_categories\": X_genres_categories_train.shape[1],\n",
    "    \"X_instruments_categories\": X_instruments_categories_train.shape[1],\n",
    "    \"X_moods_categories\": X_moods_categories_train.shape[1],\n",
    "}\n",
    "\n",
    "# Define the output size\n",
    "output_size_dict = {\n",
    "    \"y_genres\": y_genres_train.shape[1],\n",
    "    \"y_instruments\": y_instruments_train.shape[1],\n",
    "    \"y_moods\": y_moods_train.shape[1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_genres: 90\n",
      "X_instruments: 112\n",
      "X_moods: 46\n",
      "X_genres_categories: 18\n",
      "X_instruments_categories: 15\n",
      "X_moods_categories: 8\n",
      "y_genres: 90\n",
      "y_instruments: 112\n",
      "y_moods: 46\n"
     ]
    }
   ],
   "source": [
    "# Display the shapes of the inputs and outputs\n",
    "for key, value in input_size_dict.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "for key, value in output_size_dict.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# Split each array or dataframe individually\n",
    "X_genres_train_split, X_genres_val_split = train_test_split(\n",
    "    X_genres_train, test_size=VALIDATION_SPLIT, random_state=42\n",
    ")\n",
    "X_instruments_train_split, X_instruments_val_split = train_test_split(\n",
    "    X_instruments_train, test_size=VALIDATION_SPLIT, random_state=42\n",
    ")\n",
    "X_moods_train_split, X_moods_val_split = train_test_split(\n",
    "    X_moods_train, test_size=VALIDATION_SPLIT, random_state=42\n",
    ")\n",
    "y_train_split, y_val_split = train_test_split(\n",
    "    y_train_tensor.cpu().numpy(), test_size=VALIDATION_SPLIT, random_state=42\n",
    ")\n",
    "X_genres_categories_train_split, X_genres_categories_val_split = train_test_split(\n",
    "    X_genres_categories_train, test_size=VALIDATION_SPLIT, random_state=42\n",
    ")\n",
    "X_instruments_categories_train_split, X_instruments_categories_val_split = (\n",
    "    train_test_split(\n",
    "        X_instruments_categories_train, test_size=VALIDATION_SPLIT, random_state=42\n",
    "    )\n",
    ")\n",
    "X_moods_categories_train_split, X_moods_categories_val_split = train_test_split(\n",
    "    X_moods_categories_train, test_size=VALIDATION_SPLIT, random_state=42\n",
    ")\n",
    "\n",
    "# Combine back into dictionaries\n",
    "X_train_final_train = {\n",
    "    \"X_genres\": torch.tensor(X_genres_train_split.values, dtype=torch.float32).to(\n",
    "        DEVICE\n",
    "    ),\n",
    "    \"X_instruments\": torch.tensor(\n",
    "        X_instruments_train_split.values, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"X_moods\": torch.tensor(X_moods_train_split.values, dtype=torch.float32).to(DEVICE),\n",
    "    \"X_genres_categories\": torch.tensor(\n",
    "        X_genres_categories_train_split.values, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"X_instruments_categories\": torch.tensor(\n",
    "        X_instruments_categories_train_split.values, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"X_moods_categories\": torch.tensor(\n",
    "        X_moods_categories_train_split.values, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "}\n",
    "train_targets = torch.tensor(y_train_split, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "X_train_final_val = {\n",
    "    \"X_genres\": torch.tensor(X_genres_val_split.values, dtype=torch.float32).to(DEVICE),\n",
    "    \"X_instruments\": torch.tensor(\n",
    "        X_instruments_val_split.values, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"X_moods\": torch.tensor(X_moods_val_split.values, dtype=torch.float32).to(DEVICE),\n",
    "    \"X_genres_categories\": torch.tensor(\n",
    "        X_genres_categories_val_split.values, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"X_instruments_categories\": torch.tensor(\n",
    "        X_instruments_categories_val_split.values, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"X_moods_categories\": torch.tensor(\n",
    "        X_moods_categories_val_split.values, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "}\n",
    "val_targets = torch.tensor(y_val_split, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# Convert test data to tensors\n",
    "X_test_final = {\n",
    "    \"X_genres\": torch.tensor(X_genres_test.values, dtype=torch.float32).to(DEVICE),\n",
    "    \"X_instruments\": torch.tensor(X_instruments_test.values, dtype=torch.float32).to(\n",
    "        DEVICE\n",
    "    ),\n",
    "    \"X_moods\": torch.tensor(X_moods_test.values, dtype=torch.float32).to(DEVICE),\n",
    "    \"X_genres_categories\": torch.tensor(\n",
    "        X_genres_categories_test.values, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"X_instruments_categories\": torch.tensor(\n",
    "        X_instruments_categories_test.values, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"X_moods_categories\": torch.tensor(\n",
    "        X_moods_categories_test.values, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "}\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "\n",
    "# Dataset creation function\n",
    "def create_dataset(X_data, y_data):\n",
    "    return torch.utils.data.TensorDataset(\n",
    "        X_data[\"X_genres\"],\n",
    "        X_data[\"X_instruments\"],\n",
    "        X_data[\"X_moods\"],\n",
    "        X_data[\"X_genres_categories\"],\n",
    "        X_data[\"X_instruments_categories\"],\n",
    "        X_data[\"X_moods_categories\"],\n",
    "        y_data,\n",
    "    )\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = create_dataset(X_train_final_train, train_targets)\n",
    "val_dataset = create_dataset(X_train_final_val, val_targets)\n",
    "test_dataset = create_dataset(X_test_final, y_test_tensor)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "# Check loaders\n",
    "# for batch in train_loader:\n",
    "#     print(\"Train batch sample:\", batch)\n",
    "#     break\n",
    "\n",
    "# for batch in val_loader:\n",
    "#     print(\"Validation batch sample:\", batch)\n",
    "#     break\n",
    "\n",
    "# for batch in test_loader:\n",
    "#     print(\"Test batch sample:\", batch)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([70946, 90]), torch.Size([70946, 112]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.tensors[0].shape, train_dataset.tensors[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Random Forest: 100%|██████████| 10/10 [02:19<00:00, 13.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.0057, Validation F1 Score: 0.3151\n",
      "Test Accuracy: 0.0054, Test F1 Score: 0.3131\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Concaténer toutes les caractéristiques d'entrée et les prédictions des réservoirs de neurones\n",
    "def prepare_data_for_rf(X_train_final, y_train):\n",
    "    X_train_concat = np.concatenate(\n",
    "        [\n",
    "            X_train_final[\"X_genres\"].cpu().numpy(),\n",
    "            X_train_final[\"X_instruments\"].cpu().numpy(),\n",
    "            X_train_final[\"X_moods\"].cpu().numpy(),\n",
    "            X_train_final[\"X_genres_categories\"].cpu().numpy(),\n",
    "            X_train_final[\"X_instruments_categories\"].cpu().numpy(),\n",
    "            X_train_final[\"X_moods_categories\"].cpu().numpy(),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    return X_train_concat, y_train\n",
    "\n",
    "\n",
    "# Préparer les données pour le Random Forest\n",
    "X_train_rf, y_train_rf = prepare_data_for_rf(\n",
    "    X_train_final_train, train_targets.cpu().numpy()\n",
    ")\n",
    "X_val_rf, y_val_rf = prepare_data_for_rf(X_train_final_val, val_targets.cpu().numpy())\n",
    "X_test_rf, y_test_rf = prepare_data_for_rf(X_test_final, y_test_tensor.cpu().numpy())\n",
    "\n",
    "# Initialiser le Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=10, random_state=42, warm_start=True)\n",
    "\n",
    "# Entraîner le modèle avec une barre de progression\n",
    "for i in tqdm(range(rf_model.n_estimators), desc=\"Training Random Forest\"):\n",
    "    rf_model.n_estimators = i + 1\n",
    "    rf_model.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "# Évaluer le modèle sur l'ensemble de validation\n",
    "y_val_pred_rf = rf_model.predict(X_val_rf)\n",
    "val_accuracy = accuracy_score(y_val_rf, y_val_pred_rf)\n",
    "val_f1 = f1_score(y_val_rf, y_val_pred_rf, average=\"weighted\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# Évaluer le modèle sur l'ensemble de test\n",
    "y_test_pred_rf = rf_model.predict(X_test_rf)\n",
    "test_accuracy = accuracy_score(y_test_rf, y_test_pred_rf)\n",
    "test_f1 = f1_score(y_test_rf, y_test_pred_rf, average=\"weighted\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Réduire la dimensionnalité des données avec PCA\n",
    "pca = PCA(n_components=100)  # Ajustez le nombre de composants selon vos besoins\n",
    "X_train_pca = pca.fit_transform(X_train_rf)\n",
    "X_val_pca = pca.transform(X_val_rf)\n",
    "X_test_pca = pca.transform(X_test_rf)\n",
    "\n",
    "# # Initialiser le Random Forest Classifier\n",
    "# rf_model2 = RandomForestClassifier(n_estimators=100, random_state=42, warm_start=True)\n",
    "\n",
    "# # Entraîner le modèle avec une barre de progression\n",
    "# for i in tqdm(range(rf_model.n_estimators), desc=\"Training Random Forest\"):\n",
    "#     rf_model.n_estimators = i + 1\n",
    "#     rf_model.fit(X_train_pca, y_train_rf)\n",
    "\n",
    "# # Évaluer le modèle sur l'ensemble de validation\n",
    "# y_val_pred_rf = rf_model.predict(X_val_pca)\n",
    "# val_accuracy = accuracy_score(y_val_rf, y_val_pred_rf)\n",
    "# val_f1 = f1_score(y_val_rf, y_val_pred_rf, average='weighted')\n",
    "# print(f\"Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# # Évaluer le modèle sur l'ensemble de test\n",
    "# y_test_pred_rf = rf_model.predict(X_test_pca)\n",
    "# test_accuracy = accuracy_score(y_test_rf, y_test_pred_rf)\n",
    "# test_f1 = f1_score(y_test_rf, y_test_pred_rf, average='weighted')\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components to explain 95% of variance: 86\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialiser PCA\n",
    "pca = PCA()\n",
    "\n",
    "# Ajuster PCA sur les données d'entraînement\n",
    "pca.fit(X_train_rf)\n",
    "\n",
    "# Calculer la variance expliquée cumulée\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Tracer la variance expliquée cumulée\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    range(1, len(explained_variance) + 1),\n",
    "    explained_variance,\n",
    "    marker=\"o\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"Cumulative Explained Variance vs. Number of Components\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Choisir le nombre de composants pour expliquer 95% de la variance\n",
    "n_components = np.argmax(explained_variance >= 0.95) + 1\n",
    "print(f\"Number of components to explain 95% of variance: {n_components}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Concaténer toutes les caractéristiques d'entrée et les prédictions des réservoirs de neurones\n",
    "def prepare_data_for_rf(X_train_final, y_train):\n",
    "    X_train_concat = np.concatenate(\n",
    "        [\n",
    "            X_train_final[\"X_genres\"].cpu().numpy(),\n",
    "            X_train_final[\"X_instruments\"].cpu().numpy(),\n",
    "            X_train_final[\"X_moods\"].cpu().numpy(),\n",
    "            X_train_final[\"X_genres_categories\"].cpu().numpy(),\n",
    "            X_train_final[\"X_instruments_categories\"].cpu().numpy(),\n",
    "            X_train_final[\"X_moods_categories\"].cpu().numpy(),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    # Normaliser les données\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_concat)\n",
    "    \n",
    "    return X_train_scaled, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Random Forest:   2%|▏         | 1/50 [00:18<15:09, 18.56s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(rf_model\u001b[38;5;241m.\u001b[39mn_estimators), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Random Forest\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     28\u001b[0m     rf_model\u001b[38;5;241m.\u001b[39mn_estimators \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mrf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_pca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_rf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Évaluer le modèle sur l'ensemble de validation\u001b[39;00m\n\u001b[1;32m     32\u001b[0m y_val_pred_rf \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mpredict(X_val_pca)\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    201\u001b[0m         X,\n\u001b[1;32m    202\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Préparer les données pour le Random Forest\n",
    "X_train_rf, y_train_rf = prepare_data_for_rf(\n",
    "    X_train_final_train, train_targets.cpu().numpy()\n",
    ")\n",
    "X_val_rf, y_val_rf = prepare_data_for_rf(X_train_final_val, val_targets.cpu().numpy())\n",
    "X_test_rf, y_test_rf = prepare_data_for_rf(X_test_final, y_test_tensor.cpu().numpy())\n",
    "\n",
    "# Réduire la dimensionnalité des données avec PCA\n",
    "pca = PCA(n_components=100)  # Ajustez le nombre de composants selon vos besoins\n",
    "X_train_pca = pca.fit_transform(X_train_rf)\n",
    "X_val_pca = pca.transform(X_val_rf)\n",
    "X_test_pca = pca.transform(X_test_rf)\n",
    "\n",
    "# Initialiser le Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=50, random_state=42, warm_start=True)\n",
    "\n",
    "# Entraîner le modèle avec une barre de progression\n",
    "for i in tqdm(range(rf_model.n_estimators), desc=\"Training Random Forest\"):\n",
    "    rf_model.n_estimators = i + 1\n",
    "    rf_model.fit(X_train_pca, y_train_rf)\n",
    "\n",
    "# Évaluer le modèle sur l'ensemble de validation\n",
    "y_val_pred_rf = rf_model.predict(X_val_pca)\n",
    "val_accuracy = accuracy_score(y_val_rf, y_val_pred_rf)\n",
    "val_f1 = f1_score(y_val_rf, y_val_pred_rf, average='weighted')\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# Évaluer le modèle sur l'ensemble de test\n",
    "y_test_pred_rf = rf_model.predict(X_test_pca)\n",
    "test_accuracy = accuracy_score(y_test_rf, y_test_pred_rf)\n",
    "test_f1 = f1_score(y_test_rf, y_test_pred_rf, average='weighted')\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.0074, Validation F1 Score: 0.3271\n",
      "Test Accuracy: 0.0074, Test F1 Score: 0.3272\n"
     ]
    }
   ],
   "source": [
    "# Évaluer le modèle sur l'ensemble de validation\n",
    "y_val_pred_rf = rf_model.predict(X_val_pca)\n",
    "val_accuracy = accuracy_score(y_val_rf, y_val_pred_rf)\n",
    "val_f1 = f1_score(y_val_rf, y_val_pred_rf, average='weighted')\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# Évaluer le modèle sur l'ensemble de test\n",
    "y_test_pred_rf = rf_model.predict(X_test_pca)\n",
    "test_accuracy = accuracy_score(y_test_rf, y_test_pred_rf)\n",
    "test_f1 = f1_score(y_test_rf, y_test_pred_rf, average='weighted')\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de composants principaux initialement: 289\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Supposons que X_train_rf soit votre matrice de données d'origine\n",
    "# X_train_rf est un tableau NumPy ou une structure similaire\n",
    "\n",
    "# Initialiser PCA\n",
    "pca = PCA()\n",
    "\n",
    "# Ajuster PCA sur les données d'entraînement\n",
    "pca.fit(X_train_rf)\n",
    "\n",
    "# Obtenir le nombre de composants principaux\n",
    "n_components_total = pca.n_components_\n",
    "\n",
    "print(f\"Nombre total de composants principaux initialement: {n_components_total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 569, number of negative: 70377\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007766 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.008020 -> initscore=-4.817741\n",
      "[LightGBM] [Info] Start training from score -4.817741\n",
      "[LightGBM] [Info] Number of positive: 3983, number of negative: 66963\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007330 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.056141 -> initscore=-2.822105\n",
      "[LightGBM] [Info] Start training from score -2.822105\n",
      "[LightGBM] [Info] Number of positive: 812, number of negative: 70134\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006037 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011445 -> initscore=-4.458663\n",
      "[LightGBM] [Info] Start training from score -4.458663\n",
      "[LightGBM] [Info] Number of positive: 255, number of negative: 70691\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007510 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003594 -> initscore=-5.624810\n",
      "[LightGBM] [Info] Start training from score -5.624810\n",
      "[LightGBM] [Info] Number of positive: 200, number of negative: 70746\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006723 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002819 -> initscore=-5.868534\n",
      "[LightGBM] [Info] Start training from score -5.868534\n",
      "[LightGBM] [Info] Number of positive: 938, number of negative: 70008\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007715 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.013221 -> initscore=-4.312615\n",
      "[LightGBM] [Info] Start training from score -4.312615\n",
      "[LightGBM] [Info] Number of positive: 402, number of negative: 70544\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.014260 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005666 -> initscore=-5.167540\n",
      "[LightGBM] [Info] Start training from score -5.167540\n",
      "[LightGBM] [Info] Number of positive: 1606, number of negative: 69340\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.016332 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.022637 -> initscore=-3.765275\n",
      "[LightGBM] [Info] Start training from score -3.765275\n",
      "[LightGBM] [Info] Number of positive: 3237, number of negative: 67709\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.017542 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.045626 -> initscore=-3.040572\n",
      "[LightGBM] [Info] Start training from score -3.040572\n",
      "[LightGBM] [Info] Number of positive: 714, number of negative: 70232\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006285 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010064 -> initscore=-4.588676\n",
      "[LightGBM] [Info] Start training from score -4.588676\n",
      "[LightGBM] [Info] Number of positive: 2392, number of negative: 68554\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009046 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.033716 -> initscore=-3.355492\n",
      "[LightGBM] [Info] Start training from score -3.355492\n",
      "[LightGBM] [Info] Number of positive: 6648, number of negative: 64298\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009262 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.093705 -> initscore=-2.269212\n",
      "[LightGBM] [Info] Start training from score -2.269212\n",
      "[LightGBM] [Info] Number of positive: 1447, number of negative: 69499\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006326 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020396 -> initscore=-3.871820\n",
      "[LightGBM] [Info] Start training from score -3.871820\n",
      "[LightGBM] [Info] Number of positive: 4505, number of negative: 66441\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008324 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063499 -> initscore=-2.691126\n",
      "[LightGBM] [Info] Start training from score -2.691126\n",
      "[LightGBM] [Info] Number of positive: 2925, number of negative: 68021\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006768 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.041229 -> initscore=-3.146522\n",
      "[LightGBM] [Info] Start training from score -3.146522\n",
      "[LightGBM] [Info] Number of positive: 1165, number of negative: 69781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009462 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.016421 -> initscore=-4.092641\n",
      "[LightGBM] [Info] Start training from score -4.092641\n",
      "[LightGBM] [Info] Number of positive: 221, number of negative: 70725\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006642 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003115 -> initscore=-5.768392\n",
      "[LightGBM] [Info] Start training from score -5.768392\n",
      "[LightGBM] [Info] Number of positive: 460, number of negative: 70486\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.010290 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006484 -> initscore=-5.031943\n",
      "[LightGBM] [Info] Start training from score -5.031943\n",
      "[LightGBM] [Info] Number of positive: 481, number of negative: 70465\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006549 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006780 -> initscore=-4.987004\n",
      "[LightGBM] [Info] Start training from score -4.987004\n",
      "[LightGBM] [Info] Number of positive: 764, number of negative: 70182\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006346 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010769 -> initscore=-4.520279\n",
      "[LightGBM] [Info] Start training from score -4.520279\n",
      "[LightGBM] [Info] Number of positive: 2989, number of negative: 67957\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006260 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.042131 -> initscore=-3.123936\n",
      "[LightGBM] [Info] Start training from score -3.123936\n",
      "[LightGBM] [Info] Number of positive: 510, number of negative: 70436\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008774 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007189 -> initscore=-4.928049\n",
      "[LightGBM] [Info] Start training from score -4.928049\n",
      "[LightGBM] [Info] Number of positive: 430, number of negative: 70516\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009161 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006061 -> initscore=-5.099810\n",
      "[LightGBM] [Info] Start training from score -5.099810\n",
      "[LightGBM] [Info] Number of positive: 3149, number of negative: 67797\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008266 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.044386 -> initscore=-3.069433\n",
      "[LightGBM] [Info] Start training from score -3.069433\n",
      "[LightGBM] [Info] Number of positive: 2713, number of negative: 68233\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006253 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038240 -> initscore=-3.224873\n",
      "[LightGBM] [Info] Start training from score -3.224873\n",
      "[LightGBM] [Info] Number of positive: 261, number of negative: 70685\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006900 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003679 -> initscore=-5.601468\n",
      "[LightGBM] [Info] Start training from score -5.601468\n",
      "[LightGBM] [Info] Number of positive: 1189, number of negative: 69757\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006706 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.016759 -> initscore=-4.071905\n",
      "[LightGBM] [Info] Start training from score -4.071905\n",
      "[LightGBM] [Info] Number of positive: 10840, number of negative: 60106\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007664 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.152792 -> initscore=-1.712867\n",
      "[LightGBM] [Info] Start training from score -1.712867\n",
      "[LightGBM] [Info] Number of positive: 2825, number of negative: 68121\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008605 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039819 -> initscore=-3.182777\n",
      "[LightGBM] [Info] Start training from score -3.182777\n",
      "[LightGBM] [Info] Number of positive: 1253, number of negative: 69693\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008238 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.017661 -> initscore=-4.018559\n",
      "[LightGBM] [Info] Start training from score -4.018559\n",
      "[LightGBM] [Info] Number of positive: 458, number of negative: 70488\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006803 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006456 -> initscore=-5.036329\n",
      "[LightGBM] [Info] Start training from score -5.036329\n",
      "[LightGBM] [Info] Number of positive: 731, number of negative: 70215\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.020269 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010304 -> initscore=-4.564904\n",
      "[LightGBM] [Info] Start training from score -4.564904\n",
      "[LightGBM] [Info] Number of positive: 273, number of negative: 70673\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006670 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003848 -> initscore=-5.556347\n",
      "[LightGBM] [Info] Start training from score -5.556347\n",
      "[LightGBM] [Info] Number of positive: 1936, number of negative: 69010\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.018012 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.027288 -> initscore=-3.573627\n",
      "[LightGBM] [Info] Start training from score -3.573627\n",
      "[LightGBM] [Info] Number of positive: 1087, number of negative: 69859\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006756 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.015322 -> initscore=-4.163057\n",
      "[LightGBM] [Info] Start training from score -4.163057\n",
      "[LightGBM] [Info] Number of positive: 2555, number of negative: 68391\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006900 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036013 -> initscore=-3.287189\n",
      "[LightGBM] [Info] Start training from score -3.287189\n",
      "[LightGBM] [Info] Number of positive: 1928, number of negative: 69018\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009002 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.027176 -> initscore=-3.577884\n",
      "[LightGBM] [Info] Start training from score -3.577884\n",
      "[LightGBM] [Info] Number of positive: 1237, number of negative: 69709\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006698 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.017436 -> initscore=-4.031640\n",
      "[LightGBM] [Info] Start training from score -4.031640\n",
      "[LightGBM] [Info] Number of positive: 1764, number of negative: 69182\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007170 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.024864 -> initscore=-3.669157\n",
      "[LightGBM] [Info] Start training from score -3.669157\n",
      "[LightGBM] [Info] Number of positive: 4949, number of negative: 65997\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008441 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069757 -> initscore=-2.590424\n",
      "[LightGBM] [Info] Start training from score -2.590424\n",
      "[LightGBM] [Info] Number of positive: 302, number of negative: 70644\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008308 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004257 -> initscore=-5.454981\n",
      "[LightGBM] [Info] Start training from score -5.454981\n",
      "[LightGBM] [Info] Number of positive: 717, number of negative: 70229\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.020037 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010106 -> initscore=-4.584441\n",
      "[LightGBM] [Info] Start training from score -4.584441\n",
      "[LightGBM] [Info] Number of positive: 1018, number of negative: 69928\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.011348 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014349 -> initscore=-4.229626\n",
      "[LightGBM] [Info] Start training from score -4.229626\n",
      "[LightGBM] [Info] Number of positive: 342, number of negative: 70604\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009554 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004821 -> initscore=-5.330031\n",
      "[LightGBM] [Info] Start training from score -5.330031\n",
      "[LightGBM] [Info] Number of positive: 580, number of negative: 70366\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009879 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.008175 -> initscore=-4.798437\n",
      "[LightGBM] [Info] Start training from score -4.798437\n",
      "[LightGBM] [Info] Number of positive: 2097, number of negative: 68849\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007156 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.029558 -> initscore=-3.491408\n",
      "[LightGBM] [Info] Start training from score -3.491408\n",
      "[LightGBM] [Info] Number of positive: 191, number of negative: 70755\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009048 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002692 -> initscore=-5.914705\n",
      "[LightGBM] [Info] Start training from score -5.914705\n",
      "[LightGBM] [Info] Number of positive: 175, number of negative: 70771\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008022 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002467 -> initscore=-6.002419\n",
      "[LightGBM] [Info] Start training from score -6.002419\n",
      "[LightGBM] [Info] Number of positive: 373, number of negative: 70573\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007025 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005258 -> initscore=-5.242824\n",
      "[LightGBM] [Info] Start training from score -5.242824\n",
      "[LightGBM] [Info] Number of positive: 196, number of negative: 70750\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007908 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002763 -> initscore=-5.888793\n",
      "[LightGBM] [Info] Start training from score -5.888793\n",
      "[LightGBM] [Info] Number of positive: 188, number of negative: 70758\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.010849 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002650 -> initscore=-5.930579\n",
      "[LightGBM] [Info] Start training from score -5.930579\n",
      "[LightGBM] [Info] Number of positive: 267, number of negative: 70679\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007127 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003763 -> initscore=-5.578655\n",
      "[LightGBM] [Info] Start training from score -5.578655\n",
      "[LightGBM] [Info] Number of positive: 130, number of negative: 70816\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006614 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001832 -> initscore=-6.300306\n",
      "[LightGBM] [Info] Start training from score -6.300306\n",
      "[LightGBM] [Info] Number of positive: 785, number of negative: 70161\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008470 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011065 -> initscore=-4.492864\n",
      "[LightGBM] [Info] Start training from score -4.492864\n",
      "[LightGBM] [Info] Number of positive: 1107, number of negative: 69839\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008580 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.015603 -> initscore=-4.144539\n",
      "[LightGBM] [Info] Start training from score -4.144539\n",
      "[LightGBM] [Info] Number of positive: 1218, number of negative: 69728\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008981 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.017168 -> initscore=-4.047392\n",
      "[LightGBM] [Info] Start training from score -4.047392\n",
      "[LightGBM] [Info] Number of positive: 194, number of negative: 70752\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006779 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002734 -> initscore=-5.899078\n",
      "[LightGBM] [Info] Start training from score -5.899078\n",
      "[LightGBM] [Info] Number of positive: 393, number of negative: 70553\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007313 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005539 -> initscore=-5.190310\n",
      "[LightGBM] [Info] Start training from score -5.190310\n",
      "[LightGBM] [Info] Number of positive: 359, number of negative: 70587\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007501 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005060 -> initscore=-5.281279\n",
      "[LightGBM] [Info] Start training from score -5.281279\n",
      "[LightGBM] [Info] Number of positive: 546, number of negative: 70400\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006926 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007696 -> initscore=-4.859330\n",
      "[LightGBM] [Info] Start training from score -4.859330\n",
      "[LightGBM] [Info] Number of positive: 265, number of negative: 70681\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.010679 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003735 -> initscore=-5.586202\n",
      "[LightGBM] [Info] Start training from score -5.586202\n",
      "[LightGBM] [Info] Number of positive: 1217, number of negative: 69729\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007003 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.017154 -> initscore=-4.048227\n",
      "[LightGBM] [Info] Start training from score -4.048227\n",
      "[LightGBM] [Info] Number of positive: 113, number of negative: 70833\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008901 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001593 -> initscore=-6.440692\n",
      "[LightGBM] [Info] Start training from score -6.440692\n",
      "[LightGBM] [Info] Number of positive: 782, number of negative: 70164\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008877 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011022 -> initscore=-4.496736\n",
      "[LightGBM] [Info] Start training from score -4.496736\n",
      "[LightGBM] [Info] Number of positive: 398, number of negative: 70548\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007253 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005610 -> initscore=-5.177597\n",
      "[LightGBM] [Info] Start training from score -5.177597\n",
      "[LightGBM] [Info] Number of positive: 452, number of negative: 70494\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006710 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006371 -> initscore=-5.049601\n",
      "[LightGBM] [Info] Start training from score -5.049601\n",
      "[LightGBM] [Info] Number of positive: 1510, number of negative: 69436\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006708 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.021284 -> initscore=-3.828296\n",
      "[LightGBM] [Info] Start training from score -3.828296\n",
      "[LightGBM] [Info] Number of positive: 563, number of negative: 70383\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007607 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007936 -> initscore=-4.828427\n",
      "[LightGBM] [Info] Start training from score -4.828427\n",
      "[LightGBM] [Info] Number of positive: 413, number of negative: 70533\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006498 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005821 -> initscore=-5.140388\n",
      "[LightGBM] [Info] Start training from score -5.140388\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 70349\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009186 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.008415 -> initscore=-4.769307\n",
      "[LightGBM] [Info] Start training from score -4.769307\n",
      "[LightGBM] [Info] Number of positive: 1503, number of negative: 69443\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008619 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.021185 -> initscore=-3.833043\n",
      "[LightGBM] [Info] Start training from score -3.833043\n",
      "[LightGBM] [Info] Number of positive: 80, number of negative: 70866\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006845 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001128 -> initscore=-6.786519\n",
      "[LightGBM] [Info] Start training from score -6.786519\n",
      "[LightGBM] [Info] Number of positive: 116, number of negative: 70830\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.011948 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001635 -> initscore=-6.414448\n",
      "[LightGBM] [Info] Start training from score -6.414448\n",
      "[LightGBM] [Info] Number of positive: 599, number of negative: 70347\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008004 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.008443 -> initscore=-4.765934\n",
      "[LightGBM] [Info] Start training from score -4.765934\n",
      "[LightGBM] [Info] Number of positive: 312, number of negative: 70634\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.018853 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004398 -> initscore=-5.422264\n",
      "[LightGBM] [Info] Start training from score -5.422264\n",
      "[LightGBM] [Info] Number of positive: 223, number of negative: 70723\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008148 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003143 -> initscore=-5.759354\n",
      "[LightGBM] [Info] Start training from score -5.759354\n",
      "[LightGBM] [Info] Number of positive: 276, number of negative: 70670\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006579 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003890 -> initscore=-5.545376\n",
      "[LightGBM] [Info] Start training from score -5.545376\n",
      "[LightGBM] [Info] Number of positive: 1169, number of negative: 69777\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009966 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.016477 -> initscore=-4.089156\n",
      "[LightGBM] [Info] Start training from score -4.089156\n",
      "[LightGBM] [Info] Number of positive: 372, number of negative: 70574\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008186 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005243 -> initscore=-5.245523\n",
      "[LightGBM] [Info] Start training from score -5.245523\n",
      "[LightGBM] [Info] Number of positive: 285, number of negative: 70661\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008272 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004017 -> initscore=-5.513160\n",
      "[LightGBM] [Info] Start training from score -5.513160\n",
      "[LightGBM] [Info] Number of positive: 1240, number of negative: 69706\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008759 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.017478 -> initscore=-4.029175\n",
      "[LightGBM] [Info] Start training from score -4.029175\n",
      "[LightGBM] [Info] Number of positive: 4730, number of negative: 66216\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.011734 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066670 -> initscore=-2.638997\n",
      "[LightGBM] [Info] Start training from score -2.638997\n",
      "[LightGBM] [Info] Number of positive: 1036, number of negative: 69910\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006646 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014603 -> initscore=-4.211842\n",
      "[LightGBM] [Info] Start training from score -4.211842\n",
      "[LightGBM] [Info] Number of positive: 247, number of negative: 70699\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008528 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003482 -> initscore=-5.656798\n",
      "[LightGBM] [Info] Start training from score -5.656798\n",
      "[LightGBM] [Info] Number of positive: 1172, number of negative: 69774\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.010354 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.016520 -> initscore=-4.086550\n",
      "[LightGBM] [Info] Start training from score -4.086550\n",
      "[LightGBM] [Info] Number of positive: 345, number of negative: 70601\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009690 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004863 -> initscore=-5.321255\n",
      "[LightGBM] [Info] Start training from score -5.321255\n",
      "[LightGBM] [Info] Number of positive: 2243, number of negative: 68703\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.011145 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031616 -> initscore=-3.421979\n",
      "[LightGBM] [Info] Start training from score -3.421979\n",
      "[LightGBM] [Info] Number of positive: 3114, number of negative: 67832\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007013 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.043893 -> initscore=-3.081126\n",
      "[LightGBM] [Info] Start training from score -3.081126\n",
      "[LightGBM] [Info] Number of positive: 132, number of negative: 70814\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006452 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001861 -> initscore=-6.285010\n",
      "[LightGBM] [Info] Start training from score -6.285010\n",
      "[LightGBM] [Info] Number of positive: 1776, number of negative: 69170\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006881 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.025033 -> initscore=-3.662204\n",
      "[LightGBM] [Info] Start training from score -3.662204\n",
      "[LightGBM] [Info] Number of positive: 327, number of negative: 70619\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006036 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004609 -> initscore=-5.375094\n",
      "[LightGBM] [Info] Start training from score -5.375094\n",
      "[LightGBM] [Info] Number of positive: 1724, number of negative: 69222\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007635 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.024300 -> initscore=-3.692672\n",
      "[LightGBM] [Info] Start training from score -3.692672\n",
      "[LightGBM] [Info] Number of positive: 10851, number of negative: 60095\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008143 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.152947 -> initscore=-1.711669\n",
      "[LightGBM] [Info] Start training from score -1.711669\n",
      "[LightGBM] [Info] Number of positive: 1599, number of negative: 69347\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.012919 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.022538 -> initscore=-3.769744\n",
      "[LightGBM] [Info] Start training from score -3.769744\n",
      "[LightGBM] [Info] Number of positive: 821, number of negative: 70125\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008342 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011572 -> initscore=-4.447512\n",
      "[LightGBM] [Info] Start training from score -4.447512\n",
      "[LightGBM] [Info] Number of positive: 15828, number of negative: 55118\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.010794 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.223099 -> initscore=-1.247696\n",
      "[LightGBM] [Info] Start training from score -1.247696\n",
      "[LightGBM] [Info] Number of positive: 359, number of negative: 70587\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008722 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005060 -> initscore=-5.281279\n",
      "[LightGBM] [Info] Start training from score -5.281279\n",
      "[LightGBM] [Info] Number of positive: 663, number of negative: 70283\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008002 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.009345 -> initscore=-4.663510\n",
      "[LightGBM] [Info] Start training from score -4.663510\n",
      "[LightGBM] [Info] Number of positive: 299, number of negative: 70647\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008851 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004214 -> initscore=-5.465007\n",
      "[LightGBM] [Info] Start training from score -5.465007\n",
      "[LightGBM] [Info] Number of positive: 863, number of negative: 70083\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006993 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.012164 -> initscore=-4.397021\n",
      "[LightGBM] [Info] Start training from score -4.397021\n",
      "[LightGBM] [Info] Number of positive: 9132, number of negative: 61814\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.010613 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.128718 -> initscore=-1.912345\n",
      "[LightGBM] [Info] Start training from score -1.912345\n",
      "[LightGBM] [Info] Number of positive: 1286, number of negative: 69660\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.012418 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.018126 -> initscore=-3.992090\n",
      "[LightGBM] [Info] Start training from score -3.992090\n",
      "[LightGBM] [Info] Number of positive: 3730, number of negative: 67216\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007056 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.052575 -> initscore=-2.891503\n",
      "[LightGBM] [Info] Start training from score -2.891503\n",
      "[LightGBM] [Info] Number of positive: 94, number of negative: 70852\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.015997 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001325 -> initscore=-6.625054\n",
      "[LightGBM] [Info] Start training from score -6.625054\n",
      "[LightGBM] [Info] Number of positive: 56, number of negative: 70890\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006763 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000789 -> initscore=-7.143533\n",
      "[LightGBM] [Info] Start training from score -7.143533\n",
      "[LightGBM] [Info] Number of positive: 364, number of negative: 70582\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007537 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005131 -> initscore=-5.267377\n",
      "[LightGBM] [Info] Start training from score -5.267377\n",
      "[LightGBM] [Info] Number of positive: 1692, number of negative: 69254\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008169 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.023849 -> initscore=-3.711870\n",
      "[LightGBM] [Info] Start training from score -3.711870\n",
      "[LightGBM] [Info] Number of positive: 1590, number of negative: 69356\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007796 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.022411 -> initscore=-3.775519\n",
      "[LightGBM] [Info] Start training from score -3.775519\n",
      "[LightGBM] [Info] Number of positive: 428, number of negative: 70518\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008154 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006033 -> initscore=-5.104500\n",
      "[LightGBM] [Info] Start training from score -5.104500\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 70148\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006737 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011248 -> initscore=-4.476254\n",
      "[LightGBM] [Info] Start training from score -4.476254\n",
      "[LightGBM] [Info] Number of positive: 266, number of negative: 70680\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006955 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003749 -> initscore=-5.582422\n",
      "[LightGBM] [Info] Start training from score -5.582422\n",
      "[LightGBM] [Info] Number of positive: 913, number of negative: 70033\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007698 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.012869 -> initscore=-4.339986\n",
      "[LightGBM] [Info] Start training from score -4.339986\n",
      "[LightGBM] [Info] Number of positive: 218, number of negative: 70728\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.011560 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003073 -> initscore=-5.782102\n",
      "[LightGBM] [Info] Start training from score -5.782102\n",
      "[LightGBM] [Info] Number of positive: 3594, number of negative: 67352\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007691 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.050658 -> initscore=-2.930667\n",
      "[LightGBM] [Info] Start training from score -2.930667\n",
      "[LightGBM] [Info] Number of positive: 315, number of negative: 70631\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007272 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004440 -> initscore=-5.412652\n",
      "[LightGBM] [Info] Start training from score -5.412652\n",
      "[LightGBM] [Info] Number of positive: 16047, number of negative: 54899\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008353 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.226186 -> initscore=-1.229973\n",
      "[LightGBM] [Info] Start training from score -1.229973\n",
      "[LightGBM] [Info] Number of positive: 14078, number of negative: 56868\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008834 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198433 -> initscore=-1.396119\n",
      "[LightGBM] [Info] Start training from score -1.396119\n",
      "[LightGBM] [Info] Number of positive: 374, number of negative: 70572\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009594 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005272 -> initscore=-5.240133\n",
      "[LightGBM] [Info] Start training from score -5.240133\n",
      "[LightGBM] [Info] Number of positive: 20185, number of negative: 50761\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.017278 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.284512 -> initscore=-0.922189\n",
      "[LightGBM] [Info] Start training from score -0.922189\n",
      "[LightGBM] [Info] Number of positive: 785, number of negative: 70161\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007904 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011065 -> initscore=-4.492864\n",
      "[LightGBM] [Info] Start training from score -4.492864\n",
      "[LightGBM] [Info] Number of positive: 612, number of negative: 70334\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006503 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.008626 -> initscore=-4.744278\n",
      "[LightGBM] [Info] Start training from score -4.744278\n",
      "[LightGBM] [Info] Number of positive: 501, number of negative: 70445\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.011254 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007062 -> initscore=-4.945981\n",
      "[LightGBM] [Info] Start training from score -4.945981\n",
      "[LightGBM] [Info] Number of positive: 4673, number of negative: 66273\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006792 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.065867 -> initscore=-2.651981\n",
      "[LightGBM] [Info] Start training from score -2.651981\n",
      "[LightGBM] [Info] Number of positive: 1391, number of negative: 69555\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006975 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.019606 -> initscore=-3.912095\n",
      "[LightGBM] [Info] Start training from score -3.912095\n",
      "[LightGBM] [Info] Number of positive: 237, number of negative: 70709\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008178 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003341 -> initscore=-5.698268\n",
      "[LightGBM] [Info] Start training from score -5.698268\n",
      "[LightGBM] [Info] Number of positive: 3645, number of negative: 67301\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008369 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.051377 -> initscore=-2.915819\n",
      "[LightGBM] [Info] Start training from score -2.915819\n",
      "[LightGBM] [Info] Number of positive: 1911, number of negative: 69035\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006728 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026936 -> initscore=-3.586987\n",
      "[LightGBM] [Info] Start training from score -3.586987\n",
      "[LightGBM] [Info] Number of positive: 633, number of negative: 70313\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006709 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.008922 -> initscore=-4.710242\n",
      "[LightGBM] [Info] Start training from score -4.710242\n",
      "[LightGBM] [Info] Number of positive: 1903, number of negative: 69043\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007689 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026823 -> initscore=-3.591298\n",
      "[LightGBM] [Info] Start training from score -3.591298\n",
      "[LightGBM] [Info] Number of positive: 1961, number of negative: 68985\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007373 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.027641 -> initscore=-3.560435\n",
      "[LightGBM] [Info] Start training from score -3.560435\n",
      "[LightGBM] [Info] Number of positive: 1795, number of negative: 69151\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009831 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.025301 -> initscore=-3.651287\n",
      "[LightGBM] [Info] Start training from score -3.651287\n",
      "[LightGBM] [Info] Number of positive: 701, number of negative: 70245\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006982 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.009881 -> initscore=-4.607237\n",
      "[LightGBM] [Info] Start training from score -4.607237\n",
      "[LightGBM] [Info] Number of positive: 1908, number of negative: 69038\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.015233 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026894 -> initscore=-3.588602\n",
      "[LightGBM] [Info] Start training from score -3.588602\n",
      "[LightGBM] [Info] Number of positive: 562, number of negative: 70384\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007354 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007922 -> initscore=-4.830219\n",
      "[LightGBM] [Info] Start training from score -4.830219\n",
      "[LightGBM] [Info] Number of positive: 228, number of negative: 70718\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006803 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003214 -> initscore=-5.737110\n",
      "[LightGBM] [Info] Start training from score -5.737110\n",
      "[LightGBM] [Info] Number of positive: 2140, number of negative: 68806\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006891 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.030164 -> initscore=-3.470485\n",
      "[LightGBM] [Info] Start training from score -3.470485\n",
      "[LightGBM] [Info] Number of positive: 316, number of negative: 70630\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007597 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004454 -> initscore=-5.409468\n",
      "[LightGBM] [Info] Start training from score -5.409468\n",
      "[LightGBM] [Info] Number of positive: 408, number of negative: 70538\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007916 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005751 -> initscore=-5.152640\n",
      "[LightGBM] [Info] Start training from score -5.152640\n",
      "[LightGBM] [Info] Number of positive: 2343, number of negative: 68603\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006825 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.033025 -> initscore=-3.376904\n",
      "[LightGBM] [Info] Start training from score -3.376904\n",
      "[LightGBM] [Info] Number of positive: 498, number of negative: 70448\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008739 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007019 -> initscore=-4.952030\n",
      "[LightGBM] [Info] Start training from score -4.952030\n",
      "[LightGBM] [Info] Number of positive: 218, number of negative: 70728\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.010206 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003073 -> initscore=-5.782102\n",
      "[LightGBM] [Info] Start training from score -5.782102\n",
      "[LightGBM] [Info] Number of positive: 6148, number of negative: 64798\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.013434 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.086657 -> initscore=-2.355148\n",
      "[LightGBM] [Info] Start training from score -2.355148\n",
      "[LightGBM] [Info] Number of positive: 830, number of negative: 70116\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007553 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011699 -> initscore=-4.436481\n",
      "[LightGBM] [Info] Start training from score -4.436481\n",
      "[LightGBM] [Info] Number of positive: 684, number of negative: 70262\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009125 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.009641 -> initscore=-4.632028\n",
      "[LightGBM] [Info] Start training from score -4.632028\n",
      "[LightGBM] [Info] Number of positive: 1826, number of negative: 69120\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007603 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.025738 -> initscore=-3.633716\n",
      "[LightGBM] [Info] Start training from score -3.633716\n",
      "[LightGBM] [Info] Number of positive: 73, number of negative: 70873\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007109 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001029 -> initscore=-6.878185\n",
      "[LightGBM] [Info] Start training from score -6.878185\n",
      "[LightGBM] [Info] Number of positive: 207, number of negative: 70739\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006604 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002918 -> initscore=-5.834034\n",
      "[LightGBM] [Info] Start training from score -5.834034\n",
      "[LightGBM] [Info] Number of positive: 119, number of negative: 70827\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007357 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001677 -> initscore=-6.388872\n",
      "[LightGBM] [Info] Start training from score -6.388872\n",
      "[LightGBM] [Info] Number of positive: 830, number of negative: 70116\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006738 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011699 -> initscore=-4.436481\n",
      "[LightGBM] [Info] Start training from score -4.436481\n",
      "[LightGBM] [Info] Number of positive: 1098, number of negative: 69848\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007066 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.015477 -> initscore=-4.152831\n",
      "[LightGBM] [Info] Start training from score -4.152831\n",
      "[LightGBM] [Info] Number of positive: 942, number of negative: 70004\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006750 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.013278 -> initscore=-4.308302\n",
      "[LightGBM] [Info] Start training from score -4.308302\n",
      "[LightGBM] [Info] Number of positive: 245, number of negative: 70701\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007855 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003453 -> initscore=-5.664957\n",
      "[LightGBM] [Info] Start training from score -5.664957\n",
      "[LightGBM] [Info] Number of positive: 178, number of negative: 70768\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006989 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002509 -> initscore=-5.985379\n",
      "[LightGBM] [Info] Start training from score -5.985379\n",
      "[LightGBM] [Info] Number of positive: 419, number of negative: 70527\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008673 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005906 -> initscore=-5.125880\n",
      "[LightGBM] [Info] Start training from score -5.125880\n",
      "[LightGBM] [Info] Number of positive: 1165, number of negative: 69781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006932 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.016421 -> initscore=-4.092641\n",
      "[LightGBM] [Info] Start training from score -4.092641\n",
      "[LightGBM] [Info] Number of positive: 17116, number of negative: 53830\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007147 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.241254 -> initscore=-1.145817\n",
      "[LightGBM] [Info] Start training from score -1.145817\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 70781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006909 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002326 -> initscore=-6.061400\n",
      "[LightGBM] [Info] Start training from score -6.061400\n",
      "[LightGBM] [Info] Number of positive: 1844, number of negative: 69102\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008753 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.025992 -> initscore=-3.623647\n",
      "[LightGBM] [Info] Start training from score -3.623647\n",
      "[LightGBM] [Info] Number of positive: 5439, number of negative: 65507\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007200 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076664 -> initscore=-2.488562\n",
      "[LightGBM] [Info] Start training from score -2.488562\n",
      "[LightGBM] [Info] Number of positive: 834, number of negative: 70112\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007483 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011755 -> initscore=-4.431616\n",
      "[LightGBM] [Info] Start training from score -4.431616\n",
      "[LightGBM] [Info] Number of positive: 239, number of negative: 70707\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009500 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003369 -> initscore=-5.689836\n",
      "[LightGBM] [Info] Start training from score -5.689836\n",
      "[LightGBM] [Info] Number of positive: 7125, number of negative: 63821\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008555 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100428 -> initscore=-2.192473\n",
      "[LightGBM] [Info] Start training from score -2.192473\n",
      "[LightGBM] [Info] Number of positive: 3261, number of negative: 67685\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006760 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.045965 -> initscore=-3.032831\n",
      "[LightGBM] [Info] Start training from score -3.032831\n",
      "[LightGBM] [Info] Number of positive: 760, number of negative: 70186\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007325 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010712 -> initscore=-4.525586\n",
      "[LightGBM] [Info] Start training from score -4.525586\n",
      "[LightGBM] [Info] Number of positive: 2836, number of negative: 68110\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006917 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039974 -> initscore=-3.178729\n",
      "[LightGBM] [Info] Start training from score -3.178729\n",
      "[LightGBM] [Info] Number of positive: 342, number of negative: 70604\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007735 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004821 -> initscore=-5.330031\n",
      "[LightGBM] [Info] Start training from score -5.330031\n",
      "[LightGBM] [Info] Number of positive: 467, number of negative: 70479\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.025409 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006582 -> initscore=-5.016741\n",
      "[LightGBM] [Info] Start training from score -5.016741\n",
      "[LightGBM] [Info] Number of positive: 522, number of negative: 70424\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007183 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007358 -> initscore=-4.904622\n",
      "[LightGBM] [Info] Start training from score -4.904622\n",
      "[LightGBM] [Info] Number of positive: 697, number of negative: 70249\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008141 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.009824 -> initscore=-4.613016\n",
      "[LightGBM] [Info] Start training from score -4.613016\n",
      "[LightGBM] [Info] Number of positive: 2950, number of negative: 67996\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.010117 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.041581 -> initscore=-3.137644\n",
      "[LightGBM] [Info] Start training from score -3.137644\n",
      "[LightGBM] [Info] Number of positive: 216, number of negative: 70730\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008454 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003045 -> initscore=-5.791347\n",
      "[LightGBM] [Info] Start training from score -5.791347\n",
      "[LightGBM] [Info] Number of positive: 92, number of negative: 70854\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007507 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001297 -> initscore=-6.646588\n",
      "[LightGBM] [Info] Start training from score -6.646588\n",
      "[LightGBM] [Info] Number of positive: 11921, number of negative: 59025\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.012006 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.168029 -> initscore=-1.599660\n",
      "[LightGBM] [Info] Start training from score -1.599660\n",
      "[LightGBM] [Info] Number of positive: 3493, number of negative: 67453\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007383 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.049235 -> initscore=-2.960670\n",
      "[LightGBM] [Info] Start training from score -2.960670\n",
      "[LightGBM] [Info] Number of positive: 4608, number of negative: 66338\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007017 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.064951 -> initscore=-2.666969\n",
      "[LightGBM] [Info] Start training from score -2.666969\n",
      "[LightGBM] [Info] Number of positive: 4504, number of negative: 66442\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007241 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063485 -> initscore=-2.691363\n",
      "[LightGBM] [Info] Start training from score -2.691363\n",
      "[LightGBM] [Info] Number of positive: 1315, number of negative: 69631\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006961 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.018535 -> initscore=-3.969373\n",
      "[LightGBM] [Info] Start training from score -3.969373\n",
      "[LightGBM] [Info] Number of positive: 21933, number of negative: 49013\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008279 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.309151 -> initscore=-0.804093\n",
      "[LightGBM] [Info] Start training from score -0.804093\n",
      "[LightGBM] [Info] Number of positive: 225, number of negative: 70721\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.017393 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003171 -> initscore=-5.750397\n",
      "[LightGBM] [Info] Start training from score -5.750397\n",
      "[LightGBM] [Info] Number of positive: 334, number of negative: 70612\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007168 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004708 -> initscore=-5.353814\n",
      "[LightGBM] [Info] Start training from score -5.353814\n",
      "[LightGBM] [Info] Number of positive: 1748, number of negative: 69198\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007772 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.024638 -> initscore=-3.678500\n",
      "[LightGBM] [Info] Start training from score -3.678500\n",
      "[LightGBM] [Info] Number of positive: 528, number of negative: 70418\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008372 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007442 -> initscore=-4.893108\n",
      "[LightGBM] [Info] Start training from score -4.893108\n",
      "[LightGBM] [Info] Number of positive: 1304, number of negative: 69642\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007439 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.018380 -> initscore=-3.977931\n",
      "[LightGBM] [Info] Start training from score -3.977931\n",
      "[LightGBM] [Info] Number of positive: 344, number of negative: 70602\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.013414 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004849 -> initscore=-5.324172\n",
      "[LightGBM] [Info] Start training from score -5.324172\n",
      "[LightGBM] [Info] Number of positive: 596, number of negative: 70350\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009278 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.008401 -> initscore=-4.770997\n",
      "[LightGBM] [Info] Start training from score -4.770997\n",
      "[LightGBM] [Info] Number of positive: 2624, number of negative: 68322\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009140 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036986 -> initscore=-3.259532\n",
      "[LightGBM] [Info] Start training from score -3.259532\n",
      "[LightGBM] [Info] Number of positive: 325, number of negative: 70621\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007897 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004581 -> initscore=-5.381258\n",
      "[LightGBM] [Info] Start training from score -5.381258\n",
      "[LightGBM] [Info] Number of positive: 302, number of negative: 70644\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007255 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004257 -> initscore=-5.454981\n",
      "[LightGBM] [Info] Start training from score -5.454981\n",
      "[LightGBM] [Info] Number of positive: 1600, number of negative: 69346\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.011881 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.022552 -> initscore=-3.769105\n",
      "[LightGBM] [Info] Start training from score -3.769105\n",
      "[LightGBM] [Info] Number of positive: 578, number of negative: 70368\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006834 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.008147 -> initscore=-4.801920\n",
      "[LightGBM] [Info] Start training from score -4.801920\n",
      "[LightGBM] [Info] Number of positive: 1212, number of negative: 69734\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007971 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.017083 -> initscore=-4.052416\n",
      "[LightGBM] [Info] Start training from score -4.052416\n",
      "[LightGBM] [Info] Number of positive: 335, number of negative: 70611\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007040 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004722 -> initscore=-5.350811\n",
      "[LightGBM] [Info] Start training from score -5.350811\n",
      "[LightGBM] [Info] Number of positive: 3004, number of negative: 67942\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006662 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.042342 -> initscore=-3.118710\n",
      "[LightGBM] [Info] Start training from score -3.118710\n",
      "[LightGBM] [Info] Number of positive: 16363, number of negative: 54583\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007227 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.230640 -> initscore=-1.204700\n",
      "[LightGBM] [Info] Start training from score -1.204700\n",
      "[LightGBM] [Info] Number of positive: 756, number of negative: 70190\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007730 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010656 -> initscore=-4.530920\n",
      "[LightGBM] [Info] Start training from score -4.530920\n",
      "[LightGBM] [Info] Number of positive: 240, number of negative: 70706\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007120 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003383 -> initscore=-5.685647\n",
      "[LightGBM] [Info] Start training from score -5.685647\n",
      "[LightGBM] [Info] Number of positive: 1587, number of negative: 69359\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.017245 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.022369 -> initscore=-3.777450\n",
      "[LightGBM] [Info] Start training from score -3.777450\n",
      "[LightGBM] [Info] Number of positive: 355, number of negative: 70591\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009366 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005004 -> initscore=-5.292540\n",
      "[LightGBM] [Info] Start training from score -5.292540\n",
      "[LightGBM] [Info] Number of positive: 529, number of negative: 70417\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007987 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007456 -> initscore=-4.891202\n",
      "[LightGBM] [Info] Start training from score -4.891202\n",
      "[LightGBM] [Info] Number of positive: 1728, number of negative: 69218\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.010630 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.024357 -> initscore=-3.690296\n",
      "[LightGBM] [Info] Start training from score -3.690296\n",
      "[LightGBM] [Info] Number of positive: 1772, number of negative: 69174\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006676 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.024977 -> initscore=-3.664516\n",
      "[LightGBM] [Info] Start training from score -3.664516\n",
      "[LightGBM] [Info] Number of positive: 834, number of negative: 70112\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009568 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011755 -> initscore=-4.431616\n",
      "[LightGBM] [Info] Start training from score -4.431616\n",
      "[LightGBM] [Info] Number of positive: 2759, number of negative: 68187\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007637 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038889 -> initscore=-3.207386\n",
      "[LightGBM] [Info] Start training from score -3.207386\n",
      "[LightGBM] [Info] Number of positive: 2227, number of negative: 68719\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.012963 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031390 -> initscore=-3.429370\n",
      "[LightGBM] [Info] Start training from score -3.429370\n",
      "[LightGBM] [Info] Number of positive: 4294, number of negative: 66652\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007590 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.060525 -> initscore=-2.742266\n",
      "[LightGBM] [Info] Start training from score -2.742266\n",
      "[LightGBM] [Info] Number of positive: 6642, number of negative: 64304\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006585 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.093621 -> initscore=-2.270209\n",
      "[LightGBM] [Info] Start training from score -2.270209\n",
      "[LightGBM] [Info] Number of positive: 8139, number of negative: 62807\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.010146 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114721 -> initscore=-2.043399\n",
      "[LightGBM] [Info] Start training from score -2.043399\n",
      "[LightGBM] [Info] Number of positive: 1253, number of negative: 69693\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006899 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.017661 -> initscore=-4.018559\n",
      "[LightGBM] [Info] Start training from score -4.018559\n",
      "[LightGBM] [Info] Number of positive: 17756, number of negative: 53190\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007306 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.250275 -> initscore=-1.097147\n",
      "[LightGBM] [Info] Start training from score -1.097147\n",
      "[LightGBM] [Info] Number of positive: 2028, number of negative: 68918\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007152 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.028585 -> initscore=-3.525867\n",
      "[LightGBM] [Info] Start training from score -3.525867\n",
      "[LightGBM] [Info] Number of positive: 2449, number of negative: 68497\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008619 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.034519 -> initscore=-3.331110\n",
      "[LightGBM] [Info] Start training from score -3.331110\n",
      "[LightGBM] [Info] Number of positive: 13690, number of negative: 57256\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.014817 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.192964 -> initscore=-1.430867\n",
      "[LightGBM] [Info] Start training from score -1.430867\n",
      "[LightGBM] [Info] Number of positive: 3480, number of negative: 67466\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007085 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.049051 -> initscore=-2.964591\n",
      "[LightGBM] [Info] Start training from score -2.964591\n",
      "[LightGBM] [Info] Number of positive: 1544, number of negative: 69402\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.018678 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.021763 -> initscore=-3.805539\n",
      "[LightGBM] [Info] Start training from score -3.805539\n",
      "[LightGBM] [Info] Number of positive: 6645, number of negative: 64301\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008367 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.093663 -> initscore=-2.269710\n",
      "[LightGBM] [Info] Start training from score -2.269710\n",
      "[LightGBM] [Info] Number of positive: 1430, number of negative: 69516\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.010742 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020156 -> initscore=-3.883882\n",
      "[LightGBM] [Info] Start training from score -3.883882\n",
      "[LightGBM] [Info] Number of positive: 4073, number of negative: 66873\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008817 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.057410 -> initscore=-2.798415\n",
      "[LightGBM] [Info] Start training from score -2.798415\n",
      "[LightGBM] [Info] Number of positive: 9364, number of negative: 61582\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008616 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.131988 -> initscore=-1.883497\n",
      "[LightGBM] [Info] Start training from score -1.883497\n",
      "[LightGBM] [Info] Number of positive: 1548, number of negative: 69398\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008395 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.021819 -> initscore=-3.802894\n",
      "[LightGBM] [Info] Start training from score -3.802894\n",
      "[LightGBM] [Info] Number of positive: 3904, number of negative: 67042\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.018755 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.055028 -> initscore=-2.843318\n",
      "[LightGBM] [Info] Start training from score -2.843318\n",
      "[LightGBM] [Info] Number of positive: 2705, number of negative: 68241\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009324 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038128 -> initscore=-3.227944\n",
      "[LightGBM] [Info] Start training from score -3.227944\n",
      "[LightGBM] [Info] Number of positive: 1643, number of negative: 69303\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007005 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.023158 -> initscore=-3.741964\n",
      "[LightGBM] [Info] Start training from score -3.741964\n",
      "[LightGBM] [Info] Number of positive: 6019, number of negative: 64927\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009283 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.084839 -> initscore=-2.378342\n",
      "[LightGBM] [Info] Start training from score -2.378342\n",
      "[LightGBM] [Info] Number of positive: 3042, number of negative: 67904\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007464 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.042878 -> initscore=-3.105580\n",
      "[LightGBM] [Info] Start training from score -3.105580\n",
      "[LightGBM] [Info] Number of positive: 5618, number of negative: 65328\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007058 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.079187 -> initscore=-2.453445\n",
      "[LightGBM] [Info] Start training from score -2.453445\n",
      "[LightGBM] [Info] Number of positive: 1415, number of negative: 69531\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007868 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.019945 -> initscore=-3.894643\n",
      "[LightGBM] [Info] Start training from score -3.894643\n",
      "[LightGBM] [Info] Number of positive: 2126, number of negative: 68820\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.010348 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.029966 -> initscore=-3.477252\n",
      "[LightGBM] [Info] Start training from score -3.477252\n",
      "[LightGBM] [Info] Number of positive: 3927, number of negative: 67019\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007566 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.055352 -> initscore=-2.837100\n",
      "[LightGBM] [Info] Start training from score -2.837100\n",
      "[LightGBM] [Info] Number of positive: 16618, number of negative: 54328\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.014967 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.234234 -> initscore=-1.184553\n",
      "[LightGBM] [Info] Start training from score -1.184553\n",
      "[LightGBM] [Info] Number of positive: 6617, number of negative: 64329\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.014198 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.093268 -> initscore=-2.274368\n",
      "[LightGBM] [Info] Start training from score -2.274368\n",
      "[LightGBM] [Info] Number of positive: 1794, number of negative: 69152\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006871 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.025287 -> initscore=-3.651859\n",
      "[LightGBM] [Info] Start training from score -3.651859\n",
      "[LightGBM] [Info] Number of positive: 2791, number of negative: 68155\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007178 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039340 -> initscore=-3.195385\n",
      "[LightGBM] [Info] Start training from score -3.195385\n",
      "[LightGBM] [Info] Number of positive: 976, number of negative: 69970\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009747 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.013757 -> initscore=-4.272359\n",
      "[LightGBM] [Info] Start training from score -4.272359\n",
      "[LightGBM] [Info] Number of positive: 7589, number of negative: 63357\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007994 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.106969 -> initscore=-2.122086\n",
      "[LightGBM] [Info] Start training from score -2.122086\n",
      "[LightGBM] [Info] Number of positive: 13058, number of negative: 57888\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009398 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.184055 -> initscore=-1.489109\n",
      "[LightGBM] [Info] Start training from score -1.489109\n",
      "[LightGBM] [Info] Number of positive: 6440, number of negative: 64506\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007942 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.090773 -> initscore=-2.304230\n",
      "[LightGBM] [Info] Start training from score -2.304230\n",
      "[LightGBM] [Info] Number of positive: 6241, number of negative: 64705\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008677 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.087968 -> initscore=-2.338698\n",
      "[LightGBM] [Info] Start training from score -2.338698\n",
      "[LightGBM] [Info] Number of positive: 2551, number of negative: 68395\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.016512 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035957 -> initscore=-3.288814\n",
      "[LightGBM] [Info] Start training from score -3.288814\n",
      "[LightGBM] [Info] Number of positive: 3093, number of negative: 67853\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.010216 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.043597 -> initscore=-3.088202\n",
      "[LightGBM] [Info] Start training from score -3.088202\n",
      "[LightGBM] [Info] Number of positive: 2204, number of negative: 68742\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.006676 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031066 -> initscore=-3.440086\n",
      "[LightGBM] [Info] Start training from score -3.440086\n",
      "[LightGBM] [Info] Number of positive: 7839, number of negative: 63107\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.009043 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.110492 -> initscore=-2.085720\n",
      "[LightGBM] [Info] Start training from score -2.085720\n",
      "[LightGBM] [Info] Number of positive: 642, number of negative: 70304\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007433 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.009049 -> initscore=-4.695996\n",
      "[LightGBM] [Info] Start training from score -4.695996\n",
      "[LightGBM] [Info] Number of positive: 10469, number of negative: 60477\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.010996 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.147563 -> initscore=-1.753845\n",
      "[LightGBM] [Info] Start training from score -1.753845\n",
      "[LightGBM] [Info] Number of positive: 6454, number of negative: 64492\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007258 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.090971 -> initscore=-2.301841\n",
      "[LightGBM] [Info] Start training from score -2.301841\n",
      "[LightGBM] [Info] Number of positive: 369, number of negative: 70577\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008534 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005201 -> initscore=-5.253663\n",
      "[LightGBM] [Info] Start training from score -5.253663\n",
      "[LightGBM] [Info] Number of positive: 8636, number of negative: 62310\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.008934 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121726 -> initscore=-1.976182\n",
      "[LightGBM] [Info] Start training from score -1.976182\n",
      "[LightGBM] [Info] Number of positive: 2353, number of negative: 68593\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.007955 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.033166 -> initscore=-3.372499\n",
      "[LightGBM] [Info] Start training from score -3.372499\n",
      "[LightGBM] [Info] Number of positive: 9626, number of negative: 61320\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 73695\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 289\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 289 dense feature groups (19.76 MB) transferred to GPU in 0.010351 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.135681 -> initscore=-1.851638\n",
      "[LightGBM] [Info] Start training from score -1.851638\n",
      "Validation Accuracy: 0.0012, Validation F1 Score: 0.4618\n",
      "Test Accuracy: 0.0012, Test F1 Score: 0.4587\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Préparer les données pour LightGBM\n",
    "X_train_lgb, y_train_lgb = prepare_data_for_rf(X_train_final_train, train_targets.cpu().numpy())\n",
    "X_val_lgb, y_val_lgb = prepare_data_for_rf(X_train_final_val, val_targets.cpu().numpy())\n",
    "X_test_lgb, y_test_lgb = prepare_data_for_rf(X_test_final, y_test_tensor.cpu().numpy())\n",
    "\n",
    "# Initialiser les modèles LightGBM pour chaque label\n",
    "num_labels = y_train_lgb.shape[1]\n",
    "lgb_models = [lgb.LGBMClassifier(objective='binary', n_estimators=100, random_state=42, device='gpu') for _ in range(num_labels)]\n",
    "\n",
    "# Entraîner les modèles\n",
    "for i in range(num_labels):\n",
    "    lgb_models[i].fit(X_train_lgb, y_train_lgb[:, i])\n",
    "\n",
    "# Évaluer les modèles sur l'ensemble de validation\n",
    "y_val_pred_lgb = np.zeros_like(y_val_lgb)\n",
    "for i in range(num_labels):\n",
    "    y_val_pred_lgb[:, i] = lgb_models[i].predict(X_val_lgb)\n",
    "y_val_pred_lgb = (y_val_pred_lgb > 0.5).astype(int)\n",
    "val_accuracy = accuracy_score(y_val_lgb, y_val_pred_lgb)\n",
    "val_f1 = f1_score(y_val_lgb, y_val_pred_lgb, average='weighted')\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# Évaluer les modèles sur l'ensemble de test\n",
    "y_test_pred_lgb = np.zeros_like(y_test_lgb)\n",
    "for i in range(num_labels):\n",
    "    y_test_pred_lgb[:, i] = lgb_models[i].predict(X_test_lgb)\n",
    "y_test_pred_lgb = (y_test_pred_lgb > 0.5).astype(int)\n",
    "test_accuracy = accuracy_score(y_test_lgb, y_test_pred_lgb)\n",
    "test_f1 = f1_score(y_test_lgb, y_test_pred_lgb, average='weighted')\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 569, number of negative: 70377\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014323 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360829\n",
      "[LightGBM] [Info] Start training from score -10.360829\n",
      "[LightGBM] [Info] Number of positive: 3983, number of negative: 66963\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013575 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.314452\n",
      "[LightGBM] [Info] Start training from score -10.314452\n",
      "[LightGBM] [Info] Number of positive: 812, number of negative: 70134\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014923 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358541\n",
      "[LightGBM] [Info] Start training from score -10.358541\n",
      "[LightGBM] [Info] Number of positive: 255, number of negative: 70691\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013963 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360449\n",
      "[LightGBM] [Info] Start training from score -10.360449\n",
      "[LightGBM] [Info] Number of positive: 200, number of negative: 70746\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013734 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358810\n",
      "[LightGBM] [Info] Start training from score -10.358810\n",
      "[LightGBM] [Info] Number of positive: 938, number of negative: 70008\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013979 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.357110\n",
      "[LightGBM] [Info] Start training from score -10.357110\n",
      "[LightGBM] [Info] Number of positive: 402, number of negative: 70544\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013963 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361571\n",
      "[LightGBM] [Info] Start training from score -10.361571\n",
      "[LightGBM] [Info] Number of positive: 1606, number of negative: 69340\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014667 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.348509\n",
      "[LightGBM] [Info] Start training from score -10.348509\n",
      "[LightGBM] [Info] Number of positive: 3237, number of negative: 67709\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014659 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.325403\n",
      "[LightGBM] [Info] Start training from score -10.325403\n",
      "[LightGBM] [Info] Number of positive: 714, number of negative: 70232\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012893 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359561\n",
      "[LightGBM] [Info] Start training from score -10.359561\n",
      "[LightGBM] [Info] Number of positive: 2392, number of negative: 68554\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013910 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.337563\n",
      "[LightGBM] [Info] Start training from score -10.337563\n",
      "[LightGBM] [Info] Number of positive: 6648, number of negative: 64298\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012690 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000035 -> initscore=-10.274064\n",
      "[LightGBM] [Info] Start training from score -10.274064\n",
      "[LightGBM] [Info] Number of positive: 1447, number of negative: 69499\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014852 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.350647\n",
      "[LightGBM] [Info] Start training from score -10.350647\n",
      "[LightGBM] [Info] Number of positive: 4505, number of negative: 66441\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013853 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.306691\n",
      "[LightGBM] [Info] Start training from score -10.306691\n",
      "[LightGBM] [Info] Number of positive: 2925, number of negative: 68021\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015188 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.329927\n",
      "[LightGBM] [Info] Start training from score -10.329927\n",
      "[LightGBM] [Info] Number of positive: 1165, number of negative: 69781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016603 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.354325\n",
      "[LightGBM] [Info] Start training from score -10.354325\n",
      "[LightGBM] [Info] Number of positive: 221, number of negative: 70725\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015327 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359578\n",
      "[LightGBM] [Info] Start training from score -10.359578\n",
      "[LightGBM] [Info] Number of positive: 460, number of negative: 70486\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013921 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361448\n",
      "[LightGBM] [Info] Start training from score -10.361448\n",
      "[LightGBM] [Info] Number of positive: 481, number of negative: 70465\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014975 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361362\n",
      "[LightGBM] [Info] Start training from score -10.361362\n",
      "[LightGBM] [Info] Number of positive: 764, number of negative: 70182\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014846 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359053\n",
      "[LightGBM] [Info] Start training from score -10.359053\n",
      "[LightGBM] [Info] Number of positive: 2989, number of negative: 67957\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015574 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.329002\n",
      "[LightGBM] [Info] Start training from score -10.329002\n",
      "[LightGBM] [Info] Number of positive: 510, number of negative: 70436\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014694 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361214\n",
      "[LightGBM] [Info] Start training from score -10.361214\n",
      "[LightGBM] [Info] Number of positive: 430, number of negative: 70516\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016573 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361536\n",
      "[LightGBM] [Info] Start training from score -10.361536\n",
      "[LightGBM] [Info] Number of positive: 3149, number of negative: 67797\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015023 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.326682\n",
      "[LightGBM] [Info] Start training from score -10.326682\n",
      "[LightGBM] [Info] Number of positive: 2713, number of negative: 68233\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015450 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.332979\n",
      "[LightGBM] [Info] Start training from score -10.332979\n",
      "[LightGBM] [Info] Number of positive: 261, number of negative: 70685\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.021841 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360566\n",
      "[LightGBM] [Info] Start training from score -10.360566\n",
      "[LightGBM] [Info] Number of positive: 1189, number of negative: 69757\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.018294 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.354019\n",
      "[LightGBM] [Info] Start training from score -10.354019\n",
      "[LightGBM] [Info] Number of positive: 10840, number of negative: 60106\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014986 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000037 -> initscore=-10.206774\n",
      "[LightGBM] [Info] Start training from score -10.206774\n",
      "[LightGBM] [Info] Number of positive: 2825, number of negative: 68121\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.022975 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.331369\n",
      "[LightGBM] [Info] Start training from score -10.331369\n",
      "[LightGBM] [Info] Number of positive: 1253, number of negative: 69693\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015514 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.353197\n",
      "[LightGBM] [Info] Start training from score -10.353197\n",
      "[LightGBM] [Info] Number of positive: 458, number of negative: 70488\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016407 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361456\n",
      "[LightGBM] [Info] Start training from score -10.361456\n",
      "[LightGBM] [Info] Number of positive: 731, number of negative: 70215\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013787 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359391\n",
      "[LightGBM] [Info] Start training from score -10.359391\n",
      "[LightGBM] [Info] Number of positive: 273, number of negative: 70673\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013825 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360773\n",
      "[LightGBM] [Info] Start training from score -10.360773\n",
      "[LightGBM] [Info] Number of positive: 1936, number of negative: 69010\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015634 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.343974\n",
      "[LightGBM] [Info] Start training from score -10.343974\n",
      "[LightGBM] [Info] Number of positive: 1087, number of negative: 69859\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015824 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.355305\n",
      "[LightGBM] [Info] Start training from score -10.355305\n",
      "[LightGBM] [Info] Number of positive: 2555, number of negative: 68391\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.025335 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.335242\n",
      "[LightGBM] [Info] Start training from score -10.335242\n",
      "[LightGBM] [Info] Number of positive: 1928, number of negative: 69018\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014570 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.344085\n",
      "[LightGBM] [Info] Start training from score -10.344085\n",
      "[LightGBM] [Info] Number of positive: 1237, number of negative: 69709\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014146 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.353403\n",
      "[LightGBM] [Info] Start training from score -10.353403\n",
      "[LightGBM] [Info] Number of positive: 1764, number of negative: 69182\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.017896 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.346351\n",
      "[LightGBM] [Info] Start training from score -10.346351\n",
      "[LightGBM] [Info] Number of positive: 4949, number of negative: 65997\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015089 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000034 -> initscore=-10.300030\n",
      "[LightGBM] [Info] Start training from score -10.300030\n",
      "[LightGBM] [Info] Number of positive: 302, number of negative: 70644\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014737 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361149\n",
      "[LightGBM] [Info] Start training from score -10.361149\n",
      "[LightGBM] [Info] Number of positive: 717, number of negative: 70229\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015373 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359531\n",
      "[LightGBM] [Info] Start training from score -10.359531\n",
      "[LightGBM] [Info] Number of positive: 1018, number of negative: 69928\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014127 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.356153\n",
      "[LightGBM] [Info] Start training from score -10.356153\n",
      "[LightGBM] [Info] Number of positive: 342, number of negative: 70604\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015154 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361448\n",
      "[LightGBM] [Info] Start training from score -10.361448\n",
      "[LightGBM] [Info] Number of positive: 580, number of negative: 70366\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014522 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360747\n",
      "[LightGBM] [Info] Start training from score -10.360747\n",
      "[LightGBM] [Info] Number of positive: 2097, number of negative: 68849\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013796 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.341726\n",
      "[LightGBM] [Info] Start training from score -10.341726\n",
      "[LightGBM] [Info] Number of positive: 191, number of negative: 70755\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013734 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358408\n",
      "[LightGBM] [Info] Start training from score -10.358408\n",
      "[LightGBM] [Info] Number of positive: 175, number of negative: 70771\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014636 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.357559\n",
      "[LightGBM] [Info] Start training from score -10.357559\n",
      "[LightGBM] [Info] Number of positive: 373, number of negative: 70573\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013581 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361551\n",
      "[LightGBM] [Info] Start training from score -10.361551\n",
      "[LightGBM] [Info] Number of positive: 196, number of negative: 70750\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012858 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358638\n",
      "[LightGBM] [Info] Start training from score -10.358638\n",
      "[LightGBM] [Info] Number of positive: 188, number of negative: 70758\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013584 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358263\n",
      "[LightGBM] [Info] Start training from score -10.358263\n",
      "[LightGBM] [Info] Number of positive: 267, number of negative: 70679\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013712 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360673\n",
      "[LightGBM] [Info] Start training from score -10.360673\n",
      "[LightGBM] [Info] Number of positive: 130, number of negative: 70816\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012871 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.353740\n",
      "[LightGBM] [Info] Start training from score -10.353740\n",
      "[LightGBM] [Info] Number of positive: 785, number of negative: 70161\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013825 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358831\n",
      "[LightGBM] [Info] Start training from score -10.358831\n",
      "[LightGBM] [Info] Number of positive: 1107, number of negative: 69839\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014217 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.355055\n",
      "[LightGBM] [Info] Start training from score -10.355055\n",
      "[LightGBM] [Info] Number of positive: 1218, number of negative: 69728\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013798 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.353648\n",
      "[LightGBM] [Info] Start training from score -10.353648\n",
      "[LightGBM] [Info] Number of positive: 194, number of negative: 70752\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013566 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358548\n",
      "[LightGBM] [Info] Start training from score -10.358548\n",
      "[LightGBM] [Info] Number of positive: 393, number of negative: 70553\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013480 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361572\n",
      "[LightGBM] [Info] Start training from score -10.361572\n",
      "[LightGBM] [Info] Number of positive: 359, number of negative: 70587\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013365 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361516\n",
      "[LightGBM] [Info] Start training from score -10.361516\n",
      "[LightGBM] [Info] Number of positive: 546, number of negative: 70400\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013536 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360991\n",
      "[LightGBM] [Info] Start training from score -10.360991\n",
      "[LightGBM] [Info] Number of positive: 265, number of negative: 70681\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014351 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360638\n",
      "[LightGBM] [Info] Start training from score -10.360638\n",
      "[LightGBM] [Info] Number of positive: 1217, number of negative: 69729\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013757 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.353661\n",
      "[LightGBM] [Info] Start training from score -10.353661\n",
      "[LightGBM] [Info] Number of positive: 113, number of negative: 70833\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015592 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.351365\n",
      "[LightGBM] [Info] Start training from score -10.351365\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 782, number of negative: 70164\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.019054 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358863\n",
      "[LightGBM] [Info] Start training from score -10.358863\n",
      "[LightGBM] [Info] Number of positive: 398, number of negative: 70548\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012874 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361572\n",
      "[LightGBM] [Info] Start training from score -10.361572\n",
      "[LightGBM] [Info] Number of positive: 452, number of negative: 70494\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015591 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361476\n",
      "[LightGBM] [Info] Start training from score -10.361476\n",
      "[LightGBM] [Info] Number of positive: 1510, number of negative: 69436\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014274 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.349804\n",
      "[LightGBM] [Info] Start training from score -10.349804\n",
      "[LightGBM] [Info] Number of positive: 563, number of negative: 70383\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014038 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360872\n",
      "[LightGBM] [Info] Start training from score -10.360872\n",
      "[LightGBM] [Info] Number of positive: 413, number of negative: 70533\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.023992 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361563\n",
      "[LightGBM] [Info] Start training from score -10.361563\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 70349\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.024580 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360614\n",
      "[LightGBM] [Info] Start training from score -10.360614\n",
      "[LightGBM] [Info] Number of positive: 1503, number of negative: 69443\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016452 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.349898\n",
      "[LightGBM] [Info] Start training from score -10.349898\n",
      "[LightGBM] [Info] Number of positive: 80, number of negative: 70866\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015007 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.343535\n",
      "[LightGBM] [Info] Start training from score -10.343535\n",
      "[LightGBM] [Info] Number of positive: 116, number of negative: 70830\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014974 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.351840\n",
      "[LightGBM] [Info] Start training from score -10.351840\n",
      "[LightGBM] [Info] Number of positive: 599, number of negative: 70347\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014534 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360598\n",
      "[LightGBM] [Info] Start training from score -10.360598\n",
      "[LightGBM] [Info] Number of positive: 312, number of negative: 70634\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.017187 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361244\n",
      "[LightGBM] [Info] Start training from score -10.361244\n",
      "[LightGBM] [Info] Number of positive: 223, number of negative: 70723\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014617 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359641\n",
      "[LightGBM] [Info] Start training from score -10.359641\n",
      "[LightGBM] [Info] Number of positive: 276, number of negative: 70670\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013195 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360819\n",
      "[LightGBM] [Info] Start training from score -10.360819\n",
      "[LightGBM] [Info] Number of positive: 1169, number of negative: 69777\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013541 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.354274\n",
      "[LightGBM] [Info] Start training from score -10.354274\n",
      "[LightGBM] [Info] Number of positive: 372, number of negative: 70574\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012952 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361549\n",
      "[LightGBM] [Info] Start training from score -10.361549\n",
      "[LightGBM] [Info] Number of positive: 285, number of negative: 70661\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013163 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360948\n",
      "[LightGBM] [Info] Start training from score -10.360948\n",
      "[LightGBM] [Info] Number of positive: 1240, number of negative: 69706\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012487 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.353365\n",
      "[LightGBM] [Info] Start training from score -10.353365\n",
      "[LightGBM] [Info] Number of positive: 4730, number of negative: 66216\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015395 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000034 -> initscore=-10.303322\n",
      "[LightGBM] [Info] Start training from score -10.303322\n",
      "[LightGBM] [Info] Number of positive: 1036, number of negative: 69910\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015164 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.355934\n",
      "[LightGBM] [Info] Start training from score -10.355934\n",
      "[LightGBM] [Info] Number of positive: 247, number of negative: 70699\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015511 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360278\n",
      "[LightGBM] [Info] Start training from score -10.360278\n",
      "[LightGBM] [Info] Number of positive: 1172, number of negative: 69774\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014446 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.354236\n",
      "[LightGBM] [Info] Start training from score -10.354236\n",
      "[LightGBM] [Info] Number of positive: 345, number of negative: 70601\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014438 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361462\n",
      "[LightGBM] [Info] Start training from score -10.361462\n",
      "[LightGBM] [Info] Number of positive: 2243, number of negative: 68703\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.019718 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.339672\n",
      "[LightGBM] [Info] Start training from score -10.339672\n",
      "[LightGBM] [Info] Number of positive: 3114, number of negative: 67832\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013649 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.327190\n",
      "[LightGBM] [Info] Start training from score -10.327190\n",
      "[LightGBM] [Info] Number of positive: 132, number of negative: 70814\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013060 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.353975\n",
      "[LightGBM] [Info] Start training from score -10.353975\n",
      "[LightGBM] [Info] Number of positive: 1776, number of negative: 69170\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013303 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.346186\n",
      "[LightGBM] [Info] Start training from score -10.346186\n",
      "[LightGBM] [Info] Number of positive: 327, number of negative: 70619\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014149 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361360\n",
      "[LightGBM] [Info] Start training from score -10.361360\n",
      "[LightGBM] [Info] Number of positive: 1724, number of negative: 69222\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013342 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.346900\n",
      "[LightGBM] [Info] Start training from score -10.346900\n",
      "[LightGBM] [Info] Number of positive: 10851, number of negative: 60095\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013154 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000037 -> initscore=-10.206591\n",
      "[LightGBM] [Info] Start training from score -10.206591\n",
      "[LightGBM] [Info] Number of positive: 1599, number of negative: 69347\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014520 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.348603\n",
      "[LightGBM] [Info] Start training from score -10.348603\n",
      "[LightGBM] [Info] Number of positive: 821, number of negative: 70125\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.017886 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358442\n",
      "[LightGBM] [Info] Start training from score -10.358442\n",
      "[LightGBM] [Info] Number of positive: 15828, number of negative: 55118\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012950 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000040 -> initscore=-10.120205\n",
      "[LightGBM] [Info] Start training from score -10.120205\n",
      "[LightGBM] [Info] Number of positive: 359, number of negative: 70587\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.020778 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361516\n",
      "[LightGBM] [Info] Start training from score -10.361516\n",
      "[LightGBM] [Info] Number of positive: 663, number of negative: 70283\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013235 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360047\n",
      "[LightGBM] [Info] Start training from score -10.360047\n",
      "[LightGBM] [Info] Number of positive: 299, number of negative: 70647\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014584 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361117\n",
      "[LightGBM] [Info] Start training from score -10.361117\n",
      "[LightGBM] [Info] Number of positive: 863, number of negative: 70083\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.018323 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.357975\n",
      "[LightGBM] [Info] Start training from score -10.357975\n",
      "[LightGBM] [Info] Number of positive: 9132, number of negative: 61814\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015536 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000036 -> initscore=-10.234756\n",
      "[LightGBM] [Info] Start training from score -10.234756\n",
      "[LightGBM] [Info] Number of positive: 1286, number of negative: 69660\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013999 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.352769\n",
      "[LightGBM] [Info] Start training from score -10.352769\n",
      "[LightGBM] [Info] Number of positive: 3730, number of negative: 67216\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013075 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.318185\n",
      "[LightGBM] [Info] Start training from score -10.318185\n",
      "[LightGBM] [Info] Number of positive: 94, number of negative: 70852\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013772 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.347577\n",
      "[LightGBM] [Info] Start training from score -10.347577\n",
      "[LightGBM] [Info] Number of positive: 56, number of negative: 70890\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014279 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.331574\n",
      "[LightGBM] [Info] Start training from score -10.331574\n",
      "[LightGBM] [Info] Number of positive: 364, number of negative: 70582\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016007 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361530\n",
      "[LightGBM] [Info] Start training from score -10.361530\n",
      "[LightGBM] [Info] Number of positive: 1692, number of negative: 69254\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014448 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.347338\n",
      "[LightGBM] [Info] Start training from score -10.347338\n",
      "[LightGBM] [Info] Number of positive: 1590, number of negative: 69356\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013389 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.348725\n",
      "[LightGBM] [Info] Start training from score -10.348725\n",
      "[LightGBM] [Info] Number of positive: 428, number of negative: 70518\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012995 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361540\n",
      "[LightGBM] [Info] Start training from score -10.361540\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 70148\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014568 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358692\n",
      "[LightGBM] [Info] Start training from score -10.358692\n",
      "[LightGBM] [Info] Number of positive: 266, number of negative: 70680\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012510 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360656\n",
      "[LightGBM] [Info] Start training from score -10.360656\n",
      "[LightGBM] [Info] Number of positive: 913, number of negative: 70033\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013638 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.357403\n",
      "[LightGBM] [Info] Start training from score -10.357403\n",
      "[LightGBM] [Info] Number of positive: 218, number of negative: 70728\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012836 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359481\n",
      "[LightGBM] [Info] Start training from score -10.359481\n",
      "[LightGBM] [Info] Number of positive: 3594, number of negative: 67352\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013534 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.320184\n",
      "[LightGBM] [Info] Start training from score -10.320184\n",
      "[LightGBM] [Info] Number of positive: 315, number of negative: 70631\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012874 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361270\n",
      "[LightGBM] [Info] Start training from score -10.361270\n",
      "[LightGBM] [Info] Number of positive: 16047, number of negative: 54899\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013871 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000040 -> initscore=-10.116226\n",
      "[LightGBM] [Info] Start training from score -10.116226\n",
      "[LightGBM] [Info] Number of positive: 14078, number of negative: 56868\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015582 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000039 -> initscore=-10.151444\n",
      "[LightGBM] [Info] Start training from score -10.151444\n",
      "[LightGBM] [Info] Number of positive: 374, number of negative: 70572\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.022072 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361553\n",
      "[LightGBM] [Info] Start training from score -10.361553\n",
      "[LightGBM] [Info] Number of positive: 20185, number of negative: 50761\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013071 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000044 -> initscore=-10.037887\n",
      "[LightGBM] [Info] Start training from score -10.037887\n",
      "[LightGBM] [Info] Number of positive: 785, number of negative: 70161\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014578 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358831\n",
      "[LightGBM] [Info] Start training from score -10.358831\n",
      "[LightGBM] [Info] Number of positive: 612, number of negative: 70334\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016005 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360493\n",
      "[LightGBM] [Info] Start training from score -10.360493\n",
      "[LightGBM] [Info] Number of positive: 501, number of negative: 70445\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014832 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361263\n",
      "[LightGBM] [Info] Start training from score -10.361263\n",
      "[LightGBM] [Info] Number of positive: 4673, number of negative: 66273\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014623 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.304177\n",
      "[LightGBM] [Info] Start training from score -10.304177\n",
      "[LightGBM] [Info] Number of positive: 1391, number of negative: 69555\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014112 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.351391\n",
      "[LightGBM] [Info] Start training from score -10.351391\n",
      "[LightGBM] [Info] Number of positive: 237, number of negative: 70709\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.020999 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360037\n",
      "[LightGBM] [Info] Start training from score -10.360037\n",
      "[LightGBM] [Info] Number of positive: 3645, number of negative: 67301\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013226 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.319435\n",
      "[LightGBM] [Info] Start training from score -10.319435\n",
      "[LightGBM] [Info] Number of positive: 1911, number of negative: 69035\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014759 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.344321\n",
      "[LightGBM] [Info] Start training from score -10.344321\n",
      "[LightGBM] [Info] Number of positive: 633, number of negative: 70313\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013915 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360315\n",
      "[LightGBM] [Info] Start training from score -10.360315\n",
      "[LightGBM] [Info] Number of positive: 1903, number of negative: 69043\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015192 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.344432\n",
      "[LightGBM] [Info] Start training from score -10.344432\n",
      "[LightGBM] [Info] Number of positive: 1961, number of negative: 68985\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014859 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.343626\n",
      "[LightGBM] [Info] Start training from score -10.343626\n",
      "[LightGBM] [Info] Number of positive: 1795, number of negative: 69151\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014577 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.345925\n",
      "[LightGBM] [Info] Start training from score -10.345925\n",
      "[LightGBM] [Info] Number of positive: 701, number of negative: 70245\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013297 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359688\n",
      "[LightGBM] [Info] Start training from score -10.359688\n",
      "[LightGBM] [Info] Number of positive: 1908, number of negative: 69038\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013276 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.344363\n",
      "[LightGBM] [Info] Start training from score -10.344363\n",
      "[LightGBM] [Info] Number of positive: 562, number of negative: 70384\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014644 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360879\n",
      "[LightGBM] [Info] Start training from score -10.360879\n",
      "[LightGBM] [Info] Number of positive: 228, number of negative: 70718\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013347 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359791\n",
      "[LightGBM] [Info] Start training from score -10.359791\n",
      "[LightGBM] [Info] Number of positive: 2140, number of negative: 68806\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012780 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.341123\n",
      "[LightGBM] [Info] Start training from score -10.341123\n",
      "[LightGBM] [Info] Number of positive: 316, number of negative: 70630\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013307 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361278\n",
      "[LightGBM] [Info] Start training from score -10.361278\n",
      "[LightGBM] [Info] Number of positive: 408, number of negative: 70538\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013692 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361568\n",
      "[LightGBM] [Info] Start training from score -10.361568\n",
      "[LightGBM] [Info] Number of positive: 2343, number of negative: 68603\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014274 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.338258\n",
      "[LightGBM] [Info] Start training from score -10.338258\n",
      "[LightGBM] [Info] Number of positive: 498, number of negative: 70448\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013602 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361279\n",
      "[LightGBM] [Info] Start training from score -10.361279\n",
      "[LightGBM] [Info] Number of positive: 218, number of negative: 70728\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013191 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359481\n",
      "[LightGBM] [Info] Start training from score -10.359481\n",
      "[LightGBM] [Info] Number of positive: 6148, number of negative: 64798\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015216 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000034 -> initscore=-10.281783\n",
      "[LightGBM] [Info] Start training from score -10.281783\n",
      "[LightGBM] [Info] Number of positive: 830, number of negative: 70116\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015135 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358343\n",
      "[LightGBM] [Info] Start training from score -10.358343\n",
      "[LightGBM] [Info] Number of positive: 684, number of negative: 70262\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014563 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359851\n",
      "[LightGBM] [Info] Start training from score -10.359851\n",
      "[LightGBM] [Info] Number of positive: 1826, number of negative: 69120\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012542 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.345497\n",
      "[LightGBM] [Info] Start training from score -10.345497\n",
      "[LightGBM] [Info] Number of positive: 73, number of negative: 70873\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015203 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.340895\n",
      "[LightGBM] [Info] Start training from score -10.340895\n",
      "[LightGBM] [Info] Number of positive: 207, number of negative: 70739\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015855 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359090\n",
      "[LightGBM] [Info] Start training from score -10.359090\n",
      "[LightGBM] [Info] Number of positive: 119, number of negative: 70827\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014818 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.352289\n",
      "[LightGBM] [Info] Start training from score -10.352289\n",
      "[LightGBM] [Info] Number of positive: 830, number of negative: 70116\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014536 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358343\n",
      "[LightGBM] [Info] Start training from score -10.358343\n",
      "[LightGBM] [Info] Number of positive: 1098, number of negative: 69848\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014168 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.355168\n",
      "[LightGBM] [Info] Start training from score -10.355168\n",
      "[LightGBM] [Info] Number of positive: 942, number of negative: 70004\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012429 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.357063\n",
      "[LightGBM] [Info] Start training from score -10.357063\n",
      "[LightGBM] [Info] Number of positive: 245, number of negative: 70701\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014550 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360232\n",
      "[LightGBM] [Info] Start training from score -10.360232\n",
      "[LightGBM] [Info] Number of positive: 178, number of negative: 70768\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015181 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.357734\n",
      "[LightGBM] [Info] Start training from score -10.357734\n",
      "[LightGBM] [Info] Number of positive: 419, number of negative: 70527\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013757 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361556\n",
      "[LightGBM] [Info] Start training from score -10.361556\n",
      "[LightGBM] [Info] Number of positive: 1165, number of negative: 69781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016273 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.354325\n",
      "[LightGBM] [Info] Start training from score -10.354325\n",
      "[LightGBM] [Info] Number of positive: 17116, number of negative: 53830\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014665 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000041 -> initscore=-10.096570\n",
      "[LightGBM] [Info] Start training from score -10.096570\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 70781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014892 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.356922\n",
      "[LightGBM] [Info] Start training from score -10.356922\n",
      "[LightGBM] [Info] Number of positive: 1844, number of negative: 69102\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012879 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.345249\n",
      "[LightGBM] [Info] Start training from score -10.345249\n",
      "[LightGBM] [Info] Number of positive: 5439, number of negative: 65507\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014084 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000034 -> initscore=-10.292618\n",
      "[LightGBM] [Info] Start training from score -10.292618\n",
      "[LightGBM] [Info] Number of positive: 834, number of negative: 70112\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014434 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358299\n",
      "[LightGBM] [Info] Start training from score -10.358299\n",
      "[LightGBM] [Info] Number of positive: 239, number of negative: 70707\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014627 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360087\n",
      "[LightGBM] [Info] Start training from score -10.360087\n",
      "[LightGBM] [Info] Number of positive: 7125, number of negative: 63821\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014412 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000035 -> initscore=-10.266640\n",
      "[LightGBM] [Info] Start training from score -10.266640\n",
      "[LightGBM] [Info] Number of positive: 3261, number of negative: 67685\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013288 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.325053\n",
      "[LightGBM] [Info] Start training from score -10.325053\n",
      "[LightGBM] [Info] Number of positive: 760, number of negative: 70186\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014053 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359094\n",
      "[LightGBM] [Info] Start training from score -10.359094\n",
      "[LightGBM] [Info] Number of positive: 2836, number of negative: 68110\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015182 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.331210\n",
      "[LightGBM] [Info] Start training from score -10.331210\n",
      "[LightGBM] [Info] Number of positive: 342, number of negative: 70604\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012810 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361448\n",
      "[LightGBM] [Info] Start training from score -10.361448\n",
      "[LightGBM] [Info] Number of positive: 467, number of negative: 70479\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014477 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361422\n",
      "[LightGBM] [Info] Start training from score -10.361422\n",
      "[LightGBM] [Info] Number of positive: 522, number of negative: 70424\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014747 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361144\n",
      "[LightGBM] [Info] Start training from score -10.361144\n",
      "[LightGBM] [Info] Number of positive: 697, number of negative: 70249\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014607 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359727\n",
      "[LightGBM] [Info] Start training from score -10.359727\n",
      "[LightGBM] [Info] Number of positive: 2950, number of negative: 67996\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015401 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.329566\n",
      "[LightGBM] [Info] Start training from score -10.329566\n",
      "[LightGBM] [Info] Number of positive: 216, number of negative: 70730\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015628 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359414\n",
      "[LightGBM] [Info] Start training from score -10.359414\n",
      "[LightGBM] [Info] Number of positive: 92, number of negative: 70854\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013980 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.347080\n",
      "[LightGBM] [Info] Start training from score -10.347080\n",
      "[LightGBM] [Info] Number of positive: 11921, number of negative: 59025\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013421 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000038 -> initscore=-10.188644\n",
      "[LightGBM] [Info] Start training from score -10.188644\n",
      "[LightGBM] [Info] Number of positive: 3493, number of negative: 67453\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014325 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.321665\n",
      "[LightGBM] [Info] Start training from score -10.321665\n",
      "[LightGBM] [Info] Number of positive: 4608, number of negative: 66338\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013378 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.305150\n",
      "[LightGBM] [Info] Start training from score -10.305150\n",
      "[LightGBM] [Info] Number of positive: 4504, number of negative: 66442\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012954 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.306706\n",
      "[LightGBM] [Info] Start training from score -10.306706\n",
      "[LightGBM] [Info] Number of positive: 1315, number of negative: 69631\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012194 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.352390\n",
      "[LightGBM] [Info] Start training from score -10.352390\n",
      "[LightGBM] [Info] Number of positive: 21933, number of negative: 49013\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.021555 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000045 -> initscore=-10.002853\n",
      "[LightGBM] [Info] Start training from score -10.002853\n",
      "[LightGBM] [Info] Number of positive: 225, number of negative: 70721\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013845 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359702\n",
      "[LightGBM] [Info] Start training from score -10.359702\n",
      "[LightGBM] [Info] Number of positive: 334, number of negative: 70612\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.023884 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361404\n",
      "[LightGBM] [Info] Start training from score -10.361404\n",
      "[LightGBM] [Info] Number of positive: 1748, number of negative: 69198\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016599 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.346571\n",
      "[LightGBM] [Info] Start training from score -10.346571\n",
      "[LightGBM] [Info] Number of positive: 528, number of negative: 70418\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012444 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361107\n",
      "[LightGBM] [Info] Start training from score -10.361107\n",
      "[LightGBM] [Info] Number of positive: 1304, number of negative: 69642\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014211 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.352534\n",
      "[LightGBM] [Info] Start training from score -10.352534\n",
      "[LightGBM] [Info] Number of positive: 344, number of negative: 70602\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014759 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361457\n",
      "[LightGBM] [Info] Start training from score -10.361457\n",
      "[LightGBM] [Info] Number of positive: 596, number of negative: 70350\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013964 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360622\n",
      "[LightGBM] [Info] Start training from score -10.360622\n",
      "[LightGBM] [Info] Number of positive: 2624, number of negative: 68322\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015150 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.334255\n",
      "[LightGBM] [Info] Start training from score -10.334255\n",
      "[LightGBM] [Info] Number of positive: 325, number of negative: 70621\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013492 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361347\n",
      "[LightGBM] [Info] Start training from score -10.361347\n",
      "[LightGBM] [Info] Number of positive: 302, number of negative: 70644\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.025151 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361149\n",
      "[LightGBM] [Info] Start training from score -10.361149\n",
      "[LightGBM] [Info] Number of positive: 1600, number of negative: 69346\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013398 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.348590\n",
      "[LightGBM] [Info] Start training from score -10.348590\n",
      "[LightGBM] [Info] Number of positive: 578, number of negative: 70368\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015106 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360762\n",
      "[LightGBM] [Info] Start training from score -10.360762\n",
      "[LightGBM] [Info] Number of positive: 1212, number of negative: 69734\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012659 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.353725\n",
      "[LightGBM] [Info] Start training from score -10.353725\n",
      "[LightGBM] [Info] Number of positive: 335, number of negative: 70611\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013349 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361410\n",
      "[LightGBM] [Info] Start training from score -10.361410\n",
      "[LightGBM] [Info] Number of positive: 3004, number of negative: 67942\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014319 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.328785\n",
      "[LightGBM] [Info] Start training from score -10.328785\n",
      "[LightGBM] [Info] Number of positive: 16363, number of negative: 54583\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013050 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000041 -> initscore=-10.110456\n",
      "[LightGBM] [Info] Start training from score -10.110456\n",
      "[LightGBM] [Info] Number of positive: 756, number of negative: 70190\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013346 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359136\n",
      "[LightGBM] [Info] Start training from score -10.359136\n",
      "[LightGBM] [Info] Number of positive: 240, number of negative: 70706\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014706 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360112\n",
      "[LightGBM] [Info] Start training from score -10.360112\n",
      "[LightGBM] [Info] Number of positive: 1587, number of negative: 69359\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014470 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.348766\n",
      "[LightGBM] [Info] Start training from score -10.348766\n",
      "[LightGBM] [Info] Number of positive: 355, number of negative: 70591\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016118 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361502\n",
      "[LightGBM] [Info] Start training from score -10.361502\n",
      "[LightGBM] [Info] Number of positive: 529, number of negative: 70417\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013584 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361101\n",
      "[LightGBM] [Info] Start training from score -10.361101\n",
      "[LightGBM] [Info] Number of positive: 1728, number of negative: 69218\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012710 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.346845\n",
      "[LightGBM] [Info] Start training from score -10.346845\n",
      "[LightGBM] [Info] Number of positive: 1772, number of negative: 69174\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.025720 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.346241\n",
      "[LightGBM] [Info] Start training from score -10.346241\n",
      "[LightGBM] [Info] Number of positive: 834, number of negative: 70112\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013471 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358299\n",
      "[LightGBM] [Info] Start training from score -10.358299\n",
      "[LightGBM] [Info] Number of positive: 2759, number of negative: 68187\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012620 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.332319\n",
      "[LightGBM] [Info] Start training from score -10.332319\n",
      "[LightGBM] [Info] Number of positive: 2227, number of negative: 68719\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013726 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.339898\n",
      "[LightGBM] [Info] Start training from score -10.339898\n",
      "[LightGBM] [Info] Number of positive: 4294, number of negative: 66652\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015392 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.309837\n",
      "[LightGBM] [Info] Start training from score -10.309837\n",
      "[LightGBM] [Info] Number of positive: 6642, number of negative: 64304\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014193 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000035 -> initscore=-10.274157\n",
      "[LightGBM] [Info] Start training from score -10.274157\n",
      "[LightGBM] [Info] Number of positive: 8139, number of negative: 62807\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014455 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000035 -> initscore=-10.250663\n",
      "[LightGBM] [Info] Start training from score -10.250663\n",
      "[LightGBM] [Info] Number of positive: 1253, number of negative: 69693\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014278 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.353197\n",
      "[LightGBM] [Info] Start training from score -10.353197\n",
      "[LightGBM] [Info] Number of positive: 17756, number of negative: 53190\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014075 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000042 -> initscore=-10.084615\n",
      "[LightGBM] [Info] Start training from score -10.084615\n",
      "[LightGBM] [Info] Number of positive: 2028, number of negative: 68918\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014270 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.342692\n",
      "[LightGBM] [Info] Start training from score -10.342692\n",
      "[LightGBM] [Info] Number of positive: 2449, number of negative: 68497\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015765 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.336753\n",
      "[LightGBM] [Info] Start training from score -10.336753\n",
      "[LightGBM] [Info] Number of positive: 13690, number of negative: 57256\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013664 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000039 -> initscore=-10.158239\n",
      "[LightGBM] [Info] Start training from score -10.158239\n",
      "[LightGBM] [Info] Number of positive: 3480, number of negative: 67466\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014868 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.321855\n",
      "[LightGBM] [Info] Start training from score -10.321855\n",
      "[LightGBM] [Info] Number of positive: 1544, number of negative: 69402\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014705 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.349347\n",
      "[LightGBM] [Info] Start training from score -10.349347\n",
      "[LightGBM] [Info] Number of positive: 6645, number of negative: 64301\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013927 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000035 -> initscore=-10.274110\n",
      "[LightGBM] [Info] Start training from score -10.274110\n",
      "[LightGBM] [Info] Number of positive: 1430, number of negative: 69516\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014010 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.350873\n",
      "[LightGBM] [Info] Start training from score -10.350873\n",
      "[LightGBM] [Info] Number of positive: 4073, number of negative: 66873\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016717 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.313119\n",
      "[LightGBM] [Info] Start training from score -10.313119\n",
      "[LightGBM] [Info] Number of positive: 9364, number of negative: 61582\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014616 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000036 -> initscore=-10.231002\n",
      "[LightGBM] [Info] Start training from score -10.231002\n",
      "[LightGBM] [Info] Number of positive: 1548, number of negative: 69398\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013542 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.349293\n",
      "[LightGBM] [Info] Start training from score -10.349293\n",
      "[LightGBM] [Info] Number of positive: 3904, number of negative: 67042\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013999 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.315620\n",
      "[LightGBM] [Info] Start training from score -10.315620\n",
      "[LightGBM] [Info] Number of positive: 2705, number of negative: 68241\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013636 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.333094\n",
      "[LightGBM] [Info] Start training from score -10.333094\n",
      "[LightGBM] [Info] Number of positive: 1643, number of negative: 69303\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012424 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.348006\n",
      "[LightGBM] [Info] Start training from score -10.348006\n",
      "[LightGBM] [Info] Number of positive: 6019, number of negative: 64927\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014339 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000034 -> initscore=-10.283764\n",
      "[LightGBM] [Info] Start training from score -10.283764\n",
      "[LightGBM] [Info] Number of positive: 3042, number of negative: 67904\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013730 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.328234\n",
      "[LightGBM] [Info] Start training from score -10.328234\n",
      "[LightGBM] [Info] Number of positive: 5618, number of negative: 65328\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012771 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000034 -> initscore=-10.289895\n",
      "[LightGBM] [Info] Start training from score -10.289895\n",
      "[LightGBM] [Info] Number of positive: 1415, number of negative: 69531\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015953 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.351073\n",
      "[LightGBM] [Info] Start training from score -10.351073\n",
      "[LightGBM] [Info] Number of positive: 2126, number of negative: 68820\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014290 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.341319\n",
      "[LightGBM] [Info] Start training from score -10.341319\n",
      "[LightGBM] [Info] Number of positive: 3927, number of negative: 67019\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015151 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.315280\n",
      "[LightGBM] [Info] Start training from score -10.315280\n",
      "[LightGBM] [Info] Number of positive: 16618, number of negative: 54328\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014447 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000041 -> initscore=-10.105775\n",
      "[LightGBM] [Info] Start training from score -10.105775\n",
      "[LightGBM] [Info] Number of positive: 6617, number of negative: 64329\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014294 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000034 -> initscore=-10.274544\n",
      "[LightGBM] [Info] Start training from score -10.274544\n",
      "[LightGBM] [Info] Number of positive: 1794, number of negative: 69152\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012869 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.345939\n",
      "[LightGBM] [Info] Start training from score -10.345939\n",
      "[LightGBM] [Info] Number of positive: 2791, number of negative: 68155\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013599 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.331858\n",
      "[LightGBM] [Info] Start training from score -10.331858\n",
      "[LightGBM] [Info] Number of positive: 976, number of negative: 69970\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013899 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.356660\n",
      "[LightGBM] [Info] Start training from score -10.356660\n",
      "[LightGBM] [Info] Number of positive: 7589, number of negative: 63357\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014127 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000035 -> initscore=-10.259362\n",
      "[LightGBM] [Info] Start training from score -10.259362\n",
      "[LightGBM] [Info] Number of positive: 13058, number of negative: 57888\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013676 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000038 -> initscore=-10.169209\n",
      "[LightGBM] [Info] Start training from score -10.169209\n",
      "[LightGBM] [Info] Number of positive: 6440, number of negative: 64506\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013091 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000034 -> initscore=-10.277283\n",
      "[LightGBM] [Info] Start training from score -10.277283\n",
      "[LightGBM] [Info] Number of positive: 6241, number of negative: 64705\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.021106 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000034 -> initscore=-10.280352\n",
      "[LightGBM] [Info] Start training from score -10.280352\n",
      "[LightGBM] [Info] Number of positive: 2551, number of negative: 68395\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012171 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.335299\n",
      "[LightGBM] [Info] Start training from score -10.335299\n",
      "[LightGBM] [Info] Number of positive: 3093, number of negative: 67853\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015537 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.327495\n",
      "[LightGBM] [Info] Start training from score -10.327495\n",
      "[LightGBM] [Info] Number of positive: 2204, number of negative: 68742\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016255 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.340222\n",
      "[LightGBM] [Info] Start training from score -10.340222\n",
      "[LightGBM] [Info] Number of positive: 7839, number of negative: 63107\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013015 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000035 -> initscore=-10.255418\n",
      "[LightGBM] [Info] Start training from score -10.255418\n",
      "[LightGBM] [Info] Number of positive: 642, number of negative: 70304\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014969 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360236\n",
      "[LightGBM] [Info] Start training from score -10.360236\n",
      "[LightGBM] [Info] Number of positive: 10469, number of negative: 60477\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.021831 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000037 -> initscore=-10.212920\n",
      "[LightGBM] [Info] Start training from score -10.212920\n",
      "[LightGBM] [Info] Number of positive: 6454, number of negative: 64492\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013662 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000034 -> initscore=-10.277066\n",
      "[LightGBM] [Info] Start training from score -10.277066\n",
      "[LightGBM] [Info] Number of positive: 369, number of negative: 70577\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014477 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361543\n",
      "[LightGBM] [Info] Start training from score -10.361543\n",
      "[LightGBM] [Info] Number of positive: 8636, number of negative: 62310\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013025 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000036 -> initscore=-10.242734\n",
      "[LightGBM] [Info] Start training from score -10.242734\n",
      "[LightGBM] [Info] Number of positive: 2353, number of negative: 68593\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013724 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.338116\n",
      "[LightGBM] [Info] Start training from score -10.338116\n",
      "[LightGBM] [Info] Number of positive: 9626, number of negative: 61320\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012480 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000036 -> initscore=-10.226745\n",
      "[LightGBM] [Info] Start training from score -10.226745\n",
      "Validation Accuracy: 0.0001, Validation F1 Score: 0.2312\n",
      "Test Accuracy: 0.0000, Test F1 Score: 0.2227\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Préparer les données pour LightGBM\n",
    "X_train_lgb, y_train_lgb = prepare_data_for_rf(X_train_final_train, train_targets.cpu().numpy())\n",
    "X_val_lgb, y_val_lgb = prepare_data_for_rf(X_train_final_val, val_targets.cpu().numpy())\n",
    "X_test_lgb, y_test_lgb = prepare_data_for_rf(X_test_final, y_test_tensor.cpu().numpy())\n",
    "\n",
    "# Initialiser les modèles LightGBM pour chaque label\n",
    "num_labels = y_train_lgb.shape[1]\n",
    "lgb_models = [lgb.LGBMClassifier(objective='binary', n_estimators=100, random_state=42, device='gpu') for _ in range(num_labels)]\n",
    "\n",
    "# Calculer les poids de classe\n",
    "class_weights = np.sum(y_train_lgb, axis=0) / y_train_lgb.shape[0]\n",
    "class_weights = 1 / class_weights\n",
    "class_weights = class_weights / np.sum(class_weights)\n",
    "\n",
    "# Entraîner les modèles\n",
    "for i in range(num_labels):\n",
    "    sample_weight = np.where(y_train_lgb[:, i] == 1, class_weights[i], 1 - class_weights[i])\n",
    "    lgb_models[i].fit(X_train_lgb, y_train_lgb[:, i], sample_weight=sample_weight)\n",
    "\n",
    "# Évaluer les modèles sur l'ensemble de validation\n",
    "y_val_pred_lgb = np.zeros_like(y_val_lgb)\n",
    "for i in range(num_labels):\n",
    "    y_val_pred_lgb[:, i] = lgb_models[i].predict(X_val_lgb)\n",
    "y_val_pred_lgb = (y_val_pred_lgb > 0.5).astype(int)\n",
    "val_accuracy = accuracy_score(y_val_lgb, y_val_pred_lgb)\n",
    "val_f1 = f1_score(y_val_lgb, y_val_pred_lgb, average='weighted')\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# Évaluer les modèles sur l'ensemble de test\n",
    "y_test_pred_lgb = np.zeros_like(y_test_lgb)\n",
    "for i in range(num_labels):\n",
    "    y_test_pred_lgb[:, i] = lgb_models[i].predict(X_test_lgb)\n",
    "y_test_pred_lgb = (y_test_pred_lgb > 0.5).astype(int)\n",
    "test_accuracy = accuracy_score(y_test_lgb, y_test_pred_lgb)\n",
    "test_f1 = f1_score(y_test_lgb, y_test_pred_lgb, average='weighted')\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def prepare_data_for_rf(X_train_final, y_train):\n",
    "    X_train_concat = np.concatenate(\n",
    "        [\n",
    "            X_train_final[\"X_genres\"].cpu().numpy(),\n",
    "            X_train_final[\"X_instruments\"].cpu().numpy(),\n",
    "            X_train_final[\"X_moods\"].cpu().numpy(),\n",
    "            X_train_final[\"X_genres_categories\"].cpu().numpy(),\n",
    "            X_train_final[\"X_instruments_categories\"].cpu().numpy(),\n",
    "            X_train_final[\"X_moods_categories\"].cpu().numpy(),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Normaliser les données\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_concat)\n",
    "\n",
    "    # Convertir y_train en format binaire\n",
    "    lb = LabelBinarizer()\n",
    "    y_train_bin = lb.fit_transform(y_train)\n",
    "\n",
    "    # Appliquer le rééchantillonnage pour chaque label binaire\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled_list = []\n",
    "    y_train_resampled_list = []\n",
    "\n",
    "    for i in range(y_train_bin.shape[1]):\n",
    "        X_resampled, y_resampled = ros.fit_resample(X_train_scaled, y_train_bin[:, i])\n",
    "        X_train_resampled_list.append(X_resampled)\n",
    "        y_train_resampled_list.append(y_resampled.reshape(-1, 1))\n",
    "\n",
    "    # Concaténer les données rééchantillonnées\n",
    "    X_train_resampled = np.concatenate(X_train_resampled_list, axis=0)\n",
    "    y_train_resampled = np.concatenate(y_train_resampled_list, axis=0)\n",
    "\n",
    "    return X_train_resampled, y_train_resampled\n",
    "\n",
    "# Préparer les données pour LightGBM\n",
    "X_train_lgb, y_train_lgb = prepare_data_for_rf(X_train_final_train, train_targets.cpu().numpy())\n",
    "X_val_lgb, y_val_lgb = prepare_data_for_rf(X_train_final_val, val_targets.cpu().numpy())\n",
    "X_test_lgb, y_test_lgb = prepare_data_for_rf(X_test_final, y_test_tensor.cpu().numpy())\n",
    "\n",
    "# Initialiser les modèles LightGBM pour chaque label\n",
    "num_labels = y_train_lgb.shape[1]\n",
    "lgb_models = [lgb.LGBMClassifier(objective='binary', n_estimators=100, random_state=42) for _ in range(num_labels)]\n",
    "\n",
    "# Entraîner les modèles\n",
    "for i in range(num_labels):\n",
    "    lgb_models[i].fit(X_train_lgb, y_train_lgb[:, i])\n",
    "\n",
    "# Évaluer les modèles sur l'ensemble de validation\n",
    "y_val_pred_lgb = np.zeros_like(y_val_lgb)\n",
    "for i in range(num_labels):\n",
    "    y_val_pred_lgb[:, i] = lgb_models[i].predict(X_val_lgb)\n",
    "y_val_pred_lgb = (y_val_pred_lgb > 0.5).astype(int)\n",
    "val_accuracy = accuracy_score(y_val_lgb, y_val_pred_lgb)\n",
    "val_f1 = f1_score(y_val_lgb, y_val_pred_lgb, average='weighted')\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# Évaluer les modèles sur l'ensemble de test\n",
    "y_test_pred_lgb = np.zeros_like(y_test_lgb)\n",
    "for i in range(num_labels):\n",
    "    y_test_pred_lgb[:, i] = lgb_models[i].predict(X_test_lgb)\n",
    "y_test_pred_lgb = (y_test_pred_lgb > 0.5).astype(int)\n",
    "test_accuracy = accuracy_score(y_test_lgb, y_test_pred_lgb)\n",
    "test_f1 = f1_score(y_test_lgb, y_test_pred_lgb, average='weighted')\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_train_resampled, y_train_resampled\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Préparer les données pour LightGBM\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m X_train_lgb, y_train_lgb \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data_for_rf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_final_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_targets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m X_val_lgb, y_val_lgb \u001b[38;5;241m=\u001b[39m prepare_data_for_rf(X_train_final_val, val_targets\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     53\u001b[0m X_test_lgb, y_test_lgb \u001b[38;5;241m=\u001b[39m prepare_data_for_rf(X_test_final, y_test_tensor\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "Cell \u001b[0;32mIn[14], line 40\u001b[0m, in \u001b[0;36mprepare_data_for_rf\u001b[0;34m(X_train_final, y_train)\u001b[0m\n\u001b[1;32m     37\u001b[0m y_train_resampled_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(y_train_bin\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m---> 40\u001b[0m     X_resampled, y_resampled \u001b[38;5;241m=\u001b[39m \u001b[43msmote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_bin\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     X_train_resampled_list\u001b[38;5;241m.\u001b[39mappend(X_resampled)\n\u001b[1;32m     42\u001b[0m     y_train_resampled_list\u001b[38;5;241m.\u001b[39mappend(y_resampled\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/imblearn/base.py:202\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/imblearn/base.py:105\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m     99\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[1;32m    103\u001b[0m )\n\u001b[0;32m--> 105\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    109\u001b[0m )\n\u001b[1;32m    111\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m arrays_transformer\u001b[38;5;241m.\u001b[39mtransform(output[\u001b[38;5;241m0\u001b[39m], y_)\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/imblearn/over_sampling/_smote/base.py:360\u001b[0m, in \u001b[0;36mSMOTE._fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_k_\u001b[38;5;241m.\u001b[39mfit(X_class)\n\u001b[1;32m    359\u001b[0m nns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_k_\u001b[38;5;241m.\u001b[39mkneighbors(X_class, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 360\u001b[0m X_new, y_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_samples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m X_resampled\u001b[38;5;241m.\u001b[39mappend(X_new)\n\u001b[1;32m    364\u001b[0m y_resampled\u001b[38;5;241m.\u001b[39mappend(y_new)\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/imblearn/over_sampling/_smote/base.py:118\u001b[0m, in \u001b[0;36mBaseSMOTE._make_samples\u001b[0;34m(self, X, y_dtype, y_type, nn_data, nn_num, n_samples, step_size, y)\u001b[0m\n\u001b[1;32m    115\u001b[0m rows \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloor_divide(samples_indices, nn_num\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    116\u001b[0m cols \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmod(samples_indices, nn_num\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 118\u001b[0m X_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m y_new \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull(n_samples, fill_value\u001b[38;5;241m=\u001b[39my_type, dtype\u001b[38;5;241m=\u001b[39my_dtype)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_new, y_new\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/imblearn/over_sampling/_smote/base.py:187\u001b[0m, in \u001b[0;36mBaseSMOTE._generate_samples\u001b[0;34m(self, X, nn_data, nn_num, rows, cols, steps, y_type, y)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     X_new \u001b[38;5;241m=\u001b[39m X[rows] \u001b[38;5;241m+\u001b[39m steps \u001b[38;5;241m*\u001b[39m diffs\n\u001b[0;32m--> 187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mX_new\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def prepare_data_for_rf(X_train_final, y_train):\n",
    "    X_train_concat = np.concatenate(\n",
    "        [\n",
    "            X_train_final[\"X_genres\"].cpu().numpy(),\n",
    "            X_train_final[\"X_instruments\"].cpu().numpy(),\n",
    "            X_train_final[\"X_moods\"].cpu().numpy(),\n",
    "            X_train_final[\"X_genres_categories\"].cpu().numpy(),\n",
    "            X_train_final[\"X_instruments_categories\"].cpu().numpy(),\n",
    "            X_train_final[\"X_moods_categories\"].cpu().numpy(),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Normaliser les données\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_concat)\n",
    "\n",
    "    # Convertir y_train en format binaire\n",
    "    lb = LabelBinarizer()\n",
    "    y_train_bin = lb.fit_transform(y_train)\n",
    "\n",
    "    # Appliquer le rééchantillonnage pour chaque label binaire\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled_list = []\n",
    "    y_train_resampled_list = []\n",
    "\n",
    "    for i in range(y_train_bin.shape[1]):\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train_bin[:, i])\n",
    "        X_train_resampled_list.append(X_resampled)\n",
    "        y_train_resampled_list.append(y_resampled.reshape(-1, 1))\n",
    "\n",
    "    # Concaténer les données rééchantillonnées\n",
    "    X_train_resampled = np.concatenate(X_train_resampled_list, axis=0)\n",
    "    y_train_resampled = np.concatenate(y_train_resampled_list, axis=0)\n",
    "\n",
    "    return X_train_resampled, y_train_resampled\n",
    "\n",
    "# Préparer les données pour LightGBM\n",
    "X_train_lgb, y_train_lgb = prepare_data_for_rf(X_train_final_train, train_targets.cpu().numpy())\n",
    "X_val_lgb, y_val_lgb = prepare_data_for_rf(X_train_final_val, val_targets.cpu().numpy())\n",
    "X_test_lgb, y_test_lgb = prepare_data_for_rf(X_test_final, y_test_tensor.cpu().numpy())\n",
    "\n",
    "# Initialiser les modèles LightGBM pour chaque label\n",
    "num_labels = y_train_lgb.shape[1]\n",
    "lgb_models = [lgb.LGBMClassifier(objective='binary', n_estimators=100, random_state=42) for _ in range(num_labels)]\n",
    "\n",
    "# Entraîner les modèles par mini-batchs\n",
    "batch_size = 1000\n",
    "for i in range(num_labels):\n",
    "    for start in range(0, len(X_train_lgb), batch_size):\n",
    "        end = start + batch_size\n",
    "        lgb_models[i].fit(X_train_lgb[start:end], y_train_lgb[start:end, i], init_score=lgb_models[i].best_score_)\n",
    "\n",
    "# Évaluer les modèles sur l'ensemble de validation\n",
    "y_val_pred_lgb = np.zeros_like(y_val_lgb)\n",
    "for i in range(num_labels):\n",
    "    y_val_pred_lgb[:, i] = lgb_models[i].predict(X_val_lgb)\n",
    "y_val_pred_lgb = (y_val_pred_lgb > 0.5).astype(int)\n",
    "val_accuracy = accuracy_score(y_val_lgb, y_val_pred_lgb)\n",
    "val_f1 = f1_score(y_val_lgb, y_val_pred_lgb, average='weighted')\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# Évaluer les modèles sur l'ensemble de test\n",
    "y_test_pred_lgb = np.zeros_like(y_test_lgb)\n",
    "for i in range(num_labels):\n",
    "    y_test_pred_lgb[:, i] = lgb_models[i].predict(X_test_lgb)\n",
    "y_test_pred_lgb = (y_test_pred_lgb > 0.5).astype(int)\n",
    "test_accuracy = accuracy_score(y_test_lgb, y_test_pred_lgb)\n",
    "test_f1 = f1_score(y_test_lgb, y_test_pred_lgb, average='weighted')\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Entraîner les modèles\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_labels):\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mxgb_models\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_xgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_xgb\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Évaluer les modèles sur l'ensemble de validation\u001b[39;00m\n\u001b[1;32m     19\u001b[0m y_val_pred_xgb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(y_val_xgb)\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/xgboost/sklearn.py:1531\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[1;32m   1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1513\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1514\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1528\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1529\u001b[0m )\n\u001b[0;32m-> 1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/xgboost/core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     _check_call(\n\u001b[0;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Préparer les données pour XGBoost\n",
    "X_train_xgb, y_train_xgb = prepare_data_for_rf(X_train_final_train, train_targets.cpu().numpy())\n",
    "X_val_xgb, y_val_xgb = prepare_data_for_rf(X_train_final_val, val_targets.cpu().numpy())\n",
    "X_test_xgb, y_test_xgb = prepare_data_for_rf(X_test_final, y_test_tensor.cpu().numpy())\n",
    "\n",
    "# Initialiser les modèles XGBoost pour chaque label\n",
    "num_labels = y_train_xgb.shape[1]\n",
    "xgb_models = [xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, random_state=42) for _ in range(num_labels)]\n",
    "\n",
    "# Entraîner les modèles\n",
    "for i in range(num_labels):\n",
    "    xgb_models[i].fit(X_train_xgb, y_train_xgb[:, i])\n",
    "\n",
    "# Évaluer les modèles sur l'ensemble de validation\n",
    "y_val_pred_xgb = np.zeros_like(y_val_xgb)\n",
    "for i in range(num_labels):\n",
    "    y_val_pred_xgb[:, i] = xgb_models[i].predict(X_val_xgb)\n",
    "y_val_pred_xgb = (y_val_pred_xgb > 0.5).astype(int)\n",
    "val_accuracy = accuracy_score(y_val_xgb, y_val_pred_xgb)\n",
    "val_f1 = f1_score(y_val_xgb, y_val_pred_xgb, average='weighted')\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# Évaluer les modèles sur l'ensemble de test\n",
    "y_test_pred_xgb = np.zeros_like(y_test_xgb)\n",
    "for i in range(num_labels):\n",
    "    y_test_pred_xgb[:, i] = xgb_models[i].predict(X_test_xgb)\n",
    "y_test_pred_xgb = (y_test_pred_xgb > 0.5).astype(int)\n",
    "test_accuracy = accuracy_score(y_test_xgb, y_test_pred_xgb)\n",
    "test_f1 = f1_score(y_test_xgb, y_test_pred_xgb, average='weighted')\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de paramètres : 10089080\n"
     ]
    }
   ],
   "source": [
    "# Modèle de Transformeur\n",
    "class MultiTaskTransformer(nn.Module):\n",
    "    def __init__(self, embedding_size, num_heads, num_layers, num_labels, dropout):\n",
    "        super(MultiTaskTransformer, self).__init__()\n",
    "        self.embedding_genres = nn.Linear(input_size_dict[\"X_genres\"], embedding_size)\n",
    "        self.embedding_instruments = nn.Linear(\n",
    "            input_size_dict[\"X_instruments\"], embedding_size\n",
    "        )\n",
    "        self.embedding_moods = nn.Linear(input_size_dict[\"X_moods\"], embedding_size)\n",
    "        self.embedding_genres_categories = nn.Linear(\n",
    "            input_size_dict[\"X_genres_categories\"], embedding_size\n",
    "        )\n",
    "        self.embedding_instruments_categories = nn.Linear(\n",
    "            input_size_dict[\"X_instruments_categories\"], embedding_size\n",
    "        )\n",
    "        self.embedding_moods_categories = nn.Linear(\n",
    "            input_size_dict[\"X_moods_categories\"], embedding_size\n",
    "        )\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embedding_size,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.classifier = nn.Linear(embedding_size, num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_genres,\n",
    "        x_instruments,\n",
    "        x_moods,\n",
    "        x_genres_categories,\n",
    "        x_instruments_categories,\n",
    "        x_moods_categories,\n",
    "    ):\n",
    "        embedded_genres = self.embedding_genres(x_genres)\n",
    "        embedded_instruments = self.embedding_instruments(x_instruments)\n",
    "        embedded_moods = self.embedding_moods(x_moods)\n",
    "        embedded_genres_categories = self.embedding_genres_categories(\n",
    "            x_genres_categories\n",
    "        )\n",
    "        embedded_instruments_categories = self.embedding_instruments_categories(\n",
    "            x_instruments_categories\n",
    "        )\n",
    "        embedded_moods_categories = self.embedding_moods_categories(x_moods_categories)\n",
    "\n",
    "        sequence = torch.stack(\n",
    "            [\n",
    "                embedded_genres,\n",
    "                embedded_instruments,\n",
    "                embedded_moods,\n",
    "                embedded_genres_categories,\n",
    "                embedded_instruments_categories,\n",
    "                embedded_moods_categories,\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        transformer_output = self.transformer(sequence, sequence)\n",
    "        output = transformer_output.mean(dim=1)\n",
    "        predictions = self.classifier(output)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Initialiser le modèle\n",
    "model = MultiTaskTransformer(\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_labels=y_train.shape[1],\n",
    "    dropout=DROPOUT,\n",
    ").to(DEVICE)\n",
    "\n",
    "# Affichage du nombre de paramètres\n",
    "print(f\"Nombre total de paramètres : {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Optimiseur et fonction de perte\n",
    "criterion = nn.BCEWithLogitsLoss()  # Fonction de perte pour les étiquettes binaires\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "        self.attention_weights = nn.Linear(embedding_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, sequence_length, embedding_size)\n",
    "        weights = torch.softmax(\n",
    "            self.attention_weights(x), dim=1\n",
    "        )  # (batch_size, sequence_length, 1)\n",
    "        context = torch.sum(\n",
    "            weights * x, dim=1\n",
    "        )  # Weighted sum, (batch_size, embedding_size)\n",
    "        return context\n",
    "\n",
    "\n",
    "class GlobalPooling(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(GlobalPooling, self).__init__()\n",
    "        self.attention_pooling = AttentionPooling(embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, sequence_length, embedding_size)\n",
    "        attention_output = self.attention_pooling(x)  # Attention pooling\n",
    "        max_output, _ = torch.max(x, dim=1)  # Max pooling\n",
    "        return attention_output + max_output  # Combine both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de paramètres : 4872761\n"
     ]
    }
   ],
   "source": [
    "class MultiTaskTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size_dict,\n",
    "        embedding_size,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        num_labels,\n",
    "        dropout,\n",
    "    ):\n",
    "        super(MultiTaskTransformer, self).__init__()\n",
    "\n",
    "        # Parallel input embeddings\n",
    "        self.embeddings = nn.ModuleDict(\n",
    "            {\n",
    "                \"genres\": nn.Sequential(\n",
    "                    nn.Linear(input_size_dict[\"X_genres\"], embedding_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                ),\n",
    "                \"instruments\": nn.Sequential(\n",
    "                    nn.Linear(input_size_dict[\"X_instruments\"], embedding_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                ),\n",
    "                \"moods\": nn.Sequential(\n",
    "                    nn.Linear(input_size_dict[\"X_moods\"], embedding_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Categories embeddings\n",
    "        self.categories_embeddings = nn.ModuleDict(\n",
    "            {\n",
    "                \"genres\": nn.Sequential(\n",
    "                    nn.Linear(input_size_dict[\"X_genres_categories\"], embedding_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                ),\n",
    "                \"instruments\": nn.Sequential(\n",
    "                    nn.Linear(\n",
    "                        input_size_dict[\"X_instruments_categories\"], embedding_size\n",
    "                    ),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                ),\n",
    "                \"moods\": nn.Sequential(\n",
    "                    nn.Linear(input_size_dict[\"X_moods_categories\"], embedding_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Cross-attention mechanism\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=embedding_size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Transformer encoder\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(\n",
    "                d_model=embedding_size,\n",
    "                nhead=num_heads,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            ),\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "\n",
    "        # LayerNorm\n",
    "        self.layer_norm = nn.LayerNorm(embedding_size)\n",
    "\n",
    "        # Global pooling\n",
    "        self.global_pooling = GlobalPooling(embedding_size)\n",
    "\n",
    "        # Final classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_size, embedding_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embedding_size // 2, num_labels),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_genres,\n",
    "        x_instruments,\n",
    "        x_moods,\n",
    "        x_genres_categories,\n",
    "        x_instruments_categories,\n",
    "        x_moods_categories,\n",
    "    ):\n",
    "        # Embed input features\n",
    "        embedded_inputs = torch.stack(\n",
    "            [\n",
    "                self.embeddings[\"genres\"](x_genres),\n",
    "                self.embeddings[\"instruments\"](x_instruments),\n",
    "                self.embeddings[\"moods\"](x_moods),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "\n",
    "        # Embed categories\n",
    "        embedded_categories = torch.stack(\n",
    "            [\n",
    "                self.categories_embeddings[\"genres\"](x_genres_categories),\n",
    "                self.categories_embeddings[\"instruments\"](x_instruments_categories),\n",
    "                self.categories_embeddings[\"moods\"](x_moods_categories),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        # Combine inputs\n",
    "        combined_sequence = torch.cat(\n",
    "            [embedded_inputs, embedded_categories], dim=1\n",
    "        )\n",
    "        combined_sequence = self.layer_norm(combined_sequence)\n",
    "\n",
    "        # Apply cross-attention\n",
    "        attended_sequence, _ = self.cross_attention(\n",
    "            combined_sequence, combined_sequence, combined_sequence\n",
    "        )\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoded_sequence = self.transformer_encoder(attended_sequence)\n",
    "\n",
    "        # Global pooling\n",
    "        pooled_output = self.global_pooling(encoded_sequence)\n",
    "\n",
    "        # Final classification\n",
    "        predictions = self.classifier(pooled_output)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "model = MultiTaskTransformer(\n",
    "    input_size_dict=input_size_dict,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_labels=y_train.shape[1],\n",
    "    dropout=DROPOUT,\n",
    ").to(DEVICE)\n",
    "\n",
    "# Affichage du nombre de paramètres\n",
    "print(f\"Nombre total de paramètres : {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Optimizer and loss\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de paramètres : 10088440\n"
     ]
    }
   ],
   "source": [
    "# Modèle de Transformeur\n",
    "class MultiTaskTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, embedding_size, num_heads, num_layers, num_labels, dropout\n",
    "    ):\n",
    "        super(MultiTaskTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, embedding_size)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embedding_size,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.classifier = nn.Linear(embedding_size, num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_genres,\n",
    "        x_instruments,\n",
    "        x_moods,\n",
    "        x_genres_categories,\n",
    "        x_instruments_categories,\n",
    "        x_moods_categories,\n",
    "    ):\n",
    "        # Concatenate all input features\n",
    "        concatenated_inputs = torch.cat(\n",
    "            [\n",
    "                x_genres,\n",
    "                x_instruments,\n",
    "                x_moods,\n",
    "                x_genres_categories,\n",
    "                x_instruments_categories,\n",
    "                x_moods_categories,\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        # Pass through the embedding layer\n",
    "        embedded_inputs = self.embedding(concatenated_inputs)\n",
    "\n",
    "        # Add a sequence dimension\n",
    "        embedded_inputs = embedded_inputs.unsqueeze(1)\n",
    "\n",
    "        # Transformer\n",
    "        transformer_output = self.transformer(embedded_inputs, embedded_inputs)\n",
    "        output = transformer_output.mean(dim=1)\n",
    "        predictions = self.classifier(output)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Initialiser le modèle\n",
    "input_size = (\n",
    "    input_size_dict[\"X_genres\"]\n",
    "    + input_size_dict[\"X_instruments\"]\n",
    "    + input_size_dict[\"X_moods\"]\n",
    "    + input_size_dict[\"X_genres_categories\"]\n",
    "    + input_size_dict[\"X_instruments_categories\"]\n",
    "    + input_size_dict[\"X_moods_categories\"]\n",
    ")\n",
    "\n",
    "model = MultiTaskTransformer(\n",
    "    input_size=input_size,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_labels=y_train.shape[1],\n",
    "    dropout=DROPOUT,\n",
    ").to(DEVICE)\n",
    "\n",
    "# Affichage du nombre de paramètres\n",
    "print(f\"Nombre total de paramètres : {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Optimiseur et fonction de perte\n",
    "criterion = nn.BCEWithLogitsLoss()  # Fonction de perte pour les étiquettes binaires\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, epochs, patience\n",
    "):\n",
    "    \"\"\"\n",
    "    Entraîne un modèle avec early stopping.\n",
    "\n",
    "    Args:\n",
    "        model: Le modèle à entraîner.\n",
    "        train_loader: DataLoader pour les données d'entraînement.\n",
    "        val_loader: DataLoader pour les données de validation.\n",
    "        criterion: Fonction de perte.\n",
    "        optimizer: Optimiseur.\n",
    "        epochs: Nombre maximum d'époques.\n",
    "        patience: Nombre d'époques à attendre pour une amélioration avant d'arrêter l'entraînement.\n",
    "\n",
    "    Returns:\n",
    "        Le meilleur modèle basé sur les performances sur l'ensemble de validation.\n",
    "    \"\"\"\n",
    "    best_loss = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Mode entraînement\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for (\n",
    "            x_genres_batch,\n",
    "            x_instruments_batch,\n",
    "            x_moods_batch,\n",
    "            x_genres_categories_batch,\n",
    "            x_instruments_categories_batch,\n",
    "            x_moods_categories_batch,\n",
    "            y_batch,\n",
    "        ) in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(\n",
    "                x_genres_batch,\n",
    "                x_instruments_batch,\n",
    "                x_moods_batch,\n",
    "                x_genres_categories_batch,\n",
    "                x_instruments_categories_batch,\n",
    "                x_moods_categories_batch,\n",
    "            )\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # Mode validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for (\n",
    "                x_genres_batch,\n",
    "                x_instruments_batch,\n",
    "                x_moods_batch,\n",
    "                x_genres_categories_batch,\n",
    "                x_instruments_categories_batch,\n",
    "                x_moods_categories_batch,\n",
    "                y_batch,\n",
    "            ) in val_loader:\n",
    "                predictions = model(\n",
    "                    x_genres_batch,\n",
    "                    x_instruments_batch,\n",
    "                    x_moods_batch,\n",
    "                    x_genres_categories_batch,\n",
    "                    x_instruments_categories_batch,\n",
    "                    x_moods_categories_batch,\n",
    "                )\n",
    "                loss = criterion(predictions, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\n",
    "                f\"Early stopping triggered at epoch {epoch + 1}. Best validation loss: {best_loss:.4f}\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "    # Charger le meilleur modèle\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Évaluation\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (\n",
    "            x_genres_batch,\n",
    "            x_instruments_batch,\n",
    "            x_moods_batch,\n",
    "            x_genres_categories_batch,\n",
    "            x_instruments_categories_batch,\n",
    "            x_moods_categories_batch,\n",
    "            y_batch,\n",
    "        ) in test_loader:\n",
    "            predictions = model(\n",
    "                x_genres_batch,\n",
    "                x_instruments_batch,\n",
    "                x_moods_batch,\n",
    "                x_genres_categories_batch,\n",
    "                x_instruments_categories_batch,\n",
    "                x_moods_categories_batch,\n",
    "            )\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            total_loss += loss.item()\n",
    "    print(f\"Test Loss: {total_loss / len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 0.1598, Val Loss: 0.1058\n",
      "Epoch 2/30, Train Loss: 0.1003, Val Loss: 0.0939\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Entraîner le modèle\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATIENCE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Évaluer le modèle\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# evaluate_model(model, test_loader, criterion)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[51], line 47\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, epochs, patience)\u001b[0m\n\u001b[1;32m     38\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     39\u001b[0m     x_genres_batch,\n\u001b[1;32m     40\u001b[0m     x_instruments_batch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     x_moods_categories_batch,\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     46\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, y_batch)\n\u001b[0;32m---> 47\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     49\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Entraîner le modèle\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, EPOCHS, PATIENCE)\n",
    "\n",
    "# Évaluer le modèle\n",
    "# evaluate_model(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.07987426665828963\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation des performances (accuracy, precision, recall, f1-score)\n",
    "def evaluate_performance(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for (\n",
    "            x_genres_batch,\n",
    "            x_instruments_batch,\n",
    "            x_moods_batch,\n",
    "            x_genres_categories_batch,\n",
    "            x_instruments_categories_batch,\n",
    "            x_moods_categories_batch,\n",
    "            y_batch,\n",
    "        ) in test_loader:\n",
    "            predictions = model(\n",
    "                x_genres_batch,\n",
    "                x_instruments_batch,\n",
    "                x_moods_batch,\n",
    "                x_genres_categories_batch,\n",
    "                x_instruments_categories_batch,\n",
    "                x_moods_categories_batch,\n",
    "            )\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "            predictions = (predictions > 0.5).int()\n",
    "            y_true.append(y_batch.cpu().numpy())\n",
    "            y_pred.append(predictions.cpu().numpy())\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "    # Save 5% of the rows of the predictions as csv files in the data folder in predictions folder\n",
    "    # np.savetxt(\n",
    "    #     \"../data/predictions/train/y_true.csv\",\n",
    "    #     y_true[: int(0.05 * len(y_true))],\n",
    "    #     delimiter=\",\",\n",
    "    # )\n",
    "    # np.savetxt(\n",
    "    #     \"../data/predictions/train/y_pred.csv\",\n",
    "    #     y_pred[: int(0.05 * len(y_pred))],\n",
    "    #     delimiter=\",\",\n",
    "    # )\n",
    "\n",
    "    # Histograms plot of the predictions and true values for each tag\n",
    "    # for i in range(y_true.shape[1]):\n",
    "    #     plt.hist(y_true[:, i], bins=2, alpha=0.5, label=\"True\")\n",
    "    #     plt.hist(y_pred[:, i], bins=2, alpha=0.5, label=\"Predicted\")\n",
    "    #     plt.title(f\"Tag {i}\")\n",
    "    #     plt.legend()\n",
    "    #     plt.savefig(f\"../data/predictions/train/histogram_tag_{i}.png\")\n",
    "    #     plt.clf()\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # Precision, Recall, F1-Score\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    report = classification_report(y_true, y_pred, digits=4)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9713598918086835\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4118    0.0718    0.1223       195\n",
      "           1     0.5673    0.4767    0.5181      1202\n",
      "           2     0.5517    0.0643    0.1151       249\n",
      "           3     0.5000    0.0741    0.1290        81\n",
      "           4     0.6071    0.2537    0.3579        67\n",
      "           5     0.5408    0.2000    0.2920       265\n",
      "           6     0.5000    0.2523    0.3353       111\n",
      "           7     0.5196    0.2091    0.2982       507\n",
      "           8     0.5974    0.3485    0.4402      1056\n",
      "           9     0.4881    0.1767    0.2595       232\n",
      "          10     0.8514    0.5947    0.7002       713\n",
      "          11     0.7238    0.7178    0.7208      2041\n",
      "          12     0.5915    0.4289    0.4973       422\n",
      "          13     0.6400    0.1531    0.2471      1463\n",
      "          14     0.5290    0.2439    0.3339       861\n",
      "          15     0.5877    0.1754    0.2702       382\n",
      "          16     0.0000    0.0000    0.0000        77\n",
      "          17     0.5641    0.1392    0.2234       158\n",
      "          18     0.4154    0.2061    0.2755       131\n",
      "          19     0.5513    0.1604    0.2486       268\n",
      "          20     0.7877    0.5575    0.6529       992\n",
      "          21     0.6897    0.2381    0.3540       168\n",
      "          22     0.6610    0.2532    0.3662       154\n",
      "          23     0.6579    0.4119    0.5066       976\n",
      "          24     0.5695    0.1854    0.2798       906\n",
      "          25     0.0000    0.0000    0.0000        86\n",
      "          26     0.4040    0.1156    0.1798       346\n",
      "          27     0.6737    0.5611    0.6123      3463\n",
      "          28     0.7597    0.5223    0.6190       896\n",
      "          29     0.6417    0.1825    0.2841       422\n",
      "          30     0.3947    0.1364    0.2027       110\n",
      "          31     0.4286    0.0391    0.0717       230\n",
      "          32     0.8095    0.2237    0.3505        76\n",
      "          33     0.5637    0.4499    0.5004       629\n",
      "          34     0.6223    0.3846    0.4754       377\n",
      "          35     0.6627    0.5093    0.5759       756\n",
      "          36     0.4757    0.4490    0.4619       588\n",
      "          37     0.4634    0.3674    0.4099       362\n",
      "          38     0.5147    0.2541    0.3402       551\n",
      "          39     0.5421    0.3877    0.4521      1478\n",
      "          40     0.5000    0.0440    0.0808        91\n",
      "          41     0.4459    0.1404    0.2136       235\n",
      "          42     0.4648    0.0976    0.1614       338\n",
      "          43     0.7500    0.0550    0.1026       109\n",
      "          44     0.4889    0.1084    0.1774       203\n",
      "          45     0.6667    0.2427    0.3559       651\n",
      "          46     0.8065    0.4630    0.5882        54\n",
      "          47     0.5000    0.1277    0.2034        47\n",
      "          48     0.6509    0.5036    0.5679       137\n",
      "          49     0.7222    0.2500    0.3714        52\n",
      "          50     0.0000    0.0000    0.0000        67\n",
      "          51     0.5000    0.1348    0.2124        89\n",
      "          52     1.0000    0.1143    0.2051        35\n",
      "          53     0.8512    0.6745    0.7526       212\n",
      "          54     0.5915    0.2650    0.3660       366\n",
      "          55     0.6507    0.3487    0.4541       390\n",
      "          56     0.5000    0.0169    0.0328        59\n",
      "          57     0.0000    0.0000    0.0000       131\n",
      "          58     0.0000    0.0000    0.0000       112\n",
      "          59     0.5596    0.3653    0.4420       167\n",
      "          60     0.4545    0.0532    0.0952        94\n",
      "          61     0.4907    0.2969    0.3700       357\n",
      "          62     0.8000    0.0952    0.1702        42\n",
      "          63     0.6514    0.2996    0.4104       237\n",
      "          64     0.7429    0.1818    0.2921       143\n",
      "          65     0.4754    0.2148    0.2959       135\n",
      "          66     0.4768    0.3772    0.4212       464\n",
      "          67     0.6154    0.1277    0.2115       188\n",
      "          68     0.7500    0.3383    0.4663       133\n",
      "          69     0.6170    0.1368    0.2239       212\n",
      "          70     0.6139    0.1308    0.2157       474\n",
      "          71     0.0000    0.0000    0.0000        21\n",
      "          72     0.0000    0.0000    0.0000        36\n",
      "          73     0.5657    0.2947    0.3875       190\n",
      "          74     0.5000    0.0374    0.0696       107\n",
      "          75     0.6000    0.0405    0.0759        74\n",
      "          76     0.6316    0.1188    0.2000       101\n",
      "          77     0.5854    0.2000    0.2981       360\n",
      "          78     0.6364    0.0565    0.1037       124\n",
      "          79     0.5122    0.2593    0.3443        81\n",
      "          80     0.4972    0.2167    0.3019       406\n",
      "          81     0.7760    0.5279    0.6284      1398\n",
      "          82     0.5676    0.1745    0.2669       361\n",
      "          83     0.6500    0.2000    0.3059        65\n",
      "          84     0.4918    0.1604    0.2419       374\n",
      "          85     0.3158    0.0645    0.1071        93\n",
      "          86     0.7221    0.5618    0.6319       680\n",
      "          87     0.7152    0.3361    0.4573       964\n",
      "          88     0.7500    0.1333    0.2264        45\n",
      "          89     0.5300    0.1889    0.2786       561\n",
      "          90     0.8101    0.6531    0.7232        98\n",
      "          91     0.7540    0.5985    0.6673       548\n",
      "          92     0.7276    0.6583    0.6912      3477\n",
      "          93     0.5212    0.1706    0.2571       504\n",
      "          94     0.7607    0.3436    0.4734       259\n",
      "          95     0.7848    0.6445    0.7077      5097\n",
      "          96     0.7222    0.4105    0.5235        95\n",
      "          97     0.6957    0.1429    0.2370       224\n",
      "          98     1.0000    0.0769    0.1429        91\n",
      "          99     0.7216    0.5054    0.5945       277\n",
      "         100     0.7587    0.5428    0.6328      2907\n",
      "         101     0.6584    0.2697    0.3827       393\n",
      "         102     0.7655    0.3732    0.5017      1155\n",
      "         103     0.0000    0.0000    0.0000        25\n",
      "         104     1.0000    0.0500    0.0952        20\n",
      "         105     0.5714    0.0714    0.1270       112\n",
      "         106     0.7787    0.3668    0.4987       518\n",
      "         107     0.6678    0.4078    0.5064       488\n",
      "         108     0.3333    0.0272    0.0503       147\n",
      "         109     0.7101    0.1870    0.2961       262\n",
      "         110     0.5306    0.2766    0.3636        94\n",
      "         111     0.7436    0.0836    0.1503       347\n",
      "         112     0.8421    0.2133    0.3404        75\n",
      "         113     0.6821    0.4077    0.5103      1121\n",
      "         114     1.0000    0.0345    0.0667        87\n",
      "         115     0.7901    0.6965    0.7403      5232\n",
      "         116     0.6833    0.6997    0.6914      4459\n",
      "         117     0.6154    0.1322    0.2177       121\n",
      "         118     0.8189    0.6958    0.7523      6341\n",
      "         119     0.6148    0.2500    0.3555       300\n",
      "         120     0.6174    0.3966    0.4830       179\n",
      "         121     0.6522    0.0955    0.1667       157\n",
      "         122     0.7858    0.5776    0.6658      1378\n",
      "         123     0.4600    0.1159    0.1851       397\n",
      "         124     0.0000    0.0000    0.0000        71\n",
      "         125     0.6220    0.3469    0.4454      1176\n",
      "         126     0.6494    0.5501    0.5956       569\n",
      "         127     0.5102    0.1163    0.1894       215\n",
      "         128     0.5857    0.2789    0.3779       588\n",
      "         129     0.5473    0.1350    0.2166       600\n",
      "         130     0.6681    0.2680    0.3825       571\n",
      "         131     0.7778    0.2812    0.4131       224\n",
      "         132     0.6192    0.3532    0.4498       603\n",
      "         133     0.7391    0.3187    0.4454       160\n",
      "         134     0.6667    0.2424    0.3556        66\n",
      "         135     0.7470    0.4305    0.5462       734\n",
      "         136     0.5789    0.2973    0.3929       111\n",
      "         137     0.6588    0.4179    0.5114       134\n",
      "         138     0.6830    0.2436    0.3591       743\n",
      "         139     0.6200    0.2153    0.3196       144\n",
      "         140     0.6061    0.3226    0.4211        62\n",
      "         141     0.7624    0.6763    0.7168      1860\n",
      "         142     0.5714    0.1606    0.2508       249\n",
      "         143     0.6781    0.4714    0.5562       210\n",
      "         144     0.6162    0.3754    0.4666       586\n",
      "         145     0.0000    0.0000    0.0000        21\n",
      "         146     0.6818    0.2830    0.4000        53\n",
      "         147     0.5000    0.0263    0.0500        38\n",
      "         148     0.6706    0.2103    0.3202       271\n",
      "         149     0.7231    0.2597    0.3821       362\n",
      "         150     0.6352    0.3507    0.4519       288\n",
      "         151     0.8605    0.4512    0.5920        82\n",
      "         152     1.0000    0.0141    0.0278        71\n",
      "         153     0.5949    0.3219    0.4178       146\n",
      "         154     0.5608    0.2280    0.3242       364\n",
      "         155     0.7321    0.5888    0.6527      5413\n",
      "         156     1.0000    0.0625    0.1176        48\n",
      "         157     0.7401    0.3661    0.4898       560\n",
      "         158     0.6756    0.3322    0.4454      1755\n",
      "         159     0.5365    0.4274    0.4758       241\n",
      "         160     0.5000    0.1549    0.2366        71\n",
      "         161     0.7592    0.6994    0.7280      2335\n",
      "         162     0.7155    0.4191    0.5286      1038\n",
      "         163     0.4000    0.0988    0.1584       243\n",
      "         164     0.6504    0.0921    0.1613       869\n",
      "         165     0.7143    0.2404    0.3597       104\n",
      "         166     0.7027    0.3741    0.4883       139\n",
      "         167     0.6207    0.1125    0.1905       160\n",
      "         168     0.6667    0.2710    0.3854       214\n",
      "         169     0.8079    0.6703    0.7327       910\n",
      "         170     1.0000    0.0282    0.0548        71\n",
      "         171     0.0000    0.0000    0.0000        35\n",
      "         172     0.7194    0.5676    0.6345      3686\n",
      "         173     0.6818    0.6761    0.6789      1065\n",
      "         174     0.6757    0.5475    0.6049      1496\n",
      "         175     0.6501    0.4305    0.5180      1489\n",
      "         176     0.5333    0.0181    0.0349       443\n",
      "         177     0.7132    0.7191    0.7161      7002\n",
      "         178     0.0000    0.0000    0.0000        80\n",
      "         179     0.6087    0.2800    0.3836       100\n",
      "         180     0.4135    0.1083    0.1716       508\n",
      "         181     0.5846    0.2484    0.3486       153\n",
      "         182     0.7339    0.2151    0.3327       423\n",
      "         183     0.5000    0.0071    0.0141       140\n",
      "         184     0.5135    0.1086    0.1792       175\n",
      "         185     0.7527    0.2344    0.3575       883\n",
      "         186     0.5745    0.3140    0.4060        86\n",
      "         187     1.0000    0.0119    0.0235        84\n",
      "         188     0.4813    0.1676    0.2486       537\n",
      "         189     0.7444    0.5294    0.6188       187\n",
      "         190     0.7403    0.1338    0.2266       426\n",
      "         191     0.4500    0.0957    0.1579        94\n",
      "         192     0.6892    0.4465    0.5419       869\n",
      "         193     0.8405    0.7389    0.7864      4956\n",
      "         194     0.6667    0.0625    0.1143       224\n",
      "         195     0.0000    0.0000    0.0000        76\n",
      "         196     0.5294    0.1737    0.2616       518\n",
      "         197     0.7200    0.1782    0.2857       101\n",
      "         198     0.8764    0.4906    0.6290       159\n",
      "         199     0.6762    0.4495    0.5400       525\n",
      "         200     0.4324    0.0283    0.0531       566\n",
      "         201     0.5793    0.3493    0.4358       272\n",
      "         202     0.5000    0.1006    0.1674       905\n",
      "         203     0.5404    0.1155    0.1904       753\n",
      "         204     0.6306    0.3616    0.4597      1402\n",
      "         205     0.6386    0.2984    0.4068      2138\n",
      "         206     0.5942    0.2138    0.3144      2479\n",
      "         207     0.1667    0.0106    0.0199       379\n",
      "         208     0.5760    0.3518    0.4368      5503\n",
      "         209     0.5612    0.0847    0.1473       649\n",
      "         210     0.4778    0.0542    0.0974       793\n",
      "         211     0.5698    0.3068    0.3989      4364\n",
      "         212     0.6090    0.3467    0.4419      1096\n",
      "         213     0.4286    0.0193    0.0370       466\n",
      "         214     0.5205    0.1510    0.2341      2185\n",
      "         215     0.5128    0.1277    0.2044       470\n",
      "         216     0.6023    0.1673    0.2619      1267\n",
      "         217     0.6454    0.3410    0.4462      2877\n",
      "         218     0.4307    0.1269    0.1960       465\n",
      "         219     0.5614    0.2861    0.3790      1199\n",
      "         220     0.6019    0.1512    0.2417       840\n",
      "         221     0.3980    0.0788    0.1315       495\n",
      "         222     0.5884    0.2238    0.3242      1859\n",
      "         223     0.5348    0.2887    0.3750       904\n",
      "         224     0.6053    0.3116    0.4114      1688\n",
      "         225     0.3750    0.0464    0.0826       388\n",
      "         226     0.4898    0.0365    0.0679       658\n",
      "         227     0.5410    0.2045    0.2968      1291\n",
      "         228     0.5911    0.4039    0.4799      5236\n",
      "         229     0.5873    0.1963    0.2943      2073\n",
      "         230     0.3939    0.0796    0.1324       490\n",
      "         231     0.5676    0.1465    0.2329       860\n",
      "         232     0.6316    0.0782    0.1391       307\n",
      "         233     0.5801    0.2216    0.3207      2319\n",
      "         234     0.6686    0.4089    0.5074      4131\n",
      "         235     0.6422    0.3370    0.4420      2003\n",
      "         236     0.6104    0.2909    0.3940      1901\n",
      "         237     0.5263    0.0855    0.1471       819\n",
      "         238     0.5628    0.2266    0.3231       909\n",
      "         239     0.4444    0.0369    0.0682       758\n",
      "         240     0.6460    0.5119    0.5712      2446\n",
      "         241     0.5556    0.0223    0.0429       224\n",
      "         242     0.5770    0.3405    0.4283      3357\n",
      "         243     0.5966    0.2583    0.3605      2021\n",
      "         244     0.0000    0.0000    0.0000        87\n",
      "         245     0.6241    0.2416    0.3483      2790\n",
      "         246     0.4789    0.0935    0.1565       727\n",
      "         247     0.5989    0.2866    0.3876      3043\n",
      "\n",
      "   micro avg     0.6790    0.4034    0.5061    200012\n",
      "   macro avg     0.5889    0.2545    0.3294    200012\n",
      "weighted avg     0.6482    0.4034    0.4765    200012\n",
      " samples avg     0.6638    0.4036    0.4791    200012\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluer les performances pour test_loader combiné avec le train_loader\n",
    "\n",
    "# Combine les loaders\n",
    "# combined_loader = torch.utils.data.DataLoader(\n",
    "#     torch.utils.data.ConcatDataset([train_dataset, test_dataset, val_dataset]),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=False,\n",
    "# )\n",
    "\n",
    "evaluate_performance(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer et ESNs sauvegardés avec succès !\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder le modèle et les ESNs dans le dossier models et un sous dossier qui s'incrémente\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Créer un dossier models s'il n'existe pas\n",
    "if not os.path.exists(\"../models\"):\n",
    "    os.makedirs(\"../models\")\n",
    "\n",
    "# Créer un sous-dossier pour les modèles\n",
    "sub_folder = 0\n",
    "while os.path.exists(f\"../models/model_{sub_folder}\"):\n",
    "    sub_folder += 1\n",
    "os.makedirs(f\"../models/model_{sub_folder}\")\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "torch.save(model.state_dict(), f\"../models/model_{sub_folder}/transformer_weights.pth\")\n",
    "\n",
    "with open(f\"../models/model_{sub_folder}/transformer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Sauvegarder les ESNs\n",
    "with open(f\"../models/model_{sub_folder}/esn_Genre.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_Genre, f)\n",
    "\n",
    "with open(f\"../models/model_{sub_folder}/esn_Instrument.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_Instrument, f)\n",
    "\n",
    "with open(f\"../models/model_{sub_folder}/esn_Mood.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_Mood, f)\n",
    "\n",
    "with open(f\"../models/model_{sub_folder}/esn_Genre_Instrument.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_Genre_Instrument, f)\n",
    "\n",
    "with open(f\"../models/model_{sub_folder}/esn_Genre_Mood.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_Genre_Mood, f)\n",
    "\n",
    "with open(f\"../models/model_{sub_folder}/esn_Instrument_Mood.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_Instrument_Mood, f)\n",
    "\n",
    "\n",
    "print(\"Transformer et ESNs sauvegardés avec succès !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
