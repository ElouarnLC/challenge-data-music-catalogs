{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from reservoirpy.nodes import Reservoir, Ridge, ESN\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_SIZE = 256  # Taille des embeddings pour tous les inputs\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 4\n",
    "DROPOUT = 0.1\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 20\n",
    "PATIENCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, embedding_size, num_heads, num_layers, num_labels, dropout\n",
    "    ):\n",
    "        super(MultiTaskTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, embedding_size)  # Embedding Layer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embedding_size,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.classifier = nn.Linear(embedding_size, num_labels)  # Final Classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the embedding layer\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Add a positional encoding (if needed)\n",
    "        embedded = embedded.unsqueeze(1)  # Add sequence dimension\n",
    "\n",
    "        # Transformer expects (batch, seq_len, embedding_size)\n",
    "        transformer_output = self.transformer(embedded, embedded)\n",
    "\n",
    "        # Take only the output of the first token (classification token equivalent)\n",
    "        output = transformer_output[:, 0, :]  # Extract first token\n",
    "\n",
    "        # Pass through the classifier\n",
    "        predictions = self.classifier(output)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    partOfData = 1\n",
    "    X_genres = pd.read_csv(\"../data/test/input_genres_tags_data.csv\")\n",
    "    X_instruments = pd.read_csv(\"../data/test/input_instruments_tags_data.csv\")\n",
    "    X_moods = pd.read_csv(\"../data/test/input_moods_tags_data.csv\")\n",
    "\n",
    "    y_genres = pd.read_csv(\"../data/test/output_genres_tags_data.csv\")\n",
    "    y_instruments = pd.read_csv(\"../data/test/output_instruments_tags_data.csv\")\n",
    "    y_moods = pd.read_csv(\"../data/test/output_moods_tags_data.csv\")\n",
    "\n",
    "    # On peut garder seulement une partie des données\n",
    "    X_genres = X_genres[: int(partOfData * len(X_genres))]\n",
    "    X_instruments = X_instruments[: int(partOfData * len(X_instruments))]\n",
    "    X_moods = X_moods[: int(partOfData * len(X_moods))]\n",
    "    y_genres = y_genres[: int(partOfData * len(y_genres))]\n",
    "    y_instruments = y_instruments[: int(partOfData * len(y_instruments))]\n",
    "    y_moods = y_moods[: int(partOfData * len(y_moods))]\n",
    "\n",
    "    return (X_genres, X_instruments, X_moods), (y_genres, y_instruments, y_moods)\n",
    "\n",
    "\n",
    "# Ensure the input data is in the correct format\n",
    "def reshape_input(X):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        return X.values.reshape(-1, 1, X.shape[1])  # Handles pandas DataFrame\n",
    "    elif isinstance(X, np.ndarray):\n",
    "        return X.reshape(-1, 1, X.shape[1])  # Handles numpy ndarray\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a pandas DataFrame or a numpy ndarray\")\n",
    "\n",
    "\n",
    "def format_predictions(predictions):\n",
    "    # Convert the list to a NumPy array\n",
    "    predictions_array = np.array(predictions)\n",
    "\n",
    "    # Reshape the array to 2-dimensional\n",
    "    predictions_reshaped = predictions_array.reshape(-1, predictions_array.shape[-1])\n",
    "\n",
    "    return predictions_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données\n",
    "(X_genres, X_instruments, X_moods), (y_genres, y_instruments, y_moods) = load_data()\n",
    "\n",
    "# Préparation des données\n",
    "X_genres = X_genres.drop(columns=[\"ChallengeID\"])\n",
    "X_instruments = X_instruments.drop(columns=[\"ChallengeID\"])\n",
    "X_moods = X_moods.drop(columns=[\"ChallengeID\"])\n",
    "y_genres = y_genres.drop(columns=[\"ChallengeID\"])\n",
    "y_instruments = y_instruments.drop(columns=[\"ChallengeID\"])\n",
    "y_moods = y_moods.drop(columns=[\"ChallengeID\"])\n",
    "\n",
    "\n",
    "X = np.concatenate([X_genres, X_instruments, X_moods], axis=1)\n",
    "\n",
    "y = np.concatenate([y_genres, y_instruments, y_moods], axis=1)\n",
    "\n",
    "# Convertir les données en tensors PyTorch\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# Générer les données croisées pour les interactions\n",
    "X_genres_instruments = np.concatenate([X_genres, X_instruments], axis=1)\n",
    "X_genres_moods = np.concatenate([X_genres, X_moods], axis=1)\n",
    "X_instruments_moods = np.concatenate([X_instruments, X_moods], axis=1)\n",
    "y_genres_instruments = np.concatenate([y_genres, y_instruments], axis=1)\n",
    "y_genres_moods = np.concatenate([y_genres, y_moods], axis=1)\n",
    "y_instruments_moods = np.concatenate([y_instruments, y_moods], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ESNs models from ../models/model_2\n",
      "Nombre total de paramètres : 11830776\n"
     ]
    }
   ],
   "source": [
    "# Load ENSs models\n",
    "sub_folder = 0\n",
    "while os.path.exists(f\"../models/model_{sub_folder}\"):\n",
    "    sub_folder += 1\n",
    "sub_folder -= 1\n",
    "\n",
    "print(f\"Loading ESNs models from ../models/model_{sub_folder}\")\n",
    "\n",
    "with open(f\"../models/model_{sub_folder}/esn_Genre.pkl\", \"rb\") as f:\n",
    "    model_Genre = pickle.load(f)\n",
    "\n",
    "with open(f\"../models/model_{sub_folder}/esn_Instrument.pkl\", \"rb\") as f:\n",
    "    model_Instrument = pickle.load(f)\n",
    "\n",
    "with open(f\"../models/model_{sub_folder}/esn_Mood.pkl\", \"rb\") as f:\n",
    "    model_Mood = pickle.load(f)\n",
    "\n",
    "with open(f\"../models/model_{sub_folder}/esn_Genre_Instrument.pkl\", \"rb\") as f:\n",
    "    model_Genre_Instrument = pickle.load(f)\n",
    "\n",
    "with open(f\"../models/model_{sub_folder}/esn_Genre_Mood.pkl\", \"rb\") as f:\n",
    "    model_Genre_Mood = pickle.load(f)\n",
    "\n",
    "with open(f\"../models/model_{sub_folder}/esn_Instrument_Mood.pkl\", \"rb\") as f:\n",
    "    model_Instrument_Mood = pickle.load(f)\n",
    "\n",
    "# Load Transformer model\n",
    "with open(f\"../models/model_{sub_folder}/transformer.pkl\", \"rb\") as f:\n",
    "    model_Transformer = pickle.load(f)\n",
    "\n",
    "# Affichage du nombre de paramètres\n",
    "print(\n",
    "    f\"Nombre total de paramètres : {sum(p.numel() for p in model_Transformer.parameters())}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_genres_reshaped = reshape_input(X_genres)\n",
    "X_instruments_reshaped = reshape_input(X_instruments)\n",
    "X_moods_reshaped = reshape_input(X_moods)\n",
    "\n",
    "y_genres_reshaped = reshape_input(y_genres)\n",
    "y_instruments_reshaped = reshape_input(y_instruments)\n",
    "y_moods_reshaped = reshape_input(y_moods)\n",
    "\n",
    "# Reshape les données croisées pour les ESNs\n",
    "X_genres_instruments_reshaped = reshape_input(X_genres_instruments)\n",
    "X_genres_moods_reshaped = reshape_input(X_genres_moods)\n",
    "X_instruments_moods_reshaped = reshape_input(X_instruments_moods)\n",
    "\n",
    "y_genres_instruments_reshaped = reshape_input(y_genres_instruments)\n",
    "y_genres_moods_reshaped = reshape_input(y_genres_moods)\n",
    "y_instruments_moods_reshaped = reshape_input(y_instruments_moods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-15: 100%|██████████| 47615/47615 [00:01<00:00, 39385.94it/s]\n",
      "Running ESN-16: 100%|██████████| 47615/47615 [00:01<00:00, 37594.38it/s]\n",
      "Running ESN-17: 100%|██████████| 47615/47615 [00:01<00:00, 36894.81it/s]\n",
      "Running ESN-18: 100%|██████████| 47615/47615 [00:01<00:00, 37510.42it/s]\n",
      "Running ESN-19: 100%|██████████| 47615/47615 [00:01<00:00, 34802.33it/s]\n",
      "Running ESN-20: 100%|██████████| 47615/47615 [00:01<00:00, 36987.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# Obtenir les sorties des réservoirs\n",
    "y_genres_pred = model_Genre.run(X_genres_reshaped)\n",
    "y_instruments_pred = model_Instrument.run(X_instruments_reshaped)\n",
    "y_moods_pred = model_Mood.run(X_moods_reshaped)\n",
    "\n",
    "# Formater les prédictions\n",
    "y_genres_pred = format_predictions(y_genres_pred)\n",
    "y_instruments_pred = format_predictions(y_instruments_pred)\n",
    "y_moods_pred = format_predictions(y_moods_pred)\n",
    "\n",
    "# Obtenir les sorties des réservoirs croisés\n",
    "y_genres_instruments_pred = model_Genre_Instrument.run(X_genres_instruments_reshaped)\n",
    "y_genres_moods_pred = model_Genre_Mood.run(X_genres_moods_reshaped)\n",
    "y_instruments_moods_pred = model_Instrument_Mood.run(X_instruments_moods_reshaped)\n",
    "\n",
    "\n",
    "# Formater les prédictions\n",
    "y_genres_instruments_pred = format_predictions(y_genres_instruments_pred)\n",
    "y_genres_moods_pred = format_predictions(y_genres_moods_pred)\n",
    "y_instruments_moods_pred = format_predictions(y_instruments_moods_pred)\n",
    "\n",
    "\n",
    "# Combine toutes les sorties (individuelles et croisées)\n",
    "X_reservoirs = np.concatenate(\n",
    "    [\n",
    "        y_genres_pred,\n",
    "        y_instruments_pred,\n",
    "        y_moods_pred,\n",
    "        y_genres_instruments_pred,\n",
    "        y_genres_moods_pred,\n",
    "        y_instruments_moods_pred,\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des datasets\n",
    "main_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(X_reservoirs, dtype=torch.float32).to(DEVICE),\n",
    "    y_tensor.clone().detach().to(DEVICE),\n",
    ")\n",
    "\n",
    "# Création des loaders\n",
    "main_loader = torch.utils.data.DataLoader(\n",
    "    main_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation des performances (accuracy, precision, recall, f1-score)\n",
    "def evaluate_performance(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            predictions = model(X_batch)\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "            predictions = (predictions > 0.5).int()\n",
    "            y_true.append(y_batch.cpu().numpy())\n",
    "            y_pred.append(predictions.cpu().numpy())\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "    # Save 5% of the rows of the predictions as csv files in the data folder in predictions folder with timestamp\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    np.savetxt(\n",
    "        f\"../data/predictions/y_true_{timestamp}.csv\",\n",
    "        y_true[: int(0.05 * len(y_true))],\n",
    "        delimiter=\",\",\n",
    "    )\n",
    "    np.savetxt(\n",
    "        f\"../data/predictions/y_pred_{timestamp}.csv\",\n",
    "        y_pred[: int(0.05 * len(y_pred))],\n",
    "        delimiter=\",\",\n",
    "    )\n",
    "    \n",
    "    # Histograms plot of the predictions and true values for each tag\n",
    "    # for i in range(y_true.shape[1]):\n",
    "    #     plt.hist(y_true[:, i], bins=2, alpha=0.5, label=\"True\")\n",
    "    #     plt.hist(y_pred[:, i], bins=2, alpha=0.5, label=\"Predicted\")\n",
    "    #     plt.title(f\"Tag {i}\")\n",
    "    #     plt.legend()\n",
    "    #     plt.savefig(f\"../data/predictions/histogram_tag_{i}.png\")\n",
    "    #     plt.clf()\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # Precision, Recall, F1-Score\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    report = classification_report(y_true, y_pred, digits=4)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9901951302957526\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000         2\n",
      "           1     0.6732    0.7891    0.7265      1261\n",
      "           2     0.0000    0.0000    0.0000         0\n",
      "           3     0.0000    0.0000    0.0000         0\n",
      "           4     0.2439    0.9091    0.3846        11\n",
      "           5     0.1166    0.8852    0.2061        61\n",
      "           6     0.1622    0.9231    0.2759        13\n",
      "           7     0.1055    0.7073    0.1835        41\n",
      "           8     0.4561    0.7391    0.5641       506\n",
      "           9     0.1223    0.9583    0.2170        24\n",
      "          10     0.7522    0.9767    0.8498      1243\n",
      "          11     0.8600    0.8521    0.8561      4226\n",
      "          12     0.3720    0.9045    0.5272       220\n",
      "          13     0.1412    0.8404    0.2418       188\n",
      "          14     0.1492    0.7833    0.2507       120\n",
      "          15     0.1768    0.9667    0.2990        60\n",
      "          16     0.0000    0.0000    0.0000         0\n",
      "          17     0.2581    0.3077    0.2807        26\n",
      "          18     0.0000    0.0000    0.0000         1\n",
      "          19     0.0565    0.6190    0.1036        21\n",
      "          20     0.7676    0.9410    0.8455      1306\n",
      "          21     0.2231    1.0000    0.3648        29\n",
      "          22     0.2133    0.9412    0.3478        34\n",
      "          23     0.5811    0.7260    0.6456       730\n",
      "          24     0.5432    0.6111    0.5752       504\n",
      "          25     0.0000    0.0000    0.0000         0\n",
      "          26     0.0539    0.6429    0.0994        14\n",
      "          27     0.5084    0.8590    0.6387      3213\n",
      "          28     0.8183    0.7891    0.8034      1427\n",
      "          29     0.2392    0.8403    0.3724       119\n",
      "          30     0.0806    0.8333    0.1471         6\n",
      "          31     0.0000    0.0000    0.0000         0\n",
      "          32     0.1923    1.0000    0.3226         5\n",
      "          33     0.5343    0.8068    0.6429       647\n",
      "          34     0.6615    0.8828    0.7563       290\n",
      "          35     0.6297    0.7813    0.6974       814\n",
      "          36     0.3915    0.7685    0.5187       298\n",
      "          37     0.0292    0.5714    0.0556        14\n",
      "          38     0.2107    0.8148    0.3349       135\n",
      "          39     0.4412    0.9172    0.5958      1293\n",
      "          40     0.0870    1.0000    0.1600         2\n",
      "          41     0.1824    0.8378    0.2995        37\n",
      "          42     0.0584    0.8000    0.1088        10\n",
      "          43     0.0000    0.0000    0.0000         0\n",
      "          44     0.0370    0.6667    0.0702         3\n",
      "          45     0.3588    0.7598    0.4874       229\n",
      "          46     0.0400    1.0000    0.0769         2\n",
      "          47     0.1429    1.0000    0.2500         1\n",
      "          48     0.5231    0.9358    0.6711       109\n",
      "          49     0.2500    0.6667    0.3636         6\n",
      "          50     0.0000    0.0000    0.0000         0\n",
      "          51     0.0000    0.0000    0.0000         0\n",
      "          52     0.0000    0.0000    0.0000         0\n",
      "          53     0.9181    0.8600    0.8881       300\n",
      "          54     0.1236    0.8421    0.2155        38\n",
      "          55     0.4219    0.7098    0.5292       255\n",
      "          56     0.0000    0.0000    0.0000         0\n",
      "          57     0.0000    0.0000    0.0000         0\n",
      "          58     0.0000    0.0000    0.0000         0\n",
      "          59     0.4088    0.9487    0.5714        78\n",
      "          60     0.0000    0.0000    0.0000         0\n",
      "          61     0.3252    0.8760    0.4743       121\n",
      "          62     0.0000    0.0000    0.0000         0\n",
      "          63     0.3217    0.8707    0.4698       116\n",
      "          64     0.0890    0.3333    0.1405        39\n",
      "          65     0.0571    0.5714    0.1039         7\n",
      "          66     0.3990    0.8691    0.5469       275\n",
      "          67     0.0290    0.4000    0.0541         5\n",
      "          68     0.1630    0.9565    0.2785        23\n",
      "          69     0.1368    0.8000    0.2336        20\n",
      "          70     0.2265    0.5942    0.3280        69\n",
      "          71     0.0000    0.0000    0.0000         0\n",
      "          72     0.0000    0.0000    0.0000         0\n",
      "          73     0.4097    0.8532    0.5536       109\n",
      "          74     0.0000    0.0000    0.0000         0\n",
      "          75     0.1000    1.0000    0.1818         1\n",
      "          76     0.0610    1.0000    0.1149         5\n",
      "          77     0.0654    1.0000    0.1227        10\n",
      "          78     0.0000    0.0000    0.0000         0\n",
      "          79     0.0120    1.0000    0.0238         1\n",
      "          80     0.2275    0.9355    0.3659        62\n",
      "          81     0.5593    0.9316    0.6990      1346\n",
      "          82     0.1111    0.9474    0.1989        19\n",
      "          83     0.3091    0.8500    0.4533        20\n",
      "          84     0.1230    0.9583    0.2180        24\n",
      "          85     0.0000    0.0000    0.0000         0\n",
      "          86     0.6457    0.8925    0.7493      1172\n",
      "          87     0.3889    0.8268    0.5290       485\n",
      "          88     0.0000    0.0000    0.0000         0\n",
      "          89     0.0754    0.7872    0.1375        47\n",
      "          90     0.9265    0.8289    0.8750       152\n",
      "          91     0.7483    0.8413    0.7921       523\n",
      "          92     0.8412    0.9108    0.8746      5595\n",
      "          93     0.2905    0.8125    0.4280       128\n",
      "          94     0.4642    0.9462    0.6228       130\n",
      "          95     0.8422    0.8056    0.8235      8587\n",
      "          96     0.5283    0.8485    0.6512        33\n",
      "          97     0.1134    0.8462    0.2000        13\n",
      "          98     0.0952    0.6667    0.1667         3\n",
      "          99     0.7133    0.7604    0.7361       409\n",
      "         100     0.8323    0.8253    0.8288      4419\n",
      "         101     0.4599    0.9430    0.6183       158\n",
      "         102     0.5319    0.7592    0.6255       955\n",
      "         103     0.0000    0.0000    0.0000         0\n",
      "         104     0.0000    0.0000    0.0000         0\n",
      "         105     0.1569    1.0000    0.2712         8\n",
      "         106     0.7599    0.8889    0.8194       495\n",
      "         107     0.5534    0.9426    0.6973       418\n",
      "         108     0.3000    0.7500    0.4286         4\n",
      "         109     0.0757    0.7368    0.1373        19\n",
      "         110     0.3000    0.8000    0.4364        15\n",
      "         111     0.0930    1.0000    0.1702         4\n",
      "         112     0.5581    0.6154    0.5854        39\n",
      "         113     0.5025    0.8599    0.6343       714\n",
      "         114     0.1250    1.0000    0.2222         1\n",
      "         115     0.8884    0.7705    0.8252      9555\n",
      "         116     0.7267    0.9133    0.8094      7125\n",
      "         117     0.0725    0.5556    0.1282         9\n",
      "         118     0.9215    0.8480    0.8832     12549\n",
      "         119     0.2847    0.7885    0.4184        52\n",
      "         120     0.2262    0.8333    0.3559        60\n",
      "         121     0.1429    0.6667    0.2353         3\n",
      "         122     0.7986    0.9052    0.8486      1919\n",
      "         123     0.1762    0.9487    0.2972        39\n",
      "         124     0.0000    0.0000    0.0000         0\n",
      "         125     0.5798    0.8995    0.7051       925\n",
      "         126     0.4772    0.9181    0.6280       354\n",
      "         127     0.1818    0.9333    0.3043        15\n",
      "         128     0.3495    0.9612    0.5126       232\n",
      "         129     0.4000    0.7209    0.5145       172\n",
      "         130     0.3499    0.9509    0.5116       163\n",
      "         131     0.3729    0.9741    0.5394       116\n",
      "         132     0.4852    0.8333    0.6133       354\n",
      "         133     0.4727    1.0000    0.6420        78\n",
      "         134     0.3571    0.4348    0.3922        23\n",
      "         135     0.7345    0.7998    0.7658       979\n",
      "         136     0.3750    0.6750    0.4821        40\n",
      "         137     0.5980    0.8026    0.6854        76\n",
      "         138     0.3262    0.7243    0.4498       272\n",
      "         139     0.3429    1.0000    0.5106        24\n",
      "         140     0.5517    0.9231    0.6906        52\n",
      "         141     0.8937    0.8517    0.8722      3229\n",
      "         142     0.3333    0.7761    0.4664        67\n",
      "         143     0.5176    0.9419    0.6680       172\n",
      "         144     0.5121    0.8509    0.6394       322\n",
      "         145     0.0000    0.0000    0.0000         0\n",
      "         146     0.2250    1.0000    0.3673         9\n",
      "         147     0.8148    0.8800    0.8462        25\n",
      "         148     0.6136    0.7941    0.6923       136\n",
      "         149     0.1754    0.8000    0.2878        25\n",
      "         150     0.6494    0.6873    0.6678       291\n",
      "         151     0.6977    0.9677    0.8108        31\n",
      "         152     0.0000    0.0000    0.0000         0\n",
      "         153     0.2963    1.0000    0.4571        32\n",
      "         154     0.2711    0.8315    0.4088        89\n",
      "         155     0.7880    0.8759    0.8296      8017\n",
      "         156     0.3750    0.6000    0.4615         5\n",
      "         157     0.7086    0.9144    0.7984       444\n",
      "         158     0.3813    0.7426    0.5039       746\n",
      "         159     0.6303    0.5542    0.5898       240\n",
      "         160     0.6562    0.9130    0.7636        23\n",
      "         161     0.8991    0.7397    0.8116      4648\n",
      "         162     0.6509    0.8541    0.7387       932\n",
      "         163     0.1475    0.9000    0.2535        10\n",
      "         164     0.1010    0.9615    0.1828        52\n",
      "         165     0.4286    0.9643    0.5934        28\n",
      "         166     0.5748    0.9733    0.7228        75\n",
      "         167     0.2179    0.8500    0.3469        20\n",
      "         168     0.4606    0.8736    0.6032        87\n",
      "         169     0.8617    0.9553    0.9061      1520\n",
      "         170     0.0000    0.0000    0.0000         0\n",
      "         171     0.0000    0.0000    0.0000         0\n",
      "         172     0.7189    0.8892    0.7951      5172\n",
      "         173     0.8718    0.7585    0.8112      2484\n",
      "         174     0.6326    0.8953    0.7414      1604\n",
      "         175     0.5010    0.9789    0.6628       995\n",
      "         176     0.0000    0.0000    0.0000         0\n",
      "         177     0.7814    0.9324    0.8502     11886\n",
      "         178     0.0000    0.0000    0.0000         0\n",
      "         179     0.4429    0.9688    0.6078        32\n",
      "         180     0.1667    0.8438    0.2784        32\n",
      "         181     0.4028    0.4028    0.4028        72\n",
      "         182     0.5532    0.8619    0.6739       181\n",
      "         183     0.6667    1.0000    0.8000         2\n",
      "         184     0.3380    0.8000    0.4752        30\n",
      "         185     0.4313    0.8911    0.5813       349\n",
      "         186     0.4478    0.7317    0.5556        41\n",
      "         187     0.0000    0.0000    0.0000         0\n",
      "         188     0.3120    0.8425    0.4553       127\n",
      "         189     0.7324    0.9563    0.8295       229\n",
      "         190     0.4192    0.7830    0.5461       106\n",
      "         191     0.0625    0.2500    0.1000         4\n",
      "         192     0.6051    0.9658    0.7440       790\n",
      "         193     0.8622    0.9636    0.9101      7583\n",
      "         194     0.0000    0.0000    0.0000         0\n",
      "         195     0.0000    0.0000    0.0000         0\n",
      "         196     0.2762    0.7576    0.4049        66\n",
      "         197     0.3519    0.8261    0.4935        23\n",
      "         198     0.7068    0.9247    0.8012       146\n",
      "         199     0.5074    0.8327    0.6305       496\n",
      "         200     0.0752    0.9091    0.1389        11\n",
      "         201     0.3000    0.8793    0.4474        58\n",
      "         202     0.1903    0.7468    0.3033        79\n",
      "         203     0.1475    0.8871    0.2529        62\n",
      "         204     0.7113    0.7245    0.7178      1136\n",
      "         205     0.6145    0.8339    0.7076      1722\n",
      "         206     0.3031    0.7585    0.4331       679\n",
      "         207     0.0000    0.0000    0.0000         0\n",
      "         208     0.4576    0.8714    0.6000      4486\n",
      "         209     0.2336    0.8770    0.3690       122\n",
      "         210     0.0000    0.0000    0.0000         2\n",
      "         211     0.5126    0.6671    0.5797      2872\n",
      "         212     0.5031    0.9183    0.6500       808\n",
      "         213     0.0932    1.0000    0.1705        15\n",
      "         214     0.1125    0.6863    0.1934       102\n",
      "         215     0.0881    0.6071    0.1538        28\n",
      "         216     0.2346    0.6747    0.3481       372\n",
      "         217     0.6258    0.7502    0.6824      2858\n",
      "         218     0.1256    0.6842    0.2122        38\n",
      "         219     0.3553    0.7245    0.4768       617\n",
      "         220     0.1946    0.8190    0.3144       105\n",
      "         221     0.0059    1.0000    0.0117         1\n",
      "         222     0.2729    0.8826    0.4169       409\n",
      "         223     0.3427    0.7647    0.4733       255\n",
      "         224     0.5644    0.8702    0.6847      1364\n",
      "         225     0.0125    1.0000    0.0247         1\n",
      "         226     0.0165    0.4000    0.0317         5\n",
      "         227     0.2020    0.9017    0.3300       295\n",
      "         228     0.6221    0.8082    0.7031      5272\n",
      "         229     0.3655    0.7020    0.4807      1047\n",
      "         230     0.0213    0.6667    0.0412         3\n",
      "         231     0.0355    1.0000    0.0685        10\n",
      "         232     0.1023    0.8182    0.1818        11\n",
      "         233     0.3356    0.8297    0.4779       957\n",
      "         234     0.7002    0.7650    0.7312      4519\n",
      "         235     0.5335    0.8980    0.6693      1960\n",
      "         236     0.4170    0.8417    0.5577       916\n",
      "         237     0.2071    0.9318    0.3388        88\n",
      "         238     0.0367    0.9565    0.0707        23\n",
      "         239     0.0000    0.0000    0.0000         0\n",
      "         240     0.6565    0.9444    0.7745      2339\n",
      "         241     0.0000    0.0000    0.0000         0\n",
      "         242     0.4119    0.8537    0.5556      1579\n",
      "         243     0.3501    0.8211    0.4909       738\n",
      "         244     0.0000    0.0000    0.0000         0\n",
      "         245     0.4301    0.8481    0.5707      1244\n",
      "         246     0.2025    0.6633    0.3103        98\n",
      "         247     0.4708    0.8071    0.5947      1410\n",
      "\n",
      "   micro avg     0.6269    0.8486    0.7211    176397\n",
      "   macro avg     0.3267    0.6994    0.4055    176397\n",
      "weighted avg     0.7067    0.8486    0.7563    176397\n",
      " samples avg     0.6202    0.8191    0.6763    176397\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "evaluate_performance(model_Transformer, main_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
