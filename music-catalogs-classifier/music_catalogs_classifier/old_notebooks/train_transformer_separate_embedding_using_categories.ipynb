{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from reservoirpy.nodes import Reservoir, Ridge, ESN\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_SIZE = 128\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 8\n",
    "DROPOUT = 0.1\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 30\n",
    "PATIENCE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    partOfData = 1\n",
    "    X_genres = pd.read_csv(\"../data/train/input_genres_tags_data.csv\")\n",
    "    X_instruments = pd.read_csv(\"../data/train/input_instruments_tags_data.csv\")\n",
    "    X_moods = pd.read_csv(\"../data/train/input_moods_tags_data.csv\")\n",
    "    X_genres_categories = pd.read_csv(\"../data/train/input_genres_categories_data.csv\")\n",
    "    X_instruments_categories = pd.read_csv(\n",
    "        \"../data/train/input_instruments_categories_data.csv\"\n",
    "    )\n",
    "    X_moods_categories = pd.read_csv(\"../data/train/input_moods_categories_data.csv\")\n",
    "\n",
    "    y_genres = pd.read_csv(\"../data/train/output_genres_tags_data.csv\")\n",
    "    y_instruments = pd.read_csv(\"../data/train/output_instruments_tags_data.csv\")\n",
    "    y_moods = pd.read_csv(\"../data/train/output_moods_tags_data.csv\")\n",
    "\n",
    "    # On peut garder seulement une partie des données\n",
    "    X_genres = X_genres[: int(partOfData * len(X_genres))]\n",
    "    X_instruments = X_instruments[: int(partOfData * len(X_instruments))]\n",
    "    X_moods = X_moods[: int(partOfData * len(X_moods))]\n",
    "    y_genres = y_genres[: int(partOfData * len(y_genres))]\n",
    "    y_instruments = y_instruments[: int(partOfData * len(y_instruments))]\n",
    "    y_moods = y_moods[: int(partOfData * len(y_moods))]\n",
    "    X_genres_categories = X_genres_categories[\n",
    "        : int(partOfData * len(X_genres_categories))\n",
    "    ]\n",
    "    X_instruments_categories = X_instruments_categories[\n",
    "        : int(partOfData * len(X_instruments_categories))\n",
    "    ]\n",
    "    X_moods_categories = X_moods_categories[: int(partOfData * len(X_moods_categories))]\n",
    "\n",
    "    return (\n",
    "        X_genres,\n",
    "        X_instruments,\n",
    "        X_moods,\n",
    "        X_genres_categories,\n",
    "        X_instruments_categories,\n",
    "        X_moods_categories,\n",
    "    ), (y_genres, y_instruments, y_moods)\n",
    "\n",
    "\n",
    "# Ensure the input data is in the correct format\n",
    "def reshape_input(X):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        return X.values.reshape(-1, 1, X.shape[1])  # Handles pandas DataFrame\n",
    "    elif isinstance(X, np.ndarray):\n",
    "        return X.reshape(-1, 1, X.shape[1])  # Handles numpy ndarray\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a pandas DataFrame or a numpy ndarray\")\n",
    "\n",
    "\n",
    "def format_predictions(predictions):\n",
    "    # Convert the list to a NumPy array\n",
    "    predictions_array = np.array(predictions)\n",
    "\n",
    "    # Reshape the array to 2-dimensional\n",
    "    predictions_reshaped = predictions_array.reshape(-1, predictions_array.shape[-1])\n",
    "\n",
    "    return predictions_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données\n",
    "(\n",
    "    (\n",
    "        X_genres,\n",
    "        X_instruments,\n",
    "        X_moods,\n",
    "        X_genres_categories,\n",
    "        X_instruments_categories,\n",
    "        X_moods_categories,\n",
    "    ),\n",
    "    (y_genres, y_instruments, y_moods),\n",
    ") = load_data()\n",
    "\n",
    "# Train-test split\n",
    "X_genres_train, X_genres_test, y_genres_train, y_genres_test = train_test_split(\n",
    "    X_genres, y_genres, test_size=0.2, random_state=42\n",
    ")\n",
    "X_instruments_train, X_instruments_test, y_instruments_train, y_instruments_test = (\n",
    "    train_test_split(X_instruments, y_instruments, test_size=0.2, random_state=42)\n",
    ")\n",
    "X_moods_train, X_moods_test, y_moods_train, y_moods_test = train_test_split(\n",
    "    X_moods, y_moods, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train-test split for categories\n",
    "X_genres_categories_train, X_genres_categories_test = train_test_split(\n",
    "    X_genres_categories, test_size=0.2, random_state=42\n",
    ")\n",
    "X_instruments_categories_train, X_instruments_categories_test = train_test_split(\n",
    "    X_instruments_categories, test_size=0.2, random_state=42\n",
    ")\n",
    "X_moods_categories_train, X_moods_categories_test = train_test_split(\n",
    "    X_moods_categories, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données\n",
    "X_genres_train = X_genres_train.drop(columns=[\"ChallengeID\"])\n",
    "X_instruments_train = X_instruments_train.drop(columns=[\"ChallengeID\"])\n",
    "X_moods_train = X_moods_train.drop(columns=[\"ChallengeID\"])\n",
    "y_genres_train = y_genres_train.drop(columns=[\"ChallengeID\"])\n",
    "y_instruments_train = y_instruments_train.drop(columns=[\"ChallengeID\"])\n",
    "y_moods_train = y_moods_train.drop(columns=[\"ChallengeID\"])\n",
    "X_genres_categories_train = X_genres_categories_train.drop(columns=[\"ChallengeID\"])\n",
    "X_instruments_categories_train = X_instruments_categories_train.drop(\n",
    "    columns=[\"ChallengeID\"]\n",
    ")\n",
    "X_moods_categories_train = X_moods_categories_train.drop(columns=[\"ChallengeID\"])\n",
    "\n",
    "X_genres_test = X_genres_test.drop(columns=[\"ChallengeID\"])\n",
    "X_instruments_test = X_instruments_test.drop(columns=[\"ChallengeID\"])\n",
    "X_moods_test = X_moods_test.drop(columns=[\"ChallengeID\"])\n",
    "y_genres_test = y_genres_test.drop(columns=[\"ChallengeID\"])\n",
    "y_instruments_test = y_instruments_test.drop(columns=[\"ChallengeID\"])\n",
    "y_moods_test = y_moods_test.drop(columns=[\"ChallengeID\"])\n",
    "X_genres_categories_test = X_genres_categories_test.drop(columns=[\"ChallengeID\"])\n",
    "X_instruments_categories_test = X_instruments_categories_test.drop(\n",
    "    columns=[\"ChallengeID\"]\n",
    ")\n",
    "X_moods_categories_test = X_moods_categories_test.drop(columns=[\"ChallengeID\"])\n",
    "\n",
    "\n",
    "X_train = np.concatenate(\n",
    "    [\n",
    "        X_genres_train,\n",
    "        X_instruments_train,\n",
    "        X_moods_train,\n",
    "        X_genres_categories_train,\n",
    "        X_instruments_categories_train,\n",
    "        X_moods_categories_train,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "X_test = np.concatenate(\n",
    "    [\n",
    "        X_genres_test,\n",
    "        X_instruments_test,\n",
    "        X_moods_test,\n",
    "        X_genres_categories_test,\n",
    "        X_instruments_categories_test,\n",
    "        X_moods_categories_test,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "y_train = np.concatenate([y_genres_train, y_instruments_train, y_moods_train], axis=1)\n",
    "y_test = np.concatenate([y_genres_test, y_instruments_test, y_moods_test], axis=1)\n",
    "\n",
    "# Convertir les données en tensors PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(DEVICE)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(DEVICE)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# Générer les données croisées pour les interactions\n",
    "X_genres_instruments_train = np.concatenate(\n",
    "    [X_genres_train, X_instruments_train], axis=1\n",
    ")\n",
    "X_genres_moods_train = np.concatenate([X_genres_train, X_moods_train], axis=1)\n",
    "X_instruments_moods_train = np.concatenate([X_instruments_train, X_moods_train], axis=1)\n",
    "y_genres_instruments_train = np.concatenate(\n",
    "    [y_genres_train, y_instruments_train], axis=1\n",
    ")\n",
    "y_genres_moods_train = np.concatenate([y_genres_train, y_moods_train], axis=1)\n",
    "y_instruments_moods_train = np.concatenate([y_instruments_train, y_moods_train], axis=1)\n",
    "\n",
    "X_genres_instruments_test = np.concatenate([X_genres_test, X_instruments_test], axis=1)\n",
    "X_genres_moods_test = np.concatenate([X_genres_test, X_moods_test], axis=1)\n",
    "X_instruments_moods_test = np.concatenate([X_instruments_test, X_moods_test], axis=1)\n",
    "y_genres_instruments_test = np.concatenate([y_genres_test, y_instruments_test], axis=1)\n",
    "y_genres_moods_test = np.concatenate([y_genres_test, y_moods_test], axis=1)\n",
    "y_instruments_moods_test = np.concatenate([y_instruments_test, y_moods_test], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des réservoirs (genre, instrument, mood)\n",
    "reservoir_Genre = Reservoir(\n",
    "    units=100,\n",
    "    sr=0,  # Spectral radius\n",
    "    lr=1,  # Leak rate\n",
    "    input_scaling=1.0,\n",
    ")\n",
    "\n",
    "reservoir_Instrument = Reservoir(units=100, sr=0, lr=1, input_scaling=1.0)\n",
    "\n",
    "reservoir_Mood = Reservoir(units=100, sr=0, lr=1, input_scaling=1.0)\n",
    "\n",
    "# Readout pour chaque réservoir\n",
    "readout_Genre = Ridge(ridge=1e-4)\n",
    "readout_Instrument = Ridge(ridge=1e-4)\n",
    "readout_Mood = Ridge(ridge=1e-4)\n",
    "\n",
    "# Création des modèles avec ESN (Echo State Network)\n",
    "model_Genre = ESN(reservoir=reservoir_Genre, readout=readout_Genre, workers=-1)\n",
    "model_Instrument = ESN(\n",
    "    reservoir=reservoir_Instrument, readout=readout_Instrument, workers=-1\n",
    ")\n",
    "model_Mood = ESN(reservoir=reservoir_Mood, readout=readout_Mood, workers=-1)\n",
    "\n",
    "# Réservoirs pour les interactions croisées\n",
    "reservoir_Genre_Instrument = Reservoir(units=100, sr=0, lr=1, input_scaling=1.0)\n",
    "reservoir_Genre_Mood = Reservoir(units=100, sr=0, lr=1, input_scaling=1.0)\n",
    "reservoir_Instrument_Mood = Reservoir(units=100, sr=0, lr=1, input_scaling=1.0)\n",
    "\n",
    "# Readouts pour ces réservoirs\n",
    "readout_Genre_Instrument = Ridge(ridge=1e-4)\n",
    "readout_Genre_Mood = Ridge(ridge=1e-4)\n",
    "readout_Instrument_Mood = Ridge(ridge=1e-4)\n",
    "\n",
    "# Modèles ESN pour les interactions croisées\n",
    "model_Genre_Instrument = ESN(\n",
    "    reservoir=reservoir_Genre_Instrument, readout=readout_Genre_Instrument, workers=-1\n",
    ")\n",
    "model_Genre_Mood = ESN(\n",
    "    reservoir=reservoir_Genre_Mood, readout=readout_Genre_Mood, workers=-1\n",
    ")\n",
    "model_Instrument_Mood = ESN(\n",
    "    reservoir=reservoir_Instrument_Mood, readout=readout_Instrument_Mood, workers=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-0: 100%|██████████| 88683/88683 [00:10<00:00, 8599.17it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-1: 100%|██████████| 88683/88683 [00:10<00:00, 8854.15it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-2: 100%|██████████| 88683/88683 [00:09<00:00, 9478.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-3: 100%|██████████| 88683/88683 [00:14<00:00, 6176.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-4: 100%|██████████| 88683/88683 [00:13<00:00, 6539.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-5: 100%|██████████| 88683/88683 [00:13<00:00, 6722.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-5...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ESN-5': ESN('Reservoir-5', 'Ridge-5')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_genres_train_reshaped = reshape_input(X_genres_train)\n",
    "X_instruments_train_reshaped = reshape_input(X_instruments_train)\n",
    "X_moods_train_reshaped = reshape_input(X_moods_train)\n",
    "\n",
    "y_genres_train_reshaped = reshape_input(y_genres_train)\n",
    "y_instruments_train_reshaped = reshape_input(y_instruments_train)\n",
    "y_moods_train_reshaped = reshape_input(y_moods_train)\n",
    "\n",
    "X_genres_test_reshaped = reshape_input(X_genres_test)\n",
    "X_instruments_test_reshaped = reshape_input(X_instruments_test)\n",
    "X_moods_test_reshaped = reshape_input(X_moods_test)\n",
    "\n",
    "y_genres_test_reshaped = reshape_input(y_genres_test)\n",
    "y_instruments_test_reshaped = reshape_input(y_instruments_test)\n",
    "y_moods_test_reshaped = reshape_input(y_moods_test)\n",
    "\n",
    "\n",
    "# Entraîner les réservoirs\n",
    "# Train the models with one line for single timestep\n",
    "model_Genre.fit(X_genres_train_reshaped, y_genres_train_reshaped)\n",
    "model_Instrument.fit(X_instruments_train_reshaped, y_instruments_train_reshaped)\n",
    "model_Mood.fit(X_moods_train_reshaped, y_moods_train_reshaped)\n",
    "\n",
    "\n",
    "# Reshape les données croisées pour les ESNs\n",
    "X_genres_instruments_train_reshaped = reshape_input(X_genres_instruments_train)\n",
    "X_genres_moods_train_reshaped = reshape_input(X_genres_moods_train)\n",
    "X_instruments_moods_train_reshaped = reshape_input(X_instruments_moods_train)\n",
    "\n",
    "y_genres_instruments_train_reshaped = reshape_input(y_genres_instruments_train)\n",
    "y_genres_moods_train_reshaped = reshape_input(y_genres_moods_train)\n",
    "y_instruments_moods_train_reshaped = reshape_input(y_instruments_moods_train)\n",
    "\n",
    "X_genres_instruments_test_reshaped = reshape_input(X_genres_instruments_test)\n",
    "X_genres_moods_test_reshaped = reshape_input(X_genres_moods_test)\n",
    "X_instruments_moods_test_reshaped = reshape_input(X_instruments_moods_test)\n",
    "\n",
    "y_genres_instruments_test_reshaped = reshape_input(y_genres_instruments_test)\n",
    "y_genres_moods_test_reshaped = reshape_input(y_genres_moods_test)\n",
    "y_instruments_moods_test_reshaped = reshape_input(y_instruments_moods_test)\n",
    "\n",
    "# Entraîner les ESNs\n",
    "model_Genre_Instrument.fit(\n",
    "    X_genres_instruments_train_reshaped, y_genres_instruments_train_reshaped\n",
    ")\n",
    "model_Genre_Mood.fit(X_genres_moods_train_reshaped, y_genres_moods_train_reshaped)\n",
    "model_Instrument_Mood.fit(\n",
    "    X_instruments_moods_train_reshaped, y_instruments_moods_train_reshaped\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-0: 100%|██████████| 88683/88683 [00:03<00:00, 28957.10it/s]\n",
      "Running ESN-1: 100%|██████████| 88683/88683 [00:03<00:00, 23587.39it/s]\n",
      "Running ESN-2: 100%|██████████| 88683/88683 [00:03<00:00, 28699.05it/s]\n",
      "Running ESN-0: 100%|██████████| 22171/22171 [00:00<00:00, 29592.02it/s]\n",
      "Running ESN-1: 100%|██████████| 22171/22171 [00:00<00:00, 25230.13it/s]\n",
      "Running ESN-2: 100%|██████████| 22171/22171 [00:00<00:00, 25757.56it/s]\n",
      "Running ESN-3: 100%|██████████| 88683/88683 [00:03<00:00, 22920.55it/s]\n",
      "Running ESN-4: 100%|██████████| 88683/88683 [00:03<00:00, 22981.38it/s]\n",
      "Running ESN-5: 100%|██████████| 88683/88683 [00:03<00:00, 22675.91it/s]\n",
      "Running ESN-3: 100%|██████████| 22171/22171 [00:01<00:00, 21889.01it/s]\n",
      "Running ESN-4: 100%|██████████| 22171/22171 [00:00<00:00, 26532.03it/s]\n",
      "Running ESN-5: 100%|██████████| 22171/22171 [00:01<00:00, 21420.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# Obtenir les sorties des réservoirs\n",
    "y_genres_train_pred = model_Genre.run(X_genres_train_reshaped)\n",
    "y_instruments_train_pred = model_Instrument.run(X_instruments_train_reshaped)\n",
    "y_moods_train_pred = model_Mood.run(X_moods_train_reshaped)\n",
    "\n",
    "y_genres_test_pred = model_Genre.run(X_genres_test_reshaped)\n",
    "y_instruments_test_pred = model_Instrument.run(X_instruments_test_reshaped)\n",
    "y_moods_test_pred = model_Mood.run(X_moods_test_reshaped)\n",
    "\n",
    "# Formater les prédictions\n",
    "y_genres_train_pred = format_predictions(y_genres_train_pred)\n",
    "y_instruments_train_pred = format_predictions(y_instruments_train_pred)\n",
    "y_moods_train_pred = format_predictions(y_moods_train_pred)\n",
    "\n",
    "y_genres_test_pred = format_predictions(y_genres_test_pred)\n",
    "y_instruments_test_pred = format_predictions(y_instruments_test_pred)\n",
    "y_moods_test_pred = format_predictions(y_moods_test_pred)\n",
    "\n",
    "\n",
    "# Obtenir les sorties des réservoirs croisés\n",
    "y_genres_instruments_train_pred = model_Genre_Instrument.run(\n",
    "    X_genres_instruments_train_reshaped\n",
    ")\n",
    "y_genres_moods_train_pred = model_Genre_Mood.run(X_genres_moods_train_reshaped)\n",
    "y_instruments_moods_train_pred = model_Instrument_Mood.run(\n",
    "    X_instruments_moods_train_reshaped\n",
    ")\n",
    "\n",
    "y_genres_instruments_test_pred = model_Genre_Instrument.run(\n",
    "    X_genres_instruments_test_reshaped\n",
    ")\n",
    "y_genres_moods_test_pred = model_Genre_Mood.run(X_genres_moods_test_reshaped)\n",
    "y_instruments_moods_test_pred = model_Instrument_Mood.run(\n",
    "    X_instruments_moods_test_reshaped\n",
    ")\n",
    "\n",
    "# Formater les prédictions\n",
    "y_genres_instruments_train_pred = format_predictions(y_genres_instruments_train_pred)\n",
    "y_genres_moods_train_pred = format_predictions(y_genres_moods_train_pred)\n",
    "y_instruments_moods_train_pred = format_predictions(y_instruments_moods_train_pred)\n",
    "\n",
    "y_genres_instruments_test_pred = format_predictions(y_genres_instruments_test_pred)\n",
    "y_genres_moods_test_pred = format_predictions(y_genres_moods_test_pred)\n",
    "y_instruments_moods_test_pred = format_predictions(y_instruments_moods_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all outputs (individual and cross) into a dictionary\n",
    "X_train_final = {\n",
    "    \"X_genres\": X_genres_train,\n",
    "    \"X_instruments\": X_instruments_train,\n",
    "    \"X_moods\": X_moods_train,\n",
    "    \"X_genres_categories\": X_genres_categories_train,\n",
    "    \"X_instruments_categories\": X_instruments_categories_train,\n",
    "    \"X_moods_categories\": X_moods_categories_train,\n",
    "    \"y_genres_pred\": y_genres_train_pred,\n",
    "    \"y_instruments_pred\": y_instruments_train_pred,\n",
    "    \"y_moods_pred\": y_moods_train_pred,\n",
    "    \"y_genres_instruments_pred\": y_genres_instruments_train_pred,\n",
    "    \"y_genres_moods_pred\": y_genres_moods_train_pred,\n",
    "    \"y_instruments_moods_pred\": y_instruments_moods_train_pred,\n",
    "}\n",
    "\n",
    "X_test_final = {\n",
    "    \"X_genres\": X_genres_test,\n",
    "    \"X_instruments\": X_instruments_test,\n",
    "    \"X_moods\": X_moods_test,\n",
    "    \"X_genres_categories\": X_genres_categories_test,\n",
    "    \"X_instruments_categories\": X_instruments_categories_test,\n",
    "    \"X_moods_categories\": X_moods_categories_test,\n",
    "    \"y_genres_pred\": y_genres_test_pred,\n",
    "    \"y_instruments_pred\": y_instruments_test_pred,\n",
    "    \"y_moods_pred\": y_moods_test_pred,\n",
    "    \"y_genres_instruments_pred\": y_genres_instruments_test_pred,\n",
    "    \"y_genres_moods_pred\": y_genres_moods_test_pred,\n",
    "    \"y_instruments_moods_pred\": y_instruments_moods_test_pred,\n",
    "}\n",
    "\n",
    "# Define input sizes for the embeddings\n",
    "input_size_dict = {\n",
    "    \"X_genres\": X_genres_train.shape[1],\n",
    "    \"X_instruments\": X_instruments_train.shape[1],\n",
    "    \"X_moods\": X_moods_train.shape[1],\n",
    "    \"X_genres_categories\": X_genres_categories_train.shape[1],\n",
    "    \"X_instruments_categories\": X_instruments_categories_train.shape[1],\n",
    "    \"X_moods_categories\": X_moods_categories_train.shape[1],\n",
    "    \"y_genres_pred\": y_genres_train_pred.shape[1],\n",
    "    \"y_instruments_pred\": y_instruments_train_pred.shape[1],\n",
    "    \"y_moods_pred\": y_moods_train_pred.shape[1],\n",
    "    \"y_genres_instruments_pred\": y_genres_instruments_train_pred.shape[1],\n",
    "    \"y_genres_moods_pred\": y_genres_moods_train_pred.shape[1],\n",
    "    \"y_instruments_moods_pred\": y_instruments_moods_train_pred.shape[1],\n",
    "}\n",
    "\n",
    "# Define the output size\n",
    "output_size_dict = {\n",
    "    \"y_genres\": y_genres_train.shape[1],\n",
    "    \"y_instruments\": y_instruments_train.shape[1],\n",
    "    \"y_moods\": y_moods_train.shape[1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_genres: 90\n",
      "X_instruments: 112\n",
      "X_moods: 46\n",
      "X_genres_categories: 18\n",
      "X_instruments_categories: 15\n",
      "X_moods_categories: 8\n",
      "y_genres_pred: 90\n",
      "y_instruments_pred: 112\n",
      "y_moods_pred: 46\n",
      "y_genres_instruments_pred: 202\n",
      "y_genres_moods_pred: 136\n",
      "y_instruments_moods_pred: 158\n",
      "y_genres: 90\n",
      "y_instruments: 112\n",
      "y_moods: 46\n"
     ]
    }
   ],
   "source": [
    "# Display the shapes of the inputs and outputs\n",
    "for key, value in input_size_dict.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "for key, value in output_size_dict.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ancienne version\n",
    "\n",
    "# # Fraction de données pour la validation\n",
    "# VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# # Division des données en entraînement et validation pour chaque entrée\n",
    "# X_train_final_train = {}\n",
    "# X_train_final_val = {}\n",
    "\n",
    "# for key, value in X_train_final.items():\n",
    "#     X_train_final_train[key], X_train_final_val[key] = train_test_split(\n",
    "#         value, test_size=VALIDATION_SPLIT, random_state=42\n",
    "#     )\n",
    "\n",
    "# # Convert DataFrames to NumPy arrays if necessary\n",
    "# for key in X_train_final_train:\n",
    "#     if isinstance(X_train_final_train[key], pd.DataFrame):\n",
    "#         X_train_final_train[key] = X_train_final_train[key].values\n",
    "#     if isinstance(X_train_final_val[key], pd.DataFrame):\n",
    "#         X_train_final_val[key] = X_train_final_val[key].values\n",
    "\n",
    "# for key in X_test_final:\n",
    "#     if isinstance(X_test_final[key], pd.DataFrame):\n",
    "#         X_test_final[key] = X_test_final[key].values\n",
    "\n",
    "\n",
    "# # Prepare the predicted outputs from reservoirs as tensors\n",
    "# def prepare_tensors(data_dict, device):\n",
    "#     for key in data_dict:\n",
    "#         if isinstance(data_dict[key], pd.DataFrame):\n",
    "#             data_dict[key] = torch.tensor(data_dict[key].values, dtype=torch.float32).to(device)\n",
    "#         else:\n",
    "#             data_dict[key] = torch.tensor(data_dict[key], dtype=torch.float32).to(device)\n",
    "#     return data_dict\n",
    "\n",
    "# X_train_final_train = prepare_tensors(X_train_final_train, DEVICE)\n",
    "# X_train_final_val = prepare_tensors(X_train_final_val, DEVICE)\n",
    "# X_test_final = prepare_tensors(X_test_final, DEVICE)\n",
    "\n",
    "# # Création des datasets\n",
    "# def create_dataset(X_data, y_data, device):\n",
    "#     return torch.utils.data.TensorDataset(\n",
    "#         X_data[\"X_genres\"],\n",
    "#         X_data[\"X_instruments\"],\n",
    "#         X_data[\"X_moods\"],\n",
    "#         X_data[\"y_genres_pred\"],\n",
    "#         X_data[\"y_instruments_pred\"],\n",
    "#         X_data[\"y_moods_pred\"],\n",
    "#         X_data[\"y_genres_instruments_pred\"],\n",
    "#         X_data[\"y_genres_moods_pred\"],\n",
    "#         X_data[\"y_instruments_moods_pred\"],\n",
    "#         y_data,\n",
    "#     )\n",
    "\n",
    "# train_dataset = create_dataset(X_train_final_train, y_train_tensor[: len(X_train_final_train[\"X_genres\"])], DEVICE)\n",
    "# val_dataset = create_dataset(X_train_final_val, y_train_tensor[len(X_train_final_train[\"X_genres\"]) :], DEVICE)\n",
    "# test_dataset = create_dataset(X_test_final, y_test_tensor, DEVICE)\n",
    "\n",
    "# # Création des loaders\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# Split each array or dataframe individually\n",
    "X_genres_train_split, X_genres_val_split = train_test_split(\n",
    "    X_genres_train, test_size=VALIDATION_SPLIT, random_state=42\n",
    ")\n",
    "X_instruments_train_split, X_instruments_val_split = train_test_split(\n",
    "    X_instruments_train, test_size=VALIDATION_SPLIT, random_state=42\n",
    ")\n",
    "X_moods_train_split, X_moods_val_split = train_test_split(\n",
    "    X_moods_train, test_size=VALIDATION_SPLIT, random_state=42\n",
    ")\n",
    "y_genres_train_pred_split, y_genres_val_pred_split = train_test_split(\n",
    "    y_genres_train_pred, test_size=VALIDATION_SPLIT, random_state=42\n",
    ")\n",
    "y_instruments_train_pred_split, y_instruments_val_pred_split = train_test_split(\n",
    "    y_instruments_train_pred, test_size=VALIDATION_SPLIT, random_state=42\n",
    ")\n",
    "y_moods_train_pred_split, y_moods_val_pred_split = train_test_split(\n",
    "    y_moods_train_pred, test_size=VALIDATION_SPLIT, random_state=42\n",
    ")\n",
    "y_genres_instruments_train_pred_split, y_genres_instruments_val_pred_split = (\n",
    "    train_test_split(\n",
    "        y_genres_instruments_train_pred, test_size=VALIDATION_SPLIT, random_state=42\n",
    "    )\n",
    ")\n",
    "y_genres_moods_train_pred_split, y_genres_moods_val_pred_split = train_test_split(\n",
    "    y_genres_moods_train_pred, test_size=VALIDATION_SPLIT, random_state=42\n",
    ")\n",
    "y_instruments_moods_train_pred_split, y_instruments_moods_val_pred_split = (\n",
    "    train_test_split(\n",
    "        y_instruments_moods_train_pred, test_size=VALIDATION_SPLIT, random_state=42\n",
    "    )\n",
    ")\n",
    "y_train_split, y_val_split = train_test_split(\n",
    "    y_train_tensor.cpu().numpy(), test_size=VALIDATION_SPLIT, random_state=42\n",
    ")\n",
    "X_genres_categories_train_split, X_genres_categories_val_split = train_test_split(\n",
    "    X_genres_categories_train, test_size=VALIDATION_SPLIT, random_state=42\n",
    ")\n",
    "X_instruments_categories_train_split, X_instruments_categories_val_split = (\n",
    "    train_test_split(\n",
    "        X_instruments_categories_train, test_size=VALIDATION_SPLIT, random_state=42\n",
    "    )\n",
    ")\n",
    "X_moods_categories_train_split, X_moods_categories_val_split = train_test_split(\n",
    "    X_moods_categories_train, test_size=VALIDATION_SPLIT, random_state=42\n",
    ")\n",
    "\n",
    "# Combine back into dictionaries\n",
    "X_train_final_train = {\n",
    "    \"X_genres\": torch.tensor(X_genres_train_split.values, dtype=torch.float32).to(\n",
    "        DEVICE\n",
    "    ),\n",
    "    \"X_instruments\": torch.tensor(\n",
    "        X_instruments_train_split.values, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"X_moods\": torch.tensor(X_moods_train_split.values, dtype=torch.float32).to(DEVICE),\n",
    "    \"X_genres_categories\": torch.tensor(\n",
    "        X_genres_categories_train_split.values, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"X_instruments_categories\": torch.tensor(\n",
    "        X_instruments_categories_train_split.values, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"X_moods_categories\": torch.tensor(\n",
    "        X_moods_categories_train_split.values, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"y_genres_pred\": torch.tensor(y_genres_train_pred_split, dtype=torch.float32).to(\n",
    "        DEVICE\n",
    "    ),\n",
    "    \"y_instruments_pred\": torch.tensor(\n",
    "        y_instruments_train_pred_split, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"y_moods_pred\": torch.tensor(y_moods_train_pred_split, dtype=torch.float32).to(\n",
    "        DEVICE\n",
    "    ),\n",
    "    \"y_genres_instruments_pred\": torch.tensor(\n",
    "        y_genres_instruments_train_pred_split, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"y_genres_moods_pred\": torch.tensor(\n",
    "        y_genres_moods_train_pred_split, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"y_instruments_moods_pred\": torch.tensor(\n",
    "        y_instruments_moods_train_pred_split, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "}\n",
    "train_targets = torch.tensor(y_train_split, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "X_train_final_val = {\n",
    "    \"X_genres\": torch.tensor(X_genres_val_split.values, dtype=torch.float32).to(DEVICE),\n",
    "    \"X_instruments\": torch.tensor(\n",
    "        X_instruments_val_split.values, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"X_moods\": torch.tensor(X_moods_val_split.values, dtype=torch.float32).to(DEVICE),\n",
    "    \"X_genres_categories\": torch.tensor(\n",
    "        X_genres_categories_val_split.values, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"X_instruments_categories\": torch.tensor(\n",
    "        X_instruments_categories_val_split.values, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"X_moods_categories\": torch.tensor(\n",
    "        X_moods_categories_val_split.values, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"y_genres_pred\": torch.tensor(y_genres_val_pred_split, dtype=torch.float32).to(\n",
    "        DEVICE\n",
    "    ),\n",
    "    \"y_instruments_pred\": torch.tensor(\n",
    "        y_instruments_val_pred_split, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"y_moods_pred\": torch.tensor(y_moods_val_pred_split, dtype=torch.float32).to(\n",
    "        DEVICE\n",
    "    ),\n",
    "    \"y_genres_instruments_pred\": torch.tensor(\n",
    "        y_genres_instruments_val_pred_split, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"y_genres_moods_pred\": torch.tensor(\n",
    "        y_genres_moods_val_pred_split, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"y_instruments_moods_pred\": torch.tensor(\n",
    "        y_instruments_moods_val_pred_split, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "}\n",
    "val_targets = torch.tensor(y_val_split, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "# Convert test data to tensors\n",
    "X_test_final = {\n",
    "    \"X_genres\": torch.tensor(X_genres_test.values, dtype=torch.float32).to(DEVICE),\n",
    "    \"X_instruments\": torch.tensor(X_instruments_test.values, dtype=torch.float32).to(\n",
    "        DEVICE\n",
    "    ),\n",
    "    \"X_moods\": torch.tensor(X_moods_test.values, dtype=torch.float32).to(DEVICE),\n",
    "    \"X_genres_categories\": torch.tensor(\n",
    "        X_genres_categories_test.values, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"X_instruments_categories\": torch.tensor(\n",
    "        X_instruments_categories_test.values, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"X_moods_categories\": torch.tensor(\n",
    "        X_moods_categories_test.values, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"y_genres_pred\": torch.tensor(y_genres_test_pred, dtype=torch.float32).to(DEVICE),\n",
    "    \"y_instruments_pred\": torch.tensor(y_instruments_test_pred, dtype=torch.float32).to(\n",
    "        DEVICE\n",
    "    ),\n",
    "    \"y_moods_pred\": torch.tensor(y_moods_test_pred, dtype=torch.float32).to(DEVICE),\n",
    "    \"y_genres_instruments_pred\": torch.tensor(\n",
    "        y_genres_instruments_test_pred, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"y_genres_moods_pred\": torch.tensor(\n",
    "        y_genres_moods_test_pred, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "    \"y_instruments_moods_pred\": torch.tensor(\n",
    "        y_instruments_moods_test_pred, dtype=torch.float32\n",
    "    ).to(DEVICE),\n",
    "}\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "\n",
    "# Dataset creation function\n",
    "def create_dataset(X_data, y_data):\n",
    "    return torch.utils.data.TensorDataset(\n",
    "        X_data[\"X_genres\"],\n",
    "        X_data[\"X_instruments\"],\n",
    "        X_data[\"X_moods\"],\n",
    "        X_data[\"X_genres_categories\"],\n",
    "        X_data[\"X_instruments_categories\"],\n",
    "        X_data[\"X_moods_categories\"],\n",
    "        X_data[\"y_genres_pred\"],\n",
    "        X_data[\"y_instruments_pred\"],\n",
    "        X_data[\"y_moods_pred\"],\n",
    "        X_data[\"y_genres_instruments_pred\"],\n",
    "        X_data[\"y_genres_moods_pred\"],\n",
    "        X_data[\"y_instruments_moods_pred\"],\n",
    "        y_data,\n",
    "    )\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = create_dataset(X_train_final_train, train_targets)\n",
    "val_dataset = create_dataset(X_train_final_val, val_targets)\n",
    "test_dataset = create_dataset(X_test_final, y_test_tensor)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "# Check loaders\n",
    "# for batch in train_loader:\n",
    "#     print(\"Train batch sample:\", batch)\n",
    "#     break\n",
    "\n",
    "# for batch in val_loader:\n",
    "#     print(\"Validation batch sample:\", batch)\n",
    "#     break\n",
    "\n",
    "# for batch in test_loader:\n",
    "#     print(\"Test batch sample:\", batch)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([70946, 90]), torch.Size([70946, 112]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.tensors[0].shape, train_dataset.tensors[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Random Forest: 100%|██████████| 10/10 [04:34<00:00, 27.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.0054, Validation F1 Score: 0.3258\n",
      "Test Accuracy: 0.0051, Test F1 Score: 0.3230\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Concaténer toutes les caractéristiques d'entrée et les prédictions des réservoirs de neurones\n",
    "def prepare_data_for_rf(X_train_final, y_train):\n",
    "    X_train_concat = np.concatenate(\n",
    "        [\n",
    "            X_train_final[\"X_genres\"].cpu().numpy(),\n",
    "            X_train_final[\"X_instruments\"].cpu().numpy(),\n",
    "            X_train_final[\"X_moods\"].cpu().numpy(),\n",
    "            X_train_final[\"X_genres_categories\"].cpu().numpy(),\n",
    "            X_train_final[\"X_instruments_categories\"].cpu().numpy(),\n",
    "            X_train_final[\"X_moods_categories\"].cpu().numpy(),\n",
    "            X_train_final[\"y_genres_pred\"].cpu().numpy(),\n",
    "            X_train_final[\"y_instruments_pred\"].cpu().numpy(),\n",
    "            X_train_final[\"y_moods_pred\"].cpu().numpy(),\n",
    "            X_train_final[\"y_genres_instruments_pred\"].cpu().numpy(),\n",
    "            X_train_final[\"y_genres_moods_pred\"].cpu().numpy(),\n",
    "            X_train_final[\"y_instruments_moods_pred\"].cpu().numpy(),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    return X_train_concat, y_train\n",
    "\n",
    "\n",
    "# Préparer les données pour le Random Forest\n",
    "X_train_rf, y_train_rf = prepare_data_for_rf(\n",
    "    X_train_final_train, train_targets.cpu().numpy()\n",
    ")\n",
    "X_val_rf, y_val_rf = prepare_data_for_rf(X_train_final_val, val_targets.cpu().numpy())\n",
    "X_test_rf, y_test_rf = prepare_data_for_rf(X_test_final, y_test_tensor.cpu().numpy())\n",
    "\n",
    "# Initialiser le Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=10, random_state=42, warm_start=True)\n",
    "\n",
    "# Entraîner le modèle avec une barre de progression\n",
    "for i in tqdm(range(rf_model.n_estimators), desc=\"Training Random Forest\"):\n",
    "    rf_model.n_estimators = i + 1\n",
    "    rf_model.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "# Évaluer le modèle sur l'ensemble de validation\n",
    "y_val_pred_rf = rf_model.predict(X_val_rf)\n",
    "val_accuracy = accuracy_score(y_val_rf, y_val_pred_rf)\n",
    "val_f1 = f1_score(y_val_rf, y_val_pred_rf, average=\"weighted\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# Évaluer le modèle sur l'ensemble de test\n",
    "y_test_pred_rf = rf_model.predict(X_test_rf)\n",
    "test_accuracy = accuracy_score(y_test_rf, y_test_pred_rf)\n",
    "test_f1 = f1_score(y_test_rf, y_test_pred_rf, average=\"weighted\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Réduire la dimensionnalité des données avec PCA\n",
    "pca = PCA(n_components=100)  # Ajustez le nombre de composants selon vos besoins\n",
    "X_train_pca = pca.fit_transform(X_train_rf)\n",
    "X_val_pca = pca.transform(X_val_rf)\n",
    "X_test_pca = pca.transform(X_test_rf)\n",
    "\n",
    "# # Initialiser le Random Forest Classifier\n",
    "# rf_model2 = RandomForestClassifier(n_estimators=100, random_state=42, warm_start=True)\n",
    "\n",
    "# # Entraîner le modèle avec une barre de progression\n",
    "# for i in tqdm(range(rf_model.n_estimators), desc=\"Training Random Forest\"):\n",
    "#     rf_model.n_estimators = i + 1\n",
    "#     rf_model.fit(X_train_pca, y_train_rf)\n",
    "\n",
    "# # Évaluer le modèle sur l'ensemble de validation\n",
    "# y_val_pred_rf = rf_model.predict(X_val_pca)\n",
    "# val_accuracy = accuracy_score(y_val_rf, y_val_pred_rf)\n",
    "# val_f1 = f1_score(y_val_rf, y_val_pred_rf, average='weighted')\n",
    "# print(f\"Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# # Évaluer le modèle sur l'ensemble de test\n",
    "# y_test_pred_rf = rf_model.predict(X_test_pca)\n",
    "# test_accuracy = accuracy_score(y_test_rf, y_test_pred_rf)\n",
    "# test_f1 = f1_score(y_test_rf, y_test_pred_rf, average='weighted')\n",
    "# print(f\"Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAIjCAYAAAA0vUuxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB4M0lEQVR4nO3dd3xT1f/H8Xe6W6AtqwMotAxZZSPIEkU2grgYggwVfyK4wIWKFVSG+uWrIogTcIJbFKwgQxTBIlNEULGAXyzLAmVYWpLz+wMbCR1JIG1C83o+Hjxo7j2595Pb0zTv3nvPsRhjjAAAAAAAhQrwdgEAAAAA4OsITgAAAADgBMEJAAAAAJwgOAEAAACAEwQnAAAAAHCC4AQAAAAAThCcAAAAAMAJghMAAAAAOEFwAgAAAAAnCE6Anxk2bJgSExM9us05c+bIYrFo586dHt2uLzuf45iYmKhhw4Z5tB5XFcf3/3z5Yk04f4899pgsFosOHjzo7VJcsnbtWrVt21ZlypSRxWLRxo0bvV0SAB9DcALOwY4dO/R///d/qlmzpsLCwhQZGal27drpueee099//+3t8orNpEmT9Mknn3i7DLu8wFbYvzVr1ni7xAvO/v37FRQUpMGDBxfa5ujRowoPD9c111xTgpWhIMOGDZPFYlHjxo1ljMm33mKxaPTo0V6o7MKSm5ur66+/XpmZmfrvf/+rN998UzVq1CjyOfv27dO9996revXqKSIiQmXKlFGLFi30xBNP6PDhwyVTeCm2aNEiPfbYY94uA3AQ5O0CgAvNwoULdf311ys0NFRDhgxRcnKycnJy9O233+q+++7TTz/9pJdfftnbZRaLSZMm6brrrlPfvn0dlt94440aMGCAQkNDvVLXxIkTlZSUlG957dq1vVCNc9u3b1dAgG/+3SomJkZdunTRp59+qhMnTigiIiJfm48++kjZ2dlFhit3vPLKK7LZbB7Zlr/68ccf9dFHH+naa6/1dikXpB07dmjXrl165ZVXdMsttzhtv3btWvXs2VPHjh3T4MGD1aJFC0nSDz/8oClTpmjlypVavHhxcZddqi1atEgzZswgPMGnEJwAN6Snp2vAgAGqUaOGli1bpvj4ePu6UaNG6bffftPChQu9WKF3BAYGKjAw0Gv779Gjh1q2bOm1/bvLWwHTVYMGDVJqaqoWLFigAQMG5Fv/zjvvKCoqSr169Tqv/Rw/flxlypRRcHDweW3H34WHhyshIUETJ07UNddcI4vF4u2SSlRhAd8d+/fvlyRFR0c7bXv48GFdffXVCgwM1IYNG1SvXj2H9U8++aReeeWV86oHgG/yzT95Aj7qqaee0rFjx/Taa685hKY8tWvX1l133SVJ2rlzpywWi+bMmZOvncVicfgrWt69AL/88osGDx6sqKgoVa5cWePHj5cxRn/88YeuuuoqRUZGKi4uTv/5z38ctlfYPUYrVqyQxWLRihUrinxdzzzzjNq2bauKFSsqPDxcLVq00AcffJCv5uPHj2vu3Ln2S+Hy7tM5e/9XXnmlatasWeC+2rRpky/kvPXWW2rRooXCw8NVoUIFDRgwQH/88UeRNbsjJSVFAQEBWrp0qcPyW2+9VSEhIdq0aZOkf4/X/Pnz9dBDDykuLk5lypRRnz59XKrHleMo5b/HKe/4rVq1SmPGjFHlypVVpkwZXX311Tpw4EC+53/xxRfq0KGDypQpo3LlyqlXr1766aef8rX75JNPlJycrLCwMCUnJ+vjjz92+hok6eqrr1aZMmX0zjvv5Fu3f/9+LV26VNddd51CQ0P1zTff6Prrr1f16tUVGhqqhIQE3XPPPfkuWR02bJjKli2rHTt2qGfPnipXrpwGDRpkX3f2PU6uHsu8S9HyXmtoaKgaNmyo1NTUfG337Nmjm2++WVWqVFFoaKiSkpI0cuRI5eTk2NscPnxYd999txISEhQaGqratWtr6tSpTs+IudPnlyxZovbt2ys6Olply5ZV3bp19dBDDxW5/aIEBATokUce0ebNm51+j915r7jsssuUnJyszZs3q2PHjoqIiFDt2rXt34evv/5arVu3Vnh4uOrWrauvvvqqwH0ePHhQ/fr1U2RkpCpWrKi77rpL2dnZ+dq58j6QV9O6det06aWXKiIiwumxW7Zsmf3nJTo6WldddZV+/vln+/phw4apY8eOkqTrr79eFotFl112WaHbe+mll7Rnzx5NmzYtX2iSpNjYWD3yyCMOy2bOnKmGDRsqNDRUVapU0ahRo/Jdzne+xzvv98i2bducHu9Tp07p8ccfV61atRQaGqrExEQ99NBDOnnypEO7xMREXXnllfr222/VqlUrhYWFqWbNmnrjjTfyvW5Xfnbyfi8+88wzevnll+37v/jii7V27Vp7u2HDhmnGjBmS5HD5dZ558+apRYsWKleunCIjI9WoUSM999xz+WoCPM4AcFnVqlVNzZo1XWqbnp5uJJnZs2fnWyfJpKSk2B+npKQYSaZp06Zm4MCBZubMmaZXr15Gkpk2bZqpW7euGTlypJk5c6Zp166dkWS+/vpr+/Nnz55tJJn09HSH/SxfvtxIMsuXL7cvGzp0qKlRo4ZDu2rVqpnbb7/dvPDCC2batGmmVatWRpL5/PPP7W3efPNNExoaajp06GDefPNN8+abb5rvvvuuwP2/8cYbRpJJS0tz2M/OnTuNJPP000/blz3xxBPGYrGY/v37m5kzZ5oJEyaYSpUqmcTERHPo0KEij3Hefr/66itz4MABh38HDx60t8vJyTHNmjUzNWrUMFlZWcYYY1JTU40k8/jjj+c7Xo0aNTKNGzc206ZNMw8++KAJCwszF110kTlx4sR5H0djjKlRo4YZOnRovtfRrFkz06lTJzN9+nQzduxYExgYaPr16+fw3DfeeMNYLBbTvXt3M336dDN16lSTmJhooqOjHb7/X375pQkICDDJyclm2rRp5uGHHzZRUVGmYcOG+eouyA033GBCQkLMX3/95bD8+eefN5LMsmXLjDHG3HHHHaZnz55m0qRJ5qWXXjI333yzCQwMNNddd53D84YOHWpCQ0NNrVq1zNChQ82sWbPMG2+8cd7HUpJp0qSJiY+PN48//rh59tlnTc2aNU1ERIRDH9izZ4+pUqWKiYiIMHfffbeZNWuWGT9+vKlfv769nx0/ftw0btzYVKxY0Tz00ENm1qxZZsiQIcZisZi77rqryOPlap/fsmWLCQkJMS1btjTPPfecmTVrlrn33nvNpZdeWuT2CzN06FBTpkwZc+rUKVOnTh3TpEkTY7PZHI7PqFGj7I/dea/o2LGjqVKliklISDD33XefmT59umnQoIEJDAw08+bNM3Fxceaxxx4zzz77rKlataqJioqy/3wZ8+/7WqNGjUzv3r3NCy+8YAYPHmwkmRtvvNFh/66+D3Ts2NHExcWZypUrmzvuuMO89NJL5pNPPin0+CxZssQEBQWZiy66yDz11FP27ZYvX95+DL777jvz0EMPGUnmzjvvNG+++aZZvHhxodts27atCQ8PNydPniy0zZnyjkPnzp3N9OnTzejRo01gYKC5+OKLTU5OjleO99ChQ40kc91115kZM2aYIUOGGEmmb9++Du1q1Khh6tata2JjY81DDz1kXnjhBdO8eXNjsVjMli1b7O1c/dnJ+73YrFkzU7t2bTN16lTz1FNPmUqVKplq1arZj8d3331nunTpYiTZf9+8+eabxhhjFi9ebCSZK664wsyYMcPMmDHDjB492lx//fUufT+A80FwAlx05MgRI8lcddVVLrU/l+B066232pedOnXKVKtWzVgsFjNlyhT78kOHDpnw8PACP3ifa3A6MxAYczpoJCcnm06dOjksL1OmjMN+C9v/kSNHTGhoqBk7dqxDu6eeespYLBaza9cuY8zpD5WBgYHmySefdGj3448/mqCgoHzLC9tvQf9CQ0PzbTMkJMTccsst5tChQ6Zq1aqmZcuWJjc3194m73hVrVrV4QPJe++9ZySZ5557zr7sfI5jYcGpc+fODh9677nnHhMYGGgOHz5sjDHm6NGjJjo62owYMcJhe3v37jVRUVEOy5s2bWri4+PtzzXm3w8crgSnhQsXGknmpZdeclh+ySWXmKpVqxqr1VrgazbGmMmTJzt8n43594Pagw8+mK/9+RxLSSYkJMT89ttv9mWbNm0yksz06dPty4YMGWICAgLM2rVr8+0/75g//vjjpkyZMuaXX35xWP/ggw+awMBAs3v37nzPzeNqn//vf/9rJJkDBw4Uui135AUnY4yZO3eukWQ++ugj+/rzDU6SzDvvvGNftm3bNiPJBAQEmDVr1tiXf/nll/ne7/Le1/r06eOwr9tvv91IMps2bTLGuPc+kFfTrFmzXDo+TZs2NTExMQ5/ANi0aZMJCAgwQ4YMyff633//fafbLF++vGnSpIlL+9+/f78JCQkxXbt2tf/MGGPMCy+8YCSZ119/3b6spI73xo0bjSRzyy23OLS79957Hf4oYszp9ypJZuXKlQ6v6ey+7urPTt7vxYoVK5rMzEx7u08//dRIMp999pl92ahRo0xBf9+/6667TGRkpDl16lS+dUBx41I9wEVZWVmSpHLlyhXbPs68KTkwMFAtW7aUMUY333yzfXl0dLTq1q2r33//3WP7DQ8Pt3996NAhHTlyRB06dND69evPaXuRkZHq0aOH3nvvPYeRvubPn69LLrlE1atXl3R6kAGbzaZ+/frp4MGD9n9xcXGqU6eOli9f7tL+ZsyYoSVLljj8++KLLxzaJCcna8KECXr11VfVrVs3HTx4UHPnzlVQUP5bPYcMGeLwfb7uuusUHx+vRYsWFVnH+R7HW2+91eFylA4dOshqtWrXrl2STl/idfjwYQ0cONDheAUGBqp169b245WRkaGNGzdq6NChioqKsm+vS5cuatCggUu1dO3aVZUrV3a4XC89PV1r1qzRwIED7YNbnPmajx8/roMHD6pt27YyxmjDhg35tjty5EiX9u/OsezcubNq1aplf9y4cWNFRkbaf0ZsNps++eQT9e7du8B74fKO+fvvv68OHTqofPnyDse3c+fOslqtWrlyZaH1utrn8+6h+fTTTz0+IMagQYNUp04dTZw4scAR9s5F2bJlHe5zq1u3rqKjo1W/fn21bt3avjzv64Lel0aNGuXw+I477pAk+8+Tu+8DoaGhGj58uNPa834Ohg0bpgoVKtiXN27cWF26dHH681yYrKwsl38PfPXVV8rJydHdd9/tMCDMiBEjFBkZme+e2JI43nn/jxkzxqHd2LFjJSlfTQ0aNFCHDh3sjytXrpzvd5C7Pzv9+/dX+fLl7Y/ztu/K77Xo6GgdP35cS5YscdoW8DQGhwBcFBkZKen0UMzFJe/DVZ6oqCiFhYWpUqVK+Zb/9ddfHtvv559/rieeeEIbN250uMb9fG4y79+/vz755BOtXr1abdu21Y4dO7Ru3To9++yz9ja//vqrjDGqU6dOgdtwddCAVq1auTQ4xH333ad58+YpLS1NkyZNKjREnF2PxWJR7dq1nc5Tdb7H8ezvf94Hi0OHDkk6fbwkqVOnTgU+P6+P5gWtgo5r3bp1XQpyQUFB6t+/v2bOnKk9e/aoatWq9hCVd2+SJO3evVuPPvqoFixYYK8zz5EjR/Jts1q1ak73Lbl3LM8+btLpY5dXz4EDB5SVlaXk5OQi9/nrr79q8+bNqly5coHr8wYQKIwrfb5///569dVXdcstt+jBBx/UFVdcoWuuuUbXXXfdeY+0GBgYqEceeURDhw7VJ598oquvvvq8tidJ1apVy3fMo6KilJCQkG+ZpHx9QMrfD2vVqqWAgAD7z5O77wNVq1ZVSEiI09rzfg7q1q2bb139+vX15Zdf2gcocUdkZKTLvwcKqyEkJEQ1a9a0r89TEsd7165dCggIyDfqaFxcnKKjo/PV5OznS3L/Z8fZe11Rbr/9dr333nvq0aOHqlatqq5du6pfv37q3r270+cC54vgBLgoMjJSVapU0ZYtW1xqX9iHZavVWuhzChqZrrDR6s78i/K57CvPN998oz59+ujSSy/VzJkzFR8fr+DgYM2ePbvAwQFc1bt3b0VEROi9995T27Zt9d577ykgIEDXX3+9vY3NZpPFYtEXX3xR4OssW7bsOe+/IL///rs9fPz4448e3bYnjqOz73XeGYo333xTcXFx+doVdPbsfAwePFgvvPCC3n33Xd17771699131aBBAzVt2lTS6f7VpUsXZWZm6oEHHlC9evVUpkwZ7dmzR8OGDct3RiU0NNSlcODusXTlZ8QVNptNXbp00f3331/g+osuuqjI57vS58PDw7Vy5UotX75cCxcuVGpqqubPn69OnTpp8eLF5z065aBBg/T4449r4sSJ+aYNkNx/ryisnvM55mfX4O77wJlnI72hXr162rhxo3JyclwKcO4oiePtbPm57Nvdn53zeT0xMTHauHGjvvzyS33xxRf64osvNHv2bA0ZMkRz5851+nzgfBCcADdceeWVevnll7V69Wq1adOmyLZ5f0E7e+Sks/+a5wnns68PP/xQYWFh+vLLLx2GyZ49e3a+tu6cgSpTpoyuvPJKvf/++5o2bZrmz5+vDh06qEqVKvY2tWrVkjFGSUlJTj+Uni+bzaZhw4YpMjJSd999t31OqoImcc0LV3mMMfrtt9/UuHHjQrfvznE8V3mXo8XExKhz586FtsubuPPs1yGdnkPKVa1bt1atWrX0zjvvqEuXLvrpp5/05JNP2tf/+OOP+uWXXzR37lwNGTLEvvx8L6Hx9LGsXLmyIiMjnf7Ro1atWjp27FiRx7YorvR56fQoeFdccYWuuOIKTZs2TZMmTdLDDz+s5cuXn/O+8+SddRo2bJg+/fTTfOtL8n0pz6+//uowz9pvv/0mm81mH0mxuN4H8n4OCurz27ZtU6VKldw+2ySdDsirV6/Whx9+qIEDB7pcw5mjLubk5Cg9Pf28v98FcXa8a9SoIZvNpl9//VX169e3t9u3b58OHz7sdOLfgpzvz05Bivp9ExISot69e6t3796y2Wy6/fbb9dJLL2n8+PE+O38fSgfucQLccP/996tMmTK65ZZbtG/fvnzrd+zYYR8SNTIyUpUqVcp3bffMmTM9XlfeB+oz92W1Wl2aiDcwMFAWi8XhL847d+7UJ598kq9tmTJl8n3gKkr//v31559/6tVXX9WmTZvUv39/h/XXXHONAgMDNWHChHx/aTTGePRyxGnTpum7777Tyy+/rMcff1xt27bVyJEjdfDgwXxt33jjDYdLcT744ANlZGSoR48ehW7fneN4rrp166bIyEhNmjRJubm5+dbnDV0eHx+vpk2bau7cuQ6Xyy1ZskRbt251a5+DBg3Shg0blJKSIovFohtuuMG+Lu+vxmd+74wx5z0ssKePZUBAgPr27avPPvtMP/zwQ771efX369dPq1ev1pdffpmvzeHDh3Xq1Cmn+3LW5zMzM/M9J+8M3pmXJG7btk27d+92ur+CDB48WLVr19aECRPyrTuf94pzlTesdJ7p06dLkv3nqbjeB878OTjzfWvLli1avHixevbseU7bve222xQfH6+xY8fql19+ybd+//79euKJJySdvv8uJCREzz//vMNre+2113TkyJHzngutIM6Od97rPvMSUun0e6Skc6rJEz87Z8sLtWf/zjm7PwQEBNj/qHX2cOqAp3HGCXBD3l/f+/fvr/r162vIkCFKTk5WTk6OvvvuO73//vsO8/PccsstmjJlim655Ra1bNlSK1euLPAX7flq2LChLrnkEo0bN06ZmZmqUKGC5s2b59Ivq169emnatGnq3r27brjhBu3fv18zZsxQ7dq1tXnzZoe2LVq00FdffaVp06apSpUqSkpKcrhh+Wx58/Xce++9CgwM1LXXXuuwvlatWnriiSc0btw47dy5U3379lW5cuWUnp6ujz/+WLfeeqvuvfdep6/hiy++0LZt2/Itb9u2rWrWrKmff/5Z48eP17Bhw9S7d29Jp+ezadq0qf16+TNVqFBB7du31/Dhw7Vv3z49++yzql27tkaMGOGR43iuIiMj9eKLL+rGG29U8+bNNWDAAFWuXFm7d+/WwoUL1a5dO73wwguSpMmTJ6tXr15q3769brrpJmVmZmr69Olq2LChjh075vI+Bw8erIkTJ+rTTz9Vu3btHOZbqlevnmrVqqV7771Xe/bsUWRkpD788EOX7lMoSnEcy0mTJmnx4sXq2LGjbr31VtWvX18ZGRl6//339e233yo6Olr33XefFixYoCuvvFLDhg1TixYtdPz4cf3444/64IMPtHPnznz3G57NWZ+fOHGiVq5cqV69eqlGjRrav3+/Zs6cqWrVqql9+/b2dvXr11fHjh2dzsFWkMDAQD388MMFDqBwPu8V5yo9PV19+vRR9+7dtXr1ar311lu64YYb1KRJE0meex8oyNNPP60ePXqoTZs2uvnmm/X3339r+vTpioqKcphLzx3ly5fXxx9/rJ49e6pp06YaPHiwWrRoIUlav3693n33XfsVCZUrV9a4ceM0YcIEde/eXX369NH27ds1c+ZMXXzxxRo8ePA51VAUZ8e7SZMmGjp0qF5++WUdPnxYHTt2VFpamubOnau+ffvq8ssvd3ufnvjZOVveMb3zzjvVrVs3BQYGasCAAbrllluUmZmpTp06qVq1atq1a5emT5+upk2bOpxBA4pFiY3fB5Qiv/zyixkxYoRJTEw0ISEhply5cqZdu3Zm+vTpJjs7297uxIkT5uabbzZRUVGmXLlypl+/fmb//v2FDkd+9hDFZw41fKaOHTuahg0bOizbsWOH6dy5swkNDbXPubFkyRKXhiN/7bXXTJ06dUxoaKipV6+emT17tr2mM23bts1ceumlJjw83EiyD6ld2BDHxhgzaNAg+1Dbhfnwww9N+/btTZkyZUyZMmVMvXr1zKhRo8z27dsLfc6Z+y3s3+zZs82pU6fMxRdfbKpVq+YwNLcxxjz33HNGkpk/f74x5t8hid99910zbtw4ExMTY8LDw02vXr0chtY+3+NY2HDkZw+VXdAQ0XnLu3XrZqKiokxYWJipVauWGTZsmPnhhx/yHdf69eub0NBQ06BBA/PRRx8VWLczF198sZFkZs6cmW/d1q1bTefOnU3ZsmVNpUqVzIgRI+zDgZ85VHJhfTlv3bkeS5013Haes4+xMcbs2rXLDBkyxFSuXNmEhoaamjVrmlGjRjnMx3P06FEzbtw4U7t2bRMSEmIqVapk2rZta5555hmHOXeKUlSfX7p0qbnqqqtMlSpVTEhIiKlSpYoZOHBgvmGcJZmOHTs63VdhxzU3N9fUqlWrwOPj6ntFQe8zxpw+tr169cq3/Ox95X2/tm7daq677jpTrlw5U758eTN69Gjz999/53u+K+8DhdVUlK+++sq0a9fOhIeHm8jISNO7d2+zdetWhzbuDEee588//zT33HOPueiii0xYWJiJiIgwLVq0ME8++aQ5cuSIQ9sXXnjB1KtXzwQHB5vY2FgzcuTIfPPUleTxzs3NNRMmTDBJSUkmODjYJCQkmHHjxjn8/ipq3x07dszXP1352ckbjvzMufzOfD1n/l48deqUueOOO0zlypWNxWKx/+x/8MEHpmvXriYmJsaEhISY6tWrm//7v/8zGRkZ+bYJeJrFGA+NWQoAF7gVK1bo8ssv1/vvv6/rrrvO2+UAgMsee+wxTZgwQQcOHHD77A4A13CPEwAAAAA4QXACAAAAACcITgAAAADgBPc4AQAAAIATnHECAAAAACcITgAAAADghN9NgGuz2fTnn3+qXLlyslgs3i4HAAAAgJcYY3T06FFVqVJFAQFFn1Pyu+D0559/KiEhwdtlAAAAAPARf/zxh6pVq1ZkG78LTuXKlZN0+uBERkZ6tZbc3FwtXrxYXbt2VXBwsFdrge+hf8AZ+giKQv+AM/QROOMPfSQrK0sJCQn2jFAUvwtOeZfnRUZG+kRwioiIUGRkZKntjDh39A84Qx9BUegfcIY+Amf8qY+4cgsPg0MAAAAAgBMEJwAAAABwguAEAAAAAE4QnAAAAADACYITAAAAADhBcAIAAAAAJwhOAAAAAOAEwQkAAAAAnCA4AQAAAIATBCcAAAAAcILgBAAAAABOEJwAAAAAwAmCEwAAAAA4EeTtAgAA+VltRmt2/KVvftuvzX8c0d+5p5Rzyig0KEChQQGyWCw6cTJXBzIDNHv3d8q1WRQSaFGO1eT7/8zn5G2nsLbefm5p319J1nrylFXHjgTopfRVCgsO4tiwv3zPDQ4wRb6H+FKtvrK/C6lWT+zPGKM/951+HwkNCvRYrWHBgUqoEKFrm1dT29qVFBhg8favXZd4NTitXLlSTz/9tNatW6eMjAx9/PHH6tu3b5HPWbFihcaMGaOffvpJCQkJeuSRRzRs2LASqReAb7DajL779aA+WP+H/sg8oZOnbKXql+fBYzn6MytbVpsrRyNAe/YcK+5DjgtWgPT3cW8XAZ/GewicKZ73kXW7D+uTjX+qTEig/tOvibonx3t8H57m1eB0/PhxNWnSRDfddJOuueYap+3T09PVq1cv3XbbbXr77be1dOlS3XLLLYqPj1e3bt1KoGIAZyoowBR3GDmRc0o7//pbxtsvHgAAnLfjOVbd9tZ6zRrc3OfDk1eDU48ePdSjRw+X28+aNUtJSUn6z3/+I0mqX7++vv32W/33v/8lOAEuKOzyr3MJMgQYAADgKRM+26ouDeJ8+rK9C+oep9WrV6tz584Oy7p166a777670OecPHlSJ0+etD/OysqSJOXm5io3N7dY6nRV3v69XQd8U1H9Iy8AfbRxT74zPYX9/3euVX8cytYply7/AgAAKDkZR7K1+rf9ap1UoUT3687n8AsqOO3du1exsbEOy2JjY5WVlaW///5b4eHh+Z4zefJkTZgwId/yxYsXKyIiothqdceSJUu8XQJ8gM1I2w9btPaAdPBvi04ZKVABemrjUgVKskoKskgnbdK+bIuMfPcvMgAAAO5a/M33+uvnkr2W5cSJEy63vaCC07kYN26cxowZY3+clZWlhIQEde3aVZGRkV6s7HTCXbJkibp06aLg4GCv1oLiVdAZorzL3ySL9mZla1dmNpe9AQAAv9W1Q+sSP+OUdzWaKy6o4BQXF6d9+/Y5LNu3b58iIyMLPNskSaGhoQoNDc23PDg42GfCii/VgnOXc8qmud+l6/vf/1LGkWwGMwAAAHBRfFSY2tSOKfF7nNz5DH5BBac2bdpo0aJFDsuWLFmiNm3aeKki+Juzw1HeWaNdmX/rzyPZ3i4PAADggpTSu4FPDwwheTk4HTt2TL/99pv9cXp6ujZu3KgKFSqoevXqGjdunPbs2aM33nhDknTbbbfphRde0P3336+bbrpJy5Yt03vvvaeFCxd66yWglCpo9Lndf/2tv04wkAcAAICnlAkN1H+uZx4np3744Qddfvnl9sd59yINHTpUc+bMUUZGhnbv3m1fn5SUpIULF+qee+7Rc889p2rVqunVV19lKHKcl7yzSGnpmTqenau/TuTqtwPHXJx8FCg51aJDFVMuzD40/ImTuTqQeUSVK0Qq12bxmQl+vT3TvS/vryRrPXnKqmNHjqpsVDmFBQdxbNhfvucGBxgdyMwq9D3El2r1lf1dSLV6Yn/GGP25L1Nlo8opNCjQY7WGBQcqoUKErm1eTW1rV/L5M015vBqcLrvsMhlT+J0fc+bMKfA5GzZsKMaqUJqdPWHrrr9OcBapFKgQEaTEimVK1S/P0KAAhYcEqUm1aLWrU0mX1KyY7xdLbm6uFi1apJ4923KfJPL5t3+0o3+gQLyHwBneRxxdUPc4Ae46Myht/t8R7fzrBIM0FKMKEUGqUSGi2MNIru30TaStkipqaNtEhQQFePulAwCAUo7ghFIn79K7zzb9qR/3ZBGUnKgWHarKZUPPKcgQYAAAgL8gOOGCd+ZZpTW//6V9R3O8XVKJO/NMT1HhJyw4UJXLhalahXC1rVXw5V8AAADIj+CEC5I/nFWqEB6ksspxuCEz78xPQECAIkID1SqRMz0AAAAlgeCEC0bemaXHPt+iHQdOeLuc81LQYAa5VsfL3izGyg2ZAAAAPoLgBJ+WN5/S3NXp+urn/bJdQKeWEiuEqUxo8DnfC5Sbay2BKgEAAOAKghN8Us4pm8Z9tFmfbvxTp3w4LSVVDFdcZJgsFotOWm1KKH/hzUkAAAAA5whO8Bl5Z5eeXrxNG/844u1y8qkWHarYyPALcsI2AAAAnB+CE7zOajOavvRXvfj1Dp08ZfN2ObooJkJlw4IVHlz05KMAAADwHwQneE1eYHph+W9euxyvQkSQkiqV5SwSAAAAikRwQonzZmCqWCZYzauXZ8JWAAAAuIXghBLjjcAUYJGSq0Sqd5OqBCUAAACcM4ITil1JB6YqUWHq27Qq9yYBAADAYwhOKFaLNmdozHsblV2Mgz5YJDWqylklAAAAFB+CE4pFzimbhrz2vdakZxbbPmpXLqOU3g0Z0AEAAADFjuAEj7LajO56d4M+/zGjWLYfZJGualZFk69pwpklAAAAlBiCEzxm0eYM3TVvg3I9fB9TgKQuDWI0pG0S9ywBAADAKwhO8IjHP9+q175N9+g2gwKk0ZfX1h1XXERYAgAAgFcRnHBerDaj619cpfV/HPHYNglMAAAA8DUEJ5yzRZszdOe763XKQ1fmEZgAAADgqwhOOCdPLtyqV77xzKV5BCYAAAD4OoIT3Pb45z/ptW93nvd2CEwAAAC4UBCc4JYJn/2k2at2ntc2CEwAAAC40BCc4LKb56Rp6bYD5/z8QIt0RycCEwAAAC48BCe45KbZaVq2/dxDU8/kWE2/oQWBCQAAABckghOcumn291q2/eA5PTcoQHp+QDP1bFzFw1UBAAAAJYfghEKd7xxNzROi9P7IdpxlAgAAwAWP4IQCpW7J0N3zNij7HCdpurl9DY2/MtnDVQEAAADeQXBCPqlbMnTbW+vP+fkvDGimK5tyaR4AAABKD4ITHFhtRmPmbzzn58+8gfuZAAAAUPoQnODgznfX6USuze3nhQRa9PzAZuqeHF8MVQEAAADeRXCC3ecb/9TCH/e5/bxmCVH6gEEgAAAAUIoRnCBJWrT5T42et8Ht53WqW0mvD29dDBUBAAAAvoPgBKVuydDt77gfmq6oV1mvDWtVDBUBAAAAvoXg5OfOdTCI4e1qKKU3w40DAADAPxCc/Ny5DAZxc/tEjb+yYTFVBAAAAPieAG8XAO9ZtNn9wSB6JccRmgAAAOB3CE5+ymozGvPeJreeExpo0fM3NC+migAAAADfRXDyU9OX/qLsU+5dovff/s0YchwAAAB+ieDkh6w2oxe/3uHWc0Z0SFLPxkxuCwAAAP9EcPJDLyz7VSdPGZfb39w+UQ/3alCMFQEAAAC+jeDkZ6w2o5nLf3O5PYNBAAAAAAQnv3Pnu+t00ura2SYGgwAAAABOIzj5EXeHH2cwCAAAAOA0gpOfsNqM7vtws8vtmyVEMRgEAAAA8A+Ck59Y8/tfOn7S6nL7e7vWK8ZqAAAAgAsLwclP/OfLbS63LRsapEtqVSzGagAAAIALC8HJDyza/KfW/3HE5fZPXduYe5sAAACAMxCcSjl37226slE89zYBAAAAZyE4lXIvLPvV5XubggMtem5gs2KuCAAAALjwEJxKMavN6KWVv7vcfvTldbhEDwAAACgAwakUW/P7XzqR49rZprKhQRrdqXYxVwQAAABcmAhOpZg7I+kxIAQAAABQOIJTKeXOSHpMdgsAAAAUjeBUClltRo98usXl9kx2CwAAABSN4FQKpaVnKvN4rkttI0ICmewWAAAAcILgVArtzcp2ue3/XVqLe5sAAAAAJwhOpdCqXw+41C48OICR9AAAAAAXEJxKGavNaOGPGS61HXBxAmebAAAAABcQnEqZF5b9qr9zbS617dqQkfQAAAAAVxCcShGrzeillb+71DY6IlitkioUc0UAAABA6UBwKkXW/P6XTuRYXWo7vG0Sl+kBAAAALiI4lSJvrdnlUruwIAaFAAAAANxBcColrDajZdv2u9T28nqVOdsEAAAAuIHgVEq8sOxXnTzl2qAQg1snFm8xAAAAQClDcCoFrDaj2at2utQ2IiRQl9SqWLwFAQAAAKUMwakUSEvP1OG/c11q+3+X1uIyPQAAAMBNBKdSYP/RbJfaRYQEMigEAAAAcA4ITqVApbKhLrW7tUNNzjYBAAAA54DgVAqkpf/lUruLE5nwFgAAADgXBKcLnNVm9Mo36S61PXj8ZDFXAwAAAJROBKcL3AvLftWJHKtLbWPKhRVzNQAAAEDpRHC6gLkzDHl0RLBaJXGpHgAAAHAuCE4XMHeGIR/eNomBIQAAAIBzRHC6gO3NYhhyAAAAoCQQnC5gq3494FK7nslxnG0CAAAAzgPB6QJltRkt2brPpbbtalcq5moAAACA0o3gdIFKS8/UkexTLrWNiwov5moAAACA0o3gdIH6autel9oxmh4AAABw/ghOFyCrzejjjXtcastoegAAAMD5IzhdgNLSM5V53Pkw5GVDgxhNDwAAAPAAgtMFaP9R14Yh79eyGmebAAAAAA8gOF2AKpUNdandFfVji7kSAAAAwD8QnC5ExsPtAAAAABSJ4HQBWrbNtfmbDh4/WcyVAAAAAP6B4HSBcWdEvZhyYcVcDQAAAOAfCE4XGFdH1KtYJoT5mwAAAAAPIThdYFyd+PaqplUYUQ8AAADwEILTBcSdy/S6NIgr5moAAAAA/0FwuoBwmR4AAADgHV4PTjNmzFBiYqLCwsLUunVrpaWlFdn+2WefVd26dRUeHq6EhATdc889ys52bULYC52rE99ymR4AAADgWV4NTvPnz9eYMWOUkpKi9evXq0mTJurWrZv2799fYPt33nlHDz74oFJSUvTzzz/rtdde0/z58/XQQw+VcOXewcS3AAAAgHd4NThNmzZNI0aM0PDhw9WgQQPNmjVLERERev311wts/91336ldu3a64YYblJiYqK5du2rgwIFOz1KVGkx8CwAAAHhFkLd2nJOTo3Xr1mncuHH2ZQEBAercubNWr15d4HPatm2rt956S2lpaWrVqpV+//13LVq0SDfeeGOh+zl58qROnvx3ItisrCxJUm5urnJznd8vVJzy9u9qHUu2ZrjUbl/WCa+/Npw/d/sH/A99BEWhf8AZ+gic8Yc+4s5r81pwOnjwoKxWq2JjHS8ri42N1bZt2wp8zg033KCDBw+qffv2Msbo1KlTuu2224q8VG/y5MmaMGFCvuWLFy9WRETE+b0ID1myZInTNjYjvf9DoCTn9y79/tNGLfrfBg9UBl/gSv+Af6OPoCj0DzhDH4EzpbmPnDhxwuW2XgtO52LFihWaNGmSZs6cqdatW+u3337TXXfdpccff1zjx48v8Dnjxo3TmDFj7I+zsrKUkJCgrl27KjIysqRKL1Bubq6WLFmiLl26KDg4uMi236dn6viaH5xus0KZYI3u34XBIUoBd/oH/BN9BEWhf8AZ+gic8Yc+knc1miu8FpwqVaqkwMBA7du3z2H5vn37FBdX8BxE48eP14033qhbbrlFktSoUSMdP35ct956qx5++GEFBOS/ZSs0NFShofkHVQgODvaZDuBKLX+dOOXStvo2raqw0BBPlAUf4Ut9Fb6JPoKi0D/gDH0EzpTmPuLO6zqnwSHefPNNtWvXTlWqVNGuXbsknR4m/NNPP3V5GyEhIWrRooWWLl1qX2az2bR06VK1adOmwOecOHEiXzgKDAyUJBlTukdE2HnwuEvtmPgWAAAA8Dy3g9OLL76oMWPGqGfPnjp8+LCsVqskKTo6Ws8++6xb2xozZoxeeeUVzZ07Vz///LNGjhyp48ePa/jw4ZKkIUOGOAwe0bt3b7344ouaN2+e0tPTtWTJEo0fP169e/e2B6jSyGozejdtt9N28VFhTHwLAAAAFAO3L9WbPn26XnnlFfXt21dTpkyxL2/ZsqXuvfdet7bVv39/HThwQI8++qj27t2rpk2bKjU11T5gxO7dux3OMD3yyCOyWCx65JFHtGfPHlWuXFm9e/fWk08+6e7LuKCkpWdqb9ZJp+0GXFyde5sAAACAYuB2cEpPT1ezZs3yLQ8NDdXx465dTnam0aNHa/To0QWuW7FihcPjoKAgpaSkKCUlxe39XMj2H812qV1iJd8YJRAAAAAobdy+VC8pKUkbN27Mtzw1NVX169f3RE04i6v3N8WUCyvmSgAAAAD/5PYZpzFjxmjUqFHKzs6WMUZpaWl69913NXnyZL366qvFUaNf4/4mAAAAwPvcDk633HKLwsPD9cgjj+jEiRO64YYbVKVKFT333HMaMGBAcdTo17i/CQAAAPC+c5rHadCgQRo0aJBOnDihY8eOKSYmxtN14R/c3wQAAAB43zkNDnHq1CnVqVNHERERiog4/YH9119/VXBwsBITEz1do19z9b4l7m8CAAAAio/bg0MMGzZM3333Xb7l33//vYYNG+aJmnCGFjXKy9kVeAGW0+0AAAAAFA+3g9OGDRvUrl27fMsvueSSAkfbw/lZt+uQbKboNjZzuh0AAACA4uF2cLJYLDp69Gi+5UeOHJHVavVIUfiXq/c4udoOAAAAgPvcDk6XXnqpJk+e7BCSrFarJk+erPbt23u0OHCPEwAAAOAL3B4cYurUqbr00ktVt25ddejQQZL0zTffKCsrS8uWLfN4gf7u0HHnQ5EzhxMAAABQvNw+49SgQQNt3rxZ/fr10/79+3X06FENGTJE27ZtU3JycnHU6LesNqPHF/7stN34Xg2YwwkAAAAoRuc0j1OVKlU0adIkT9eCs6SlZyrjiPN7l8qXCSmBagAAAAD/dU7B6fDhw0pLS9P+/ftls9kc1g0ZMsQjhYGBIQAAAABf4XZw+uyzzzRo0CAdO3ZMkZGRslj+vUTMYrEQnDxo58HjLrVjYAgAAACgeLl9j9PYsWN100036dixYzp8+LAOHTpk/5eZmVkcNfolq83o3bTdTtsxMAQAAABQ/NwOTnv27NGdd96piIiI4qgH/0hLz9TeLOcj6g24uDoDQwAAAADFzO3g1K1bN/3www/FUQvO4Op9S4mVCLAAAABAcXP7HqdevXrpvvvu09atW9WoUSMFBwc7rO/Tp4/HivNnTHwLAAAA+A63g9OIESMkSRMnTsy3zmKxyGq1nn9VUKukCoqOCNbhE7mFtikfEcz9TQAAAEAJcDs4nT38OLzHeLsAAAAAwE+4fY8TSkZaemaRZ5sk6fCJXKWlM5IhAAAAUNzOaQLc48eP6+uvv9bu3buVk5PjsO7OO+/0SGH+jslvAQAAAN/hdnDasGGDevbsqRMnTuj48eOqUKGCDh48qIiICMXExBCcPITBIQAAAADf4falevfcc4969+6tQ4cOKTw8XGvWrNGuXbvUokULPfPMM8VRo1/KGxyiMBYx+S0AAABQUtwOThs3btTYsWMVEBCgwMBAnTx5UgkJCXrqqaf00EMPFUeNfmnJ1r1F3uNkJKX0bsDktwAAAEAJcDs4BQcHKyDg9NNiYmK0e/duSVJUVJT++OMPz1bnp6w2owmfbS2yTXREsLo0iCuhigAAAAD/5vY9Ts2aNdPatWtVp04ddezYUY8++qgOHjyoN998U8nJycVRo99JS89UxpGiB33IG1GvTa2KJVQVAAAA4L/cPuM0adIkxcfHS5KefPJJlS9fXiNHjtSBAwf08ssve7xAf8SIegAAAIBvcfuMU8uWLe1fx8TEKDU11aMFgRH1AAAAAF/DBLg+iBH1AAAAAN/i0hmn5s2ba+nSpSpfvryaNWsmi6XwkdzWr1/vseL8FSPqAQAAAL7FpeB01VVXKTQ0VJLUt2/f4qzH7zGiHgAAAOB7XApOKSkpkiSr1arLL79cjRs3VnR0dHHW5bcYUQ8AAADwPW7d4xQYGKiuXbvq0KFDxVWP32NEPQAAAMD3uD04RHJysn7//ffiqAViRD0AAADAF7kdnJ544gnde++9+vzzz5WRkaGsrCyHfzg/rZIqKD6q6FDEiHoAAABAyXJ7HqeePXtKkvr06eMwup4xRhaLRVar1XPV+aHAAIv6NInXSyvTC23Tp0k8I+oBAAAAJcjt4LR8+fLiqAP/sNqMFmzKKLLNgk0Zur97fcITAAAAUELcDk4dO3YsjjrwD1dG1cs4ks2oegAAAEAJcjs45Tlx4oR2796tnJwch+WNGzc+76L8GaPqAQAAAL7H7eB04MABDR8+XF988UWB67nH6fwwqh4AAADge9weVe/uu+/W4cOH9f333ys8PFypqamaO3eu6tSpowULFhRHjX4lb1S9wu5esohR9QAAAICS5vYZp2XLlunTTz9Vy5YtFRAQoBo1aqhLly6KjIzU5MmT1atXr+Ko02+4MqpeSu8GDAwBAAAAlCC3zzgdP35cMTExkqTy5cvrwIEDkqRGjRpp/fr1nq3OD6VuydDLRYSmWy9NUvfk+BKsCAAAAIDbwalu3bravn27JKlJkyZ66aWXtGfPHs2aNUvx8XygPx9Wm9GEz7bKFNFmwaYMWW1FtQAAAADgaW5fqnfXXXcpI+P0PEMpKSnq3r273n77bYWEhGjOnDmers+vMBQ5AAAA4JtcDk7XXXedbrnlFg0aNEgWy+n7a1q0aKFdu3Zp27Ztql69uipVqlRshfoDhiIHAAAAfJPLl+odOnRIvXr1UvXq1fXoo4/q999/lyRFRESoefPmhCYPYChyAAAAwDe5HJyWLl2q33//XTfffLPeeust1alTR506ddI777yjkydPFmeNfoOhyAEAAADf5NbgEDVq1NBjjz2m33//XUuWLFGVKlU0YsQIxcfHa9SoUVq3bl1x1ekXAgMsSundoMB1eWGKocgBAACAkuf2qHp5OnXqpLfeekt79+7V5MmTNW/ePLVu3dqTtfmtqIjgfMuiI4L14uDmDEUOAAAAeIHbo+qdKT09XXPmzNGcOXN05MgRde7c2VN1+aXULRka+db6AocjP3Qit8TrAQAAAHCa22ecsrOz9dZbb6lTp06qU6eO3njjDd18881KT09XampqcdToF5zN4WSRNOGzrczhBAAAAHiBy2ec0tLS9Prrr2v+/PnKzs7W1VdfrdTUVF1xxRX24clx7pzN4WTEHE4AAACAt7gcnC655BI1adJEjz/+uAYNGqTy5csXZ11+hzmcAAAAAN/lcnD64Ycf1Lx58+Ksxa8xhxMAAADgu1y+x4nQVLyYwwkAAADwXec8HDk868w5nM4OT8zhBAAAAHgXwcmHdE+O162XJunssTYsFunWS5OYwwkAAADwEoKTD0ndkqGXV6br7BHHbUZ6eWW6UrdkeKcwAAAAwM8RnHyEs3mcJOZxAgAAALzFpVH1mjVr5vJcTevXrz+vgvwV8zgBAAAAvsul4NS3b1/719nZ2Zo5c6YaNGigNm3aSJLWrFmjn376SbfffnuxFOkPmMcJAAAA8F0uBaeUlBT717fccovuvPNOPf744/na/PHHH56tzo8wjxMAAADgu9y+x+n999/XkCFD8i0fPHiwPvzwQ48U5Y+YxwkAAADwXW4Hp/DwcK1atSrf8lWrViksjLMh5+rMeZzOxjxOAAAAgHe5dKneme6++26NHDlS69evV6tWrSRJ33//vV5//XWNHz/e4wX6k+7J8XpxcHPdNW+jTp6y2ZfHRYUppXcD5nECAAAAvMTt4PTggw+qZs2aeu655/TWW29JkurXr6/Zs2erX79+Hi/Q33RPjlezhJ1ak56pIW1qqEdyvFolVeBMEwAAAOBFbgcnSerXrx8hqZhYbUa5NqOyoYGqVj6c0AQAAAD4gHOaAPfw4cN69dVX9dBDDykzM1PS6fmb9uzZ49Hi/E3qlgy1n7pM63Yd0rGTVk1atE3tpy5T6pYMb5cGAAAA+DW3g9PmzZt10UUXaerUqXr66ad1+PBhSdJHH32kcePGebo+v5G6JUMj31qfbxLcvUeyNfKt9YQnAAAAwIvcDk5jxozRsGHD9OuvvzqMotezZ0+tXLnSo8X5C6vNaMJnW2UKWJe3bMJnW2W1FdQCAAAAQHFzOzitXbtW//d//5dvedWqVbV3716PFOVvfth1KN+ZpjMZSRlHspWWnllyRQEAAACwczs4hYaGKisrK9/yX375RZUrV/ZIUf5m/9GTLrYrPFwBAAAAKD5uB6c+ffpo4sSJys3NlSRZLBbt3r1bDzzwgK699lqPF+gPYsqFutiOCYYBAAAAb3A7OP3nP//RsWPHFBMTo7///lsdO3ZU7dq1Va5cOT355JPFUWOp17JGecVHhamwQcctkuKjwtQqqUJJlgUAAADgH27P4xQVFaUlS5bo22+/1ebNm3Xs2DE1b95cnTt3Lo76/EJggEUpvRto5Fvr863LC1MpvRswnxMAAADgJec0Aa4ktW/fXu3bt/dkLX6te3K8XhzcXA9++KMO/51rXx4XFaaU3g3UPTnei9UBAAAA/u2cgtPSpUu1dOlS7d+/XzabzWHd66+/7pHC/FH35HhlHs/RQx9vUXKVSD3cq4FaJVXgTBMAAADgZW4HpwkTJmjixIlq2bKl4uPjZbHwod6TAgMsqlgmRPXjI9WmVkVvlwMAAABA5xCcZs2apTlz5ujGG28sjnr8mtVmVL1CGT3au4FiyoXJajOcbQIAAAB8gNvBKScnR23bti2OWvxa6pYMTfhsq8NEuPHc3wQAAAD4BLeHI7/lllv0zjvvFEctfuvLn/Zp5FvrHUKTJO09kq2Rb61X6pYML1UGAAAAQDqHM07Z2dl6+eWX9dVXX6lx48YKDg52WD9t2jSPFecPbEaavGibTAHrjE4PRz7hs63q0iCOy/YAAAAAL3E7OG3evFlNmzaVJG3ZssVhHQNFuG9HlkV7s04Wut5IyjiSrbT0TAaLAAAAALzE7eC0fPny4qjDb2XlOm8jSfuPZjtvBAAAAKBYuH2PEzwrMth5G0mKKRdWvIUAAAAAKJRLZ5yuueYazZkzR5GRkbrmmmuKbPvRRx95pDB/USvSKC4yVPuyThZ4n5NFUlxUmFolVSjp0gAAAAD8w6XgFBUVZb9/KSoqqlgL8jcBFumRnvV0x7xNskgO4SnvjrGU3g0YGAIAAADwIpeC0+zZswv82hNmzJihp59+Wnv37lWTJk00ffp0tWrVqtD2hw8f1sMPP6yPPvpImZmZqlGjhp599ln17NnTo3WVpG4NY/Xi4Ob55nGKYx4nAAAAwCe4PTiEJ82fP19jxozRrFmz1Lp1az377LPq1q2btm/frpiYmHztc3Jy1KVLF8XExOiDDz5Q1apVtWvXLkVHR5d88R7WPTlel9WN0cVPfqUTJ616aUgLXV43hjNNAAAAgA84p+D0wQcf6L333tPu3buVk5PjsG79+vUub2fatGkaMWKEhg8fLkmaNWuWFi5cqNdff10PPvhgvvavv/66MjMz9d1339nnj0pMTDyXl+CTwoID9eNj3bxdBgAAAICzuB2cnn/+eT388MMaNmyYPv30Uw0fPlw7duzQ2rVrNWrUKJe3k5OTo3Xr1mncuHH2ZQEBAercubNWr15d4HMWLFigNm3aaNSoUfr0009VuXJl3XDDDXrggQcUGBhY4HNOnjypkyf/nScpKytLkpSbm6vcXBfHAi8mefv3dh3wTfQPOEMfQVHoH3CGPgJn/KGPuPPa3A5OM2fO1Msvv6yBAwdqzpw5uv/++1WzZk09+uijyszMdHk7Bw8elNVqVWxsrMPy2NhYbdu2rcDn/P7771q2bJkGDRqkRYsW6bffftPtt9+u3NxcpaSkFPicyZMna8KECfmWL168WBERES7XW5yWLFkiSbKZ0xPiZuWeHqa8VqQRV+ohr38AhaGPoCj0DzhDH4EzpbmPnDhxwuW2bgen3bt3q23btpKk8PBwHT16VJJ044036pJLLtELL7zg7iZdZrPZFBMTo5dfflmBgYFq0aKF9uzZo6effrrQ4DRu3DiNGTPG/jgrK0sJCQnq2rWrIiMji61WV+Tm5mrJkiXq0qWLlv2SqYmf/ay/jv976WNcZKge6VlP3RrGFrEVlFZn9o+8S1OBM9FHUBT6B5yhj8AZf+gjeVejucLt4BQXF2cfza569epas2aNmjRpovT0dBlT0ExEBatUqZICAwO1b98+h+X79u1TXFxcgc+Jj49XcHCww2V59evX1969e5WTk6OQkJB8zwkNDVVoaGi+5cHBwT7TAZb9kqk75m3KN4/TvqyTumPeJr04uDkj6/kxX+qr8E30ERSF/gFn6CNwpjT3EXdeV4C7G+/UqZMWLFggSRo+fLjuuecedenSRf3799fVV1/t8nZCQkLUokULLV261L7MZrNp6dKlatOmTYHPadeunX777TfZbDb7sl9++UXx8fEFhqYLgc1ITyzaVuDkt3nLJny2VVab66EUAAAAgGe5fcbp5ZdftgeXUaNGqWLFivruu+/Up08f/d///Z9b2xozZoyGDh2qli1bqlWrVnr22Wd1/Phx+yh7Q4YMUdWqVTV58mRJ0siRI/XCCy/orrvu0h133KFff/1VkyZN0p133unuy/AZO7Is2pt1stD1RlLGkWylpWeqTa2KJVcYAAAAADu3g1NAQIACAv49UTVgwAANGDDgnHbev39/HThwQI8++qj27t2rpk2bKjU11T5gxO7dux32lZCQoC+//FL33HOPGjdurKpVq+quu+7SAw88cE779wVZLg7ksf9otvNGAAAAAIqFS8Fp8+bNLm+wcePGbhUwevRojR49usB1K1asyLesTZs2WrNmjVv78GWRLl5WGVMurHgLAQAAAFAol4JT06ZNZbFYnA7+YLFYZLVaPVKYv6gVaRQXGap9WScLvM/JIikuKkytkiqUdGkAAAAA/uFScEpPTy/uOvxWgEV6pGc93TFvU751eVM4pfRuoEAmdAIAAAC8xqXgVKNGjeKuw691axirFwc314TPtirjyL/3MsVFhSmldwOGIgcAAAC8zO3BISRp+/btmj59un7++WdJp+dSuuOOO1S3bl2PFudPuifHq0uDOKWlZ2r/0WzFlDt9eR5nmgAAAADvc3sepw8//FDJyclat26dmjRpoiZNmmj9+vVKTk7Whx9+WBw1+o3AAIva1Kqoq5pWVZtaFQlNAAAAgI9w+4zT/fffr3HjxmnixIkOy1NSUnT//ffr2muv9Vhx/mba4u3amnFUN7VPVNtalbxdDgAAAIB/uH3GKSMjQ0OGDMm3fPDgwcrIyPBIUf5q3e5D+urnfTpwtPAJcQEAAACUPLeD02WXXaZvvvkm3/Jvv/1WHTp08EhR/urvnNNDuYcGBXq5EgAAAABncvtSvT59+uiBBx7QunXrdMkll0iS1qxZo/fff18TJkzQggULHNrCddm5NklSeAjBCQAAAPAlbgen22+/XZI0c+ZMzZw5s8B1EpPhnovsU6ePV1iQ2ycCAQAAABQjt4OTzWYrjjogKfufS/XCgjnjBAAAAPgSj57aOHHihCc353eyT3GpHgAAAOCL3A5OV1xxhfbs2ZNv+ffff6+mTZt6oia/ZLUZ/Z1zSpL0859ZstqMlysCAAAAkMft4BQWFqbGjRtr/vz5kk5fuvfYY4+pQ4cO6tmzp8cL9Adf/rRP7acu09//DA5x1/yNaj91mVK3MLw7AAAA4Avcvsdp4cKFmjFjhm666SZ9+umn2rlzp3bt2qXPP/9cXbt2LY4aS7VNf1k0e/UmnX1+ae+RbI18a71eHNxc3ZPjvVIbAAAAgNPcDk6SNGrUKP3vf//T1KlTFRQUpBUrVqht27aerq3Us9qMPtoZkC80SZKRZJE04bOt6tIgToEBlhKuDgAAAEAety/VO3TokK699lq9+OKLeumll9SvXz917do139DkcO6HXYd0OKfwQGQkZRzJVlp6ZskVBQAAACAft884JScnKykpSRs2bFBSUpJGjBih+fPn6/bbb9fChQu1cOHC4qizVNp/9KSL7bKLuRIAAAAARXH7jNNtt92mlStXKikpyb6sf//+2rRpk3JycjxaXGkXUy7UxXZhxVwJAAAAgKK4HZzGjx+vgID8T6tWrZqWLFnikaL8Rcsa5RUdYlTYxXoWSfFRYWqVVKEkywIAAABwFpeD01NPPaW///7b/njVqlU6efLfS82OHj2q22+/3bPVlXKBARZdk2grcF1emErp3YCBIQAAAAAvczk4jRs3TkePHrU/7tGjh8NEuCdOnNBLL73k2er8QJOKRtMHNFGFMiEOy+OiwhiKHAAAAPARLg8OYYwp8jHOXbeGsSoTFqLhc9aqanS4nrm+iVolVeBMEwAAAOAjzmkeJ3hervX0JXsxkaFqU6uil6sBAAAAcCaCk4/oXD9WP0/sLitn8gAAAACf41ZwevXVV1W2bFlJ0qlTpzRnzhxVqlRJkhzuf4L7AgIsCg8J9HYZAAAAAArgcnCqXr26XnnlFfvjuLg4vfnmm/naAAAAAEBp43Jw2rlzZzGWga9/OaBPN+5RyxoVdENrAigAAADgS9yeABfFY/veLH20fo9+2Jnp7VIAAAAAnIXg5CNO5p4eVS80mG8JAAAA4Gv4lO4jcv4Zjjw0iAEiAAAAAF9DcPIRJ0+dDk4hQXxLAAAAAF/Dp3QfcTLXKkkKJTgBAAAAPuecPqXv2LFDjzzyiAYOHKj9+/dLkr744gv99NNPHi3On+RdqhcSSHACAAAAfI3bn9K//vprNWrUSN9//70++ugjHTt2TJK0adMmpaSkeLxAf8HgEAAAAIDvcvtT+oMPPqgnnnhCS5YsUUhIiH15p06dtGbNGo8W50+evLqR1j3SWYNa1/B2KQAAAADO4nZw+vHHH3X11VfnWx4TE6ODBw96pCh/Y7UZbfzjsL797aA2/++IrDbj7ZIAAAAAnCHI3SdER0crIyNDSUlJDss3bNigqlWreqwwf7HpL4sm/2el9madtC+LjwpTSu8G6p4c78XKAAAAAORx+4zTgAED9MADD2jv3r2yWCyy2WxatWqV7r33Xg0ZMqQ4aiy1vvxpn17/JcAhNEnS3iPZGvnWeqVuyfBSZQAAAADO5HZwmjRpkurVq6eEhAQdO3ZMDRo00KWXXqq2bdvqkUceKY4aSyWrzeiJRdsKXJd3od6Ez7Zy2R4AAADgA9y+VC8kJESvvPKKxo8fry1btujYsWNq1qyZ6tSpUxz1lVpp6Zn/nGmyFLjeSMo4kq209Ey1qVWxRGsDAAAA4Mjt4PTtt9+qffv2ql69uqpXr14cNfmF/UezPdoOAAAAQPFx+1K9Tp06KSkpSQ899JC2bt1aHDX5hZhyYR5tBwAAAKD4uB2c/vzzT40dO1Zff/21kpOT1bRpUz399NP63//+Vxz1lVqtkiooLjJU/97R5Mii06PrtUqqUKJ1AQAAAMjP7eBUqVIljR49WqtWrdKOHTt0/fXXa+7cuUpMTFSnTp2Ko8ZSKTDAokd61pOU/y6nvMcpvRsoMKDge6AAAAAAlBy3g9OZkpKS9OCDD2rKlClq1KiRvv76a0/V5Re6NYzVTRfZFBsZ6rA8LipMLw5uzjxOAAAAgI9we3CIPKtWrdLbb7+tDz74QNnZ2brqqqs0efJkT9bmF5pUNLp/0KVa9stf2nckW7Vjyqpt7UqcaQIAAAB8iNvBady4cZo3b57+/PNPdenSRc8995yuuuoqRUREFEd9fiEwwKKejTi7BAAAAPgqt4PTypUrdd9996lfv36qVKlScdQEAAAAAD7F7eC0atWq4qjDr+VabZqU+pNCAgM0putFCg0K9HZJAAAAAM7gUnBasGCBevTooeDgYC1YsKDItn369PFIYf4kO9em2at2SpLu6XKRd4sBAAAAkI9Lwalv377au3evYmJi1Ldv30LbWSwWWa1WT9XmN3KtNvvXIYHnNdAhAAAAgGLgUnCy2WwFfg3PyAtOQQEWBTCaHgAAAOBz3D698cYbb+jkyZP5lufk5OiNN97wSFH+Juef4BTM2SYAAADAJ7n9SX348OE6cuRIvuVHjx7V8OHDPVKUv8k9ZSRJwYGcbQIAAAB8kdvByRgjiyX/B/z//e9/ioqK8khR/ibvUr0QRtMDAAAAfJLLw5E3a9ZMFotFFotFV1xxhYKC/n2q1WpVenq6unfvXixFlnZ5l+qFcMYJAAAA8EkuB6e80fQ2btyobt26qWzZsvZ1ISEhSkxM1LXXXuvxAv3BRTFltWxsRxlvFwIAAACgQC4Hp5SUFElSYmKi+vfvr7CwsGIryt+EBgeqZmWOJwAAAOCrXA5OeYYOHVocdQAAAACAz3I7OFmtVv33v//Ve++9p927dysnJ8dhfWZmpseK8xe/7j+mhVv2qXqFCPW/uLq3ywEAAABwFrdH1ZswYYKmTZum/v3768iRIxozZoyuueYaBQQE6LHHHiuGEku/HQeOa8byHfpw3R5vlwIAAACgAG4Hp7fffluvvPKKxo4dq6CgIA0cOFCvvvqqHn30Ua1Zs6Y4aiz18oYjDw5iVD0AAADAF7kdnPbu3atGjRpJksqWLWufDPfKK6/UwoULPVudn8jOtUqSMo/naPWOv2S1Mb4eAAAA4EvcDk7VqlVTRkaGJKlWrVpavHixJGnt2rUKDQ31bHV+YNNfFk1J/UWS9HPGUQ18ZY3aT12m1C0ZXq4MAAAAQB63g9PVV1+tpUuXSpLuuOMOjR8/XnXq1NGQIUN00003ebzA0uzLn/bp9V8ClJV9ymH53iPZGvnWesITAAAA4CPcHlVvypQp9q/79++v6tWra/Xq1apTp4569+7t0eJKM6vN6IlF2wpcZyRZJE34bKu6NIhTYAD3PgEAAADe5HZwOlubNm3Upk0bT9TiV9LSM7U366ROR6T8jKSMI9lKS89Um1oVS7Q2AAAAAI5cCk4LFixweYN9+vQ552L8yf6j2R5tBwAAAKD4uBSc+vbt69LGLBaLrFbr+dTjN2LKhXm0HQAAAIDi41JwstlsxV2H32mVVEFxkaHam5Wtgi7Xs0iKiwpTq6QKJV4bAAAAAEduj6oHzwgMsOiRnvUKXJcXo1J6N2BgCAAAAMAHuD04xMSJE4tc/+ijj55zMf6mW8NY3XSRTfN2huhEzr+XOMZFhSmldwN1T473YnUAAAAA8rgdnD7++GOHx7m5uUpPT1dQUJBq1apFcHJTk4pGR8vF6f11e9QjOU5D2iSqVVIFzjQBAAAAPsTt4LRhw4Z8y7KysjRs2DBdffXVHinK39iMkSQlV41i6HEAAADAB3nkHqfIyEhNmDBB48eP98Tm/M4p6+ngFBLILWcAAACAL/LYJ/UjR47oyJEjntqcX8kLTkGBXJ4HAAAA+CK3L9V7/vnnHR4bY5SRkaE333xTPXr08Fhh/iT3n+HegzjjBAAAAPgkt4PTf//7X4fHAQEBqly5soYOHapx48Z5rDB/knfGKZgBIQAAAACf5HZwSk9PL446/Nr93S7SbZfVVmLFCG+XAgAAAKAAbgcneF6tymUUHBzs7TIAAAAAFMLt4JSdna3p06dr+fLl2r9/v2z/3J+TZ/369R4rDgAAAAB8gdvB6eabb9bixYt13XXXqVWrVrJYuC/nfH22OUNHsq3qXD9WCRW4XA8AAADwNW4Hp88//1yLFi1Su3btiqMev/T6ql3a8meWEiuWITgBAAAAPsjt8a+rVq2qcuXKFUctfuuUNW84cs7eAQAAAL7I7eD0n//8Rw888IB27dpVHPX4pVzbPxPgBjCPEwAAAOCL3L5Ur2XLlsrOzlbNmjUVERGRbzS4zMxMjxXnL+zzOHHGCQAAAPBJbgengQMHas+ePZo0aZJiY2MZHMIDcv+5VC84kDNOAAAAgC9yOzh99913Wr16tZo0aVIc9filU3mX6nHGCQAAAPBJbp/iqFevnv7+++/iqMVvccYJAAAA8G1uf1KfMmWKxo4dqxUrVuivv/5SVlaWw79zMWPGDCUmJiosLEytW7dWWlqaS8+bN2+eLBaL+vbte0779RXTBzTRnOEXq2p0uLdLAQAAAFAAty/V6969uyTpiiuucFhujJHFYpHVanVre/Pnz9eYMWM0a9YstW7dWs8++6y6deum7du3KyYmptDn7dy5U/fee686dOjg7kvwOa2TKuQbZAMAAACA73A7OC1fvtyjBUybNk0jRozQ8OHDJUmzZs3SwoUL9frrr+vBBx8s8DlWq1WDBg3ShAkT9M033+jw4cMerQkAAAAAzuR2cOrYsaPHdp6Tk6N169Zp3Lhx9mUBAQHq3LmzVq9eXejzJk6cqJiYGN1888365ptvitzHyZMndfLkSfvjvMsJc3NzlZube56v4Pzk5ubKGOnN1TsVFhKkPo3jFRoc6NWa4Dvy+qe3+yl8F30ERaF/wBn6CJzxhz7izmtzOzitXLmyyPWXXnqpy9s6ePCgrFarYmNjHZbHxsZq27ZtBT7n22+/1WuvvaaNGze6tI/JkydrwoQJ+ZYvXrxYERERLtdaXGxGmrjol9MP/rdZZbhiD2dZsmSJt0uAj6OPoCj0DzhDH4EzpbmPnDhxwuW2bgenyy67LN+yM+dycvceJ3ccPXpUN954o1555RVVqlTJpeeMGzdOY8aMsT/OyspSQkKCunbtqsjIyOIq1SW5ublamPpvR+zRvavKhrr9LUEplZubqyVLlqhLly7cA4cC0UdQFPoHnKGPwBl/6CPuDG7n9qf0Q4cOOTzOzc3Vhg0bNH78eD355JNubatSpUoKDAzUvn37HJbv27dPcXFx+drv2LFDO3fuVO/eve3LbLbTQ3kHBQVp+/btqlWrlsNzQkNDFRoamm9bwcHBPtEBrObfryPCQhQcxKV6cOQrfRW+iz6CotA/4Ax9BM6U5j7izutyOzhFRUXlW9alSxeFhIRozJgxWrduncvbCgkJUYsWLbR06VL7kOI2m01Lly7V6NGj87WvV6+efvzxR4dljzzyiI4eParnnntOCQkJ7r0YH5Br+/frdTsPqXXNigoMYCJcAAAAwJd47Lqw2NhYbd++3e3njRkzRkOHDlXLli3VqlUrPfvsszp+/Lh9lL0hQ4aoatWqmjx5ssLCwpScnOzw/OjoaEnKt/xC8OVP+/T05n/PMN3w6veKjwpTSu8G6p4c78XKAAAAAJzJ7eC0efNmh8fGGGVkZGjKlClq2rSp2wX0799fBw4c0KOPPqq9e/eqadOmSk1NtQ8YsXv3bgUEuD1Pr89L3ZKhO+Ztkjlr+d4j2Rr51nq9OLg54QkAAADwEW4Hp6ZNm8piscgYx4/8l1xyiV5//fVzKmL06NEFXponSStWrCjyuXPmzDmnfXqT1WY04bOt/4Qmx8vyzD9LJny2VV0axHHZHgAAAOAD3A5O6enpDo8DAgJUuXJlhYWFeayo0i4tPVMZR7ILXW8kZRzJVlp6ptrUqlhyhQEAAAAokNvBqUaNGsVRh1/Zf7Tw0HQu7QAAAAAUL5dvHlq2bJkaNGhQ4FjnR44cUcOGDfXNN994tLjSKqaca2fnXG0HAAAAoHi5HJyeffZZjRgxosBJY6OiovR///d/mjZtmkeLK61aJVVQfFSYCrt7ySIpPipMrZIqlGRZAAAAAArhcnDatGmTunfvXuj6rl27ujWHkz8LDLAopXeDfx45DrKRF6ZSejdgYAgAAADAR7gcnPbt21fkzLpBQUE6cOCAR4ryB92T4zV9QBOVPesus7ioMIYiBwAAAHyMy4NDVK1aVVu2bFHt2rULXL9582bFx/Nh3x3dGsZq6yabZv4cqLjIUP23fzO1SqrAmSYAAADAx7h8xqlnz54aP368srPzj/T2999/KyUlRVdeeaVHi/MHeRfqlS8Tqja1KhKaAAAAAB/k8hmnRx55RB999JEuuugijR49WnXr1pUkbdu2TTNmzJDVatXDDz9cbIWWVrZ/klMQgQkAAADwWS4Hp9jYWH333XcaOXKkxo0bJ2NOf+K3WCzq1q2bZsyYodjY2GIrtLTKC06caQIAAAB8l1sT4NaoUUOLFi3SoUOH9Ntvv8kYozp16qh8+fLFVV+pZ+WMEwAAAODz3ApOecqXL6+LL77Y07X4pepljf5zXSNVigz3dikAAAAACnFOwQmeUz5U6tkkvsih3gEAAAB4l8uj6gEAAACAvyI4edlf2dLirfu0Yfchb5cCAAAAoBAEJy/bfsSiUe9u0swVO7xdCgAAAIBCEJy8jHmcAAAAAN9HcPIy5nECAAAAfB/BycuYxwkAAADwfQQnL/v3jBPfCgAAAMBX8Wndy2z//M8ZJwAAAMB3EZy8zH7GKZDgBAAAAPiqIG8X4O8aljdq17yB6sRFebsUAAAAAIUgOHlZtTJSz5bVFBwc7O1SAAAAABSCS/UAAAAAwAmCk5ftPSF989tB7Tx43NulAAAAACgEwcnLVu0L0E1z1+uDdf/zdikAAAAACkFw8jKrfR4nRtUDAAAAfBXBycvMP8GJeZwAAAAA30Vw8jIr8zgBAAAAPo/g5GW2f/7njBMAAADguwhOXmaz3+PEtwIAAADwVXxa9zIb9zgBAAAAPi/I2wX4u1aVjXq0qquWieW9XQoAAACAQhCcvKxBeaOebWsoODjY26UAAAAAKASX6gEAAACAEwQnL9tzXFq365D+OnbS26UAAAAAKATBycs+3hmgAa+u1be/HfR2KQAAAAAKQXDyMps5PZpeEMORAwAAAD6LT+teZrXP48Rw5AAAAICvIjh5kdVmdOLU6a9/239U1rxJnQAAAAD4FIKTl6RuydBl/1mp/dmnzzQ9s/gXtZ+6TKlbMrxcGQAAAICzEZy8IHVLhka+tV57sxxH0tt7JFsj31pPeAIAAAB8DMGphFltRhM+26qCLsrLWzbhs61ctgcAAAD4EIJTCUtLz1TGkexC1xtJGUeylZaeWXJFAQAAACgSwamE7T9aeGg6l3YAAAAAih/BqYTFlAvzaDsAAAAAxY/gVMJaJVVQfFSYCpu1ySIpPipMrZIqlGRZAAAAAIpAcCphgQEWpfRuIEn5wlPe45TeDZgQFwAAAPAhBCcv6J4crxcHN1dsZKjD8rioML04uLm6J8d7qTIAAAAABQnydgH+qntyvC6rU1FNJyxWts2ip65trGtbVONMEwAAAOCDOOPkRYEBFgX88x1oXiOa0AQAAAD4KIKTl5l/5rm1WAhNAAAAgK8iOHmZ7Z//AwlOAAAAgM8iOHlZ3hknLtMDAAAAfBeDQ3hZpyo2JdWsrciwYG+XAgAAAKAQBCcv65Fg1LNLHQUHE5wAAAAAX8WlegAAAADgBMHJy/b9LaUfPK5TVpvzxgAAAAC8guDkRTab0aSNQer63CplZZ/ydjkAAAAACkFw8iJb3pB6YjhyAAAAwJcRnLzI+m9ukoXvBAAAAOCz+LjuRTYbZ5wAAACACwHByYusZ16qxwS4AAAAgM8iOHmROSM4ccIJAAAA8F0EJy86cwRyLtUDAAAAfFeQtwvwZyFBFl0Wb1ONxEQu1QMAAAB8GMHJiyJCgnR1ok09e9aThTNOAAAAgM/iUj0AAAAAcILg5EWnrDZlnpT2ZWV7uxQAAAAARSA4edG+oyc1YX2QOj/7rbdLAQAAAFAEgpMXWf+ZADeA+5sAAAAAn0Zw8iKbITgBAAAAFwKCkxfZ/pnHKZDvAgAAAODT+MjuRVbOOAEAAAAXBIKTFxmCEwAAAHBBIDh5kfWfS/UCyE0AAACATwvydgH+LDoiWG1jbWpQp4q3SwEAAABQBIKTF8VHhal/TZt6drvI26UAAAAAKAKX6gEAAACAEwQnL8q12nQ8VzqanevtUgAAAAAUgeDkRZv+d0QP/RCka2d97+1SAAAAABSB4ORFVtvp4cgtDEcOAAAA+DSCkxfZ/pnHKZDvAgAAAODT+MjuRf/O48QZJwAAAMCXEZy8yPxzxongBAAAAPg2gpMXWe2X6hGcAAAAAF9GcPKivMEhyE0AAACAbwvydgH+LC4yTC0r2XRJnUreLgUAAABAEQhOXtSwSqRurGNTzytqe7sUAAAAAEXwiUv1ZsyYocTERIWFhal169ZKS0srtO0rr7yiDh06qHz58ipfvrw6d+5cZHsAAAAAOF9eD07z58/XmDFjlJKSovXr16tJkybq1q2b9u/fX2D7FStWaODAgVq+fLlWr16thIQEde3aVXv27Cnhys/fKatNOVYpN29ccgAAAAA+yevBadq0aRoxYoSGDx+uBg0aaNasWYqIiNDrr79eYPu3335bt99+u5o2bap69erp1Vdflc1m09KlS0u48vO3aMs+3ZcWpFveWO/tUgAAAAAUwav3OOXk5GjdunUaN26cfVlAQIA6d+6s1atXu7SNEydOKDc3VxUqVChw/cmTJ3Xy5En746ysLElSbm6ucnNzz6P685d76tQ/Xxmv1wLfk9cn6BsoDH0ERaF/wBn6CJzxhz7izmvzanA6ePCgrFarYmNjHZbHxsZq27ZtLm3jgQceUJUqVdS5c+cC10+ePFkTJkzIt3zx4sWKiIhwv2gP+nG/RVKgDmX+pUWLFnm1FviuJUuWeLsE+Dj6CIpC/4Az9BE4U5r7yIkTJ1xue0GPqjdlyhTNmzdPK1asUFhYWIFtxo0bpzFjxtgfZ2Vl2e+LioyMLKlSC5SVtlvasU0xlSurZ88WXq0Fvic3N1dLlixRly5dFBwc7O1y4IPoIygK/QPO0EfgjD/0kbyr0Vzh1eBUqVIlBQYGat++fQ7L9+3bp7i4uCKf+8wzz2jKlCn66quv1Lhx40LbhYaGKjQ0NN/y4OBgr3cAS8DpW8wCAwK8Xgt8ly/0Vfg2+giKQv+AM/QROFOa+4g7r8urg0OEhISoRYsWDgM75A300KZNm0Kf99RTT+nxxx9XamqqWrZsWRKlFgubMZKkwACLlysBAAAAUBSvX6o3ZswYDR06VC1btlSrVq307LPP6vjx4xo+fLgkaciQIapataomT54sSZo6daoeffRRvfPOO0pMTNTevXslSWXLllXZsmW99jrOhc12OjiRmwAAAADf5vXg1L9/fx04cECPPvqo9u7dq6ZNmyo1NdU+YMTu3bsVEPDvibEXX3xROTk5uu666xy2k5KSoscee6wkSz9v1cqHq3EFm5okRHm7FAAAAABF8HpwkqTRo0dr9OjRBa5bsWKFw+OdO3cWf0ElpONFlXX8N5t6tk/ydikAAAAAiuD1CXABAAAAwNcRnAAAAADACYKTF738TbruXh2ohz75ydulAAAAACgCwcmLrDYjI4bUAwAAAHwdwcmL/hmNXAEWwhMAAADgywhOXnTKapMk7c48odU7/pI1L0kBAAAA8Ck+MRy5P0rdkqE5q3dJklb/nqnVv69RfFSYUno3UPfkeC9XBwAAAOBMnHHygtQtGRr51nodO2l1WL73SLZGvrVeqVsyvFQZAAAAgIIQnEqY1WY04bOtKuiivLxlEz7bymV7AAAAgA8hOJWwtPRMZRzJLnS9kZRxJFtp6ZklVxQAAACAIhGcStj+o4WHpnNpBwAAAKD4EZxKWEy5MI+2AwAAAFD8CE4lrFVSBcVHhRU67a1FUnxUmFolVSjJsgAAAAAUgeBUwgIDLErp3UCS8oWnvMcpvRsoMIBJcQEAAABfQXDygu7J8XpxcHPFRoY6LI+LCtOLg5szjxMAAADgY5gA10u6J8frsjoV9cL8VNVs2FTx0WXUKqkCZ5oAAAAAH0Rw8qLAAIvqRBn1bByv4OBgb5cDAAAAoBBcqgcAAAAAThCcAAAAAMAJghMAAAAAOEFwAgAAAAAnCE4AAAAA4ATBCQAAAACcIDgBAAAAgBMEJwAAAABwguAEAAAAAE4QnAAAAADACYITAAAAADhBcAIAAAAAJwhOAAAAAOBEkLcLKGnGGElSVlaWlyuRcnNzdeLECWVlZSk4ONjb5cDH0D/gDH0ERaF/wBn6CJzxhz6SlwnyMkJR/C44HT16VJKUkJDg5UoAAAAA+IKjR48qKiqqyDYW40q8KkVsNpv+/PNPlStXThaLxau1ZGVlKSEhQX/88YciIyO9Wgt8D/0DztBHUBT6B5yhj8AZf+gjxhgdPXpUVapUUUBA0Xcx+d0Zp4CAAFWrVs3bZTiIjIwstZ0R54/+AWfoIygK/QPO0EfgTGnvI87ONOVhcAgAAAAAcILgBAAAAABOEJy8KDQ0VCkpKQoNDfV2KfBB9A84Qx9BUegfcIY+AmfoI478bnAIAAAAAHAXZ5wAAAAAwAmCEwAAAAA4QXACAAAAACcITgAAAADgBMHJS2bMmKHExESFhYWpdevWSktL83ZJKAGTJ0/WxRdfrHLlyikmJkZ9+/bV9u3bHdpkZ2dr1KhRqlixosqWLatrr71W+/btc2ize/du9erVSxEREYqJidF9992nU6dOleRLQQmYMmWKLBaL7r77bvsy+gf27NmjwYMHq2LFigoPD1ejRo30ww8/2NcbY/Too48qPj5e4eHh6ty5s3799VeHbWRmZmrQoEGKjIxUdHS0br75Zh07dqykXwqKgdVq1fjx45WUlKTw8HDVqlVLjz/+uM4cC4w+4l9Wrlyp3r17q0qVKrJYLPrkk08c1nuqP2zevFkdOnRQWFiYEhIS9NRTTxX3Syt5BiVu3rx5JiQkxLz++uvmp59+MiNGjDDR0dFm37593i4Nxaxbt25m9uzZZsuWLWbjxo2mZ8+epnr16ubYsWP2NrfddptJSEgwS5cuNT/88IO55JJLTNu2be3rT506ZZKTk03nzp3Nhg0bzKJFi0ylSpXMuHHjvPGSUEzS0tJMYmKiady4sbnrrrvsy+kf/i0zM9PUqFHDDBs2zHz//ffm999/N19++aX57bff7G2mTJlioqKizCeffGI2bdpk+vTpY5KSkszff/9tb9O9e3fTpEkTs2bNGvPNN9+Y2rVrm4EDB3rjJcHDnnzySVOxYkXz+eefm/T0dPP++++bsmXLmueee87ehj7iXxYtWmQefvhh89FHHxlJ5uOPP3ZY74n+cOTIERMbG2sGDRpktmzZYt59910THh5uXnrppZJ6mSWC4OQFrVq1MqNGjbI/tlqtpkqVKmby5MlerAresH//fiPJfP3118YYYw4fPmyCg4PN+++/b2/z888/G0lm9erVxpjTb4ABAQFm79699jYvvviiiYyMNCdPnizZF4BicfToUVOnTh2zZMkS07FjR3twon/ggQceMO3bty90vc1mM3Fxcebpp5+2Lzt8+LAJDQ017777rjHGmK1btxpJZu3atfY2X3zxhbFYLGbPnj3FVzxKRK9evcxNN93ksOyaa64xgwYNMsbQR/zd2cHJU/1h5syZpnz58g6/Zx544AFTt27dYn5FJYtL9UpYTk6O1q1bp86dO9uXBQQEqHPnzlq9erUXK4M3HDlyRJJUoUIFSdK6deuUm5vr0D/q1aun6tWr2/vH6tWr1ahRI8XGxtrbdOvWTVlZWfrpp59KsHoUl1GjRqlXr14O/UCif0BasGCBWrZsqeuvv14xMTFq1qyZXnnlFfv69PR07d2716GPREVFqXXr1g59JDo6Wi1btrS36dy5swICAvT999+X3ItBsWjbtq2WLl2qX375RZK0adMmffvtt+rRo4ck+ggceao/rF69WpdeeqlCQkLsbbp166bt27fr0KFDJfRqil+QtwvwNwcPHpTVanX4UCNJsbGx2rZtm5eqgjfYbDbdfffdateunZKTkyVJe/fuVUhIiKKjox3axsbGau/evfY2BfWfvHW4sM2bN0/r16/X2rVr862jf+D333/Xiy++qDFjxuihhx7S2rVrdeeddyokJERDhw61f48L6gNn9pGYmBiH9UFBQapQoQJ9pBR48MEHlZWVpXr16ikwMFBWq1VPPvmkBg0aJEn0ETjwVH/Yu3evkpKS8m0jb1358uWLpf6SRnACvGTUqFHasmWLvv32W2+XAh/xxx9/6K677tKSJUsUFhbm7XLgg2w2m1q2bKlJkyZJkpo1a6YtW7Zo1qxZGjp0qJergy9477339Pbbb+udd95Rw4YNtXHjRt19992qUqUKfQQ4T1yqV8IqVaqkwMDAfKNg7du3T3FxcV6qCiVt9OjR+vzzz7V8+XJVq1bNvjwuLk45OTk6fPiwQ/sz+0dcXFyB/SdvHS5c69at0/79+9W8eXMFBQUpKChIX3/9tZ5//nkFBQUpNjaW/uHn4uPj1aBBA4dl9evX1+7duyX9+z0u6ndMXFyc9u/f77D+1KlTyszMpI+UAvfdd58efPBBDRgwQI0aNdKNN96oe+65R5MnT5ZEH4EjT/UHf/ndQ3AqYSEhIWrRooWWLl1qX2az2bR06VK1adPGi5WhJBhjNHr0aH388cdatmxZvtPaLVq0UHBwsEP/2L59u3bv3m3vH23atNGPP/7o8Ca2ZMkSRUZG5vtAhQvLFVdcoR9//FEbN260/2vZsqUGDRpk/5r+4d/atWuXbwqDX375RTVq1JAkJSUlKS4uzqGPZGVl6fvvv3foI4cPH9a6devsbZYtWyabzabWrVuXwKtAcTpx4oQCAhw/3gUGBspms0mij8CRp/pDmzZttHLlSuXm5trbLFmyRHXr1i01l+lJYjhyb5g3b54JDQ01c+bMMVu3bjW33nqriY6OdhgFC6XTyJEjTVRUlFmxYoXJyMiw/ztx4oS9zW233WaqV69uli1bZn744QfTpk0b06ZNG/v6vOGmu3btajZu3GhSU1NN5cqVGW66lDpzVD1j6B/+Li0tzQQFBZknn3zS/Prrr+btt982ERER5q233rK3mTJliomOjjaffvqp2bx5s7nqqqsKHFq4WbNm5vvvvzfffvutqVOnDkNNlxJDhw41VatWtQ9H/tFHH5lKlSqZ+++/396GPuJfjh49ajZs2GA2bNhgJJlp06aZDRs2mF27dhljPNMfDh8+bGJjY82NN95otmzZYubNm2ciIiIYjhyeMX36dFO9enUTEhJiWrVqZdasWePtklACJBX4b/bs2fY2f//9t7n99ttN+fLlTUREhLn66qtNRkaGw3Z27txpevToYcLDw02lSpXM2LFjTW5ubgm/GpSEs4MT/QOfffaZSU5ONqGhoaZevXrm5Zdfdlhvs9nM+PHjTWxsrAkNDTVXXHGF2b59u0Obv/76ywwcONCULVvWREZGmuHDh5ujR4+W5MtAMcnKyjJ33XWXqV69ugkLCzM1a9Y0Dz/8sMMw0fQR/7J8+fICP3sMHTrUGOO5/rBp0ybTvn17ExoaaqpWrWqmTJlSUi+xxFiMOWMqaQAAAABAPtzjBAAAAABOEJwAAAAAwAmCEwAAAAA4QXACAAAAACcITgAAAADgBMEJAAAAAJwgOAEAAACAEwQnAAAAAHCC4AQAcNvOnTtlsVi0ceNGb5dit23bNl1yySUKCwtT06ZNvV0OAKCUITgBwAVo2LBhslgsmjJlisPyTz75RBaLxUtVeVdKSorKlCmj7du3a+nSpYW227t3r+644w7VrFlToaGhSkhIUO/evYt8jj8aNmyY+vbt6+0yAMBnEJwA4AIVFhamqVOn6tChQ94uxWNycnLO+bk7duxQ+/btVaNGDVWsWLHANjt37lSLFi20bNkyPf300/rxxx+Vmpqqyy+/XKNGjTrnfQMASj+CEwBcoDp37qy4uDhNnjy50DaPPfZYvsvWnn32WSUmJtof551ZmDRpkmJjYxUdHa2JEyfq1KlTuu+++1ShQgVVq1ZNs2fPzrf9bdu2qW3btgoLC1NycrK+/vprh/VbtmxRjx49VLZsWcXGxurGG2/UwYMH7esvu+wyjR49WnfffbcqVaqkbt26Ffg6bDabJk6cqGrVqik0NFRNmzZVamqqfb3FYtG6des0ceJEWSwWPfbYYwVu5/bbb5fFYlFaWpquvfZaXXTRRWrYsKHGjBmjNWvW2Nvt3r1bV111lcqWLavIyEj169dP+/bty3dcX3/9dVWvXl1ly5bV7bffLqvVqqeeekpxcXGKiYnRk08+6bB/i8WiF198UT169FB4eLhq1qypDz74wKHNjz/+qE6dOik8PFwVK1bUrbfeqmPHjuX7fj3zzDOKj49XxYoVNWrUKOXm5trbnDx5Uvfee6+qVq2qMmXKqHXr1lqxYoV9/Zw5cxQdHa0vv/xS9evXV9myZdW9e3dlZGTYX9/cuXP16aefymKxyGKxaMWKFcrJydHo0aMVHx+vsLAw1ahRo8j+BwClCcEJAC5QgYGBmjRpkqZPn67//e9/57WtZcuW6c8//9TKlSs1bdo0paSk6Morr1T58uX1/fff67bbbtP//d//5dvPfffdp7Fjx2rDhg1q06aNevfurb/++kuSdPjwYXXq1EnNmjXTDz/8oNTUVO3bt0/9+vVz2MbcuXMVEhKiVatWadasWQXW99xzz+k///mPnnnmGW3evFndunVTnz599Ouvv0qSMjIy1LBhQ40dO1YZGRm69957820jMzNTqampGjVqlMqUKZNvfXR0tKTTIe2qq65SZmamvv76ay1ZskS///67+vfv79B+x44d+uKLL5Samqp3331Xr732mnr16qX//e9/+vrrrzV16lQ98sgj+v777x2eN378eF177bXatGmTBg0apAEDBujnn3+WJB0/flzdunVT+fLltXbtWr3//vv66quvNHr0aIdtLF++XDt27NDy5cs1d+5czZkzR3PmzLGvHz16tFavXq158+Zp8+bNuv7669W9e3f78ZKkEydO6JlnntGbb76plStXavfu3fbjdu+996pfv372MJWRkaG2bdvq+eef14IFC/Tee+9p+/btevvttx1COACUagYAcMEZOnSoueqqq4wxxlxyySXmpptuMsYY8/HHH5sz39pTUlJMkyZNHJ773//+19SoUcNhWzVq1DBWq9W+rG7duqZDhw72x6dOnTJlypQx7777rjHGmPT0dCPJTJkyxd4mNzfXVKtWzUydOtUYY8zjjz9uunbt6rDvP/74w0gy27dvN8YY07FjR9OsWTOnr7dKlSrmySefdFh28cUXm9tvv93+uEmTJiYlJaXQbXz//fdGkvnoo4+K3NfixYtNYGCg2b17t33ZTz/9ZCSZtLQ0Y8zp4xoREWGysrLsbbp162YSExPzHcfJkyfbH0syt912m8P+WrdubUaOHGmMMebll1825cuXN8eOHbOvX7hwoQkICDB79+41xvz7/Tp16pS9zfXXX2/69+9vjDFm165dJjAw0OzZs8dhP1dccYUZN26cMcaY2bNnG0nmt99+s6+fMWOGiY2NtT8+s4/lueOOO0ynTp2MzWYr9PgBQGnFGScAuMBNnTpVc+fOtZ+1OBcNGzZUQMC/vxJiY2PVqFEj++PAwEBVrFhR+/fvd3hemzZt7F8HBQWpZcuW9jo2bdqk5cuXq2zZsvZ/9erVk3T6bE2eFi1aFFlbVlaW/vzzT7Vr185hebt27dx6zcYYl9r9/PPPSkhIUEJCgn1ZgwYNFB0d7bC/xMRElStXzv44NjZWDRo0yHccizpmeY/ztvvzzz+rSZMmDmfE2rVrJ5vNpu3bt9uXNWzYUIGBgfbH8fHx9v38+OOPslqtuuiiixyO/ddff+1w3CMiIlSrVq0Ct1GYYcOGaePGjapbt67uvPNOLV68uMj2AFCaBHm7AADA+bn00kvVrVs3jRs3TsOGDXNYFxAQkC8wnHkvTJ7g4GCHxxaLpcBlNpvN5bqOHTum3r17a+rUqfnWxcfH278u6LK54lCnTh1ZLBZt27bNI9srjmN2PvvO28+xY8cUGBiodevWOYQrSSpbtmyR23AWLps3b6709HR98cUX+uqrr9SvXz917tw5331aAFAaccYJAEqBKVOm6LPPPtPq1asdlleuXFl79+51+EDsybmXzhxQ4dSpU1q3bp3q168v6fSH7J9++kmJiYmqXbu2wz93wlJkZKSqVKmiVatWOSxftWqVGjRo4PJ2KlSooG7dumnGjBk6fvx4vvWHDx+WJNWvX19//PGH/vjjD/u6rVu36vDhw27trzBnHrO8x3nHrH79+tq0aZNDfatWrVJAQIDq1q3r0vabNWsmq9Wq/fv35zvucXFxLtcZEhIiq9Wab3lkZKT69++vV155RfPnz9eHH36ozMxMl7cLABcqghMAlAKNGjXSoEGD9Pzzzzssv+yyy3TgwAE99dRT2rFjh2bMmKEvvvjCY/udMWOGPv74Y23btk2jRo3SoUOHdNNNN0mSRo0apczMTA0cOFBr167Vjh079OWXX2r48OEFfiAvyn333aepU6dq/vz52r59ux588EFt3LhRd911l9v1Wq1WtWrVSh9++KF+/fVX/fzzz3r++eftl9B17tzZfjzXr1+vtLQ0DRkyRB07dlTLli3d2l9B3n//fb3++uv65ZdflJKSorS0NPvgD4MGDVJYWJiGDh2qLVu2aPny5brjjjt04403KjY21qXtX3TRRRo0aJCGDBmijz76SOnp6UpLS9PkyZO1cOFCl+tMTEzU5s2btX37dh08eFC5ubmaNm2a3n33XW3btk2//PKL3n//fcXFxdkH1gCA0ozgBAClxMSJE/NdFla/fn3NnDlTM2bMUJMmTZSWllbgiHPnasqUKZoyZYqaNGmib7/9VgsWLFClSpUkyX6WyGq1qmvXrmrUqJHuvvtuRUdHO9wH5Io777xTY8aM0dixY9WoUSOlpqZqwYIFqlOnjlvbqVmzptavX6/LL79cY8eOVXJysrp06aKlS5fqxRdflHT6krVPP/1U5cuX16WXXqrOnTurZs2amj9/vlv7KsyECRM0b948NW7cWG+88Ybeffdd+5msiIgIffnll8rMzNTFF1+s6667TldccYVeeOEFt/Yxe/ZsDRkyRGPHjlXdunXVt29frV27VtWrV3d5GyNGjFDdunXVsmVLVa5cWatWrVK5cuX01FNPqWXLlrr44ou1c+dOLVq0yO3vJwBciCzG1btlAQDAebFYLPr444/Vt29fb5cCAHATfyICAAAAACcITgAAAADgBMORAwBQQrg6HgAuXJxxAgAAAAAnCE4AAAAA4ATBCQAAAACcIDgBAAAAgBMEJwAAAABwguAEAAAAAE4QnAAAAADACYITAAAAADjx/42bkwzfubb5AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components to explain 95% of variance: 86\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialiser PCA\n",
    "pca = PCA()\n",
    "\n",
    "# Ajuster PCA sur les données d'entraînement\n",
    "pca.fit(X_train_rf)\n",
    "\n",
    "# Calculer la variance expliquée cumulée\n",
    "explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Tracer la variance expliquée cumulée\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(\n",
    "    range(1, len(explained_variance) + 1),\n",
    "    explained_variance,\n",
    "    marker=\"o\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"Cumulative Explained Variance vs. Number of Components\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Choisir le nombre de composants pour expliquer 95% de la variance\n",
    "n_components = np.argmax(explained_variance >= 0.95) + 1\n",
    "print(f\"Number of components to explain 95% of variance: {n_components}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Concaténer toutes les caractéristiques d'entrée et les prédictions des réservoirs de neurones\n",
    "def prepare_data_for_rf(X_train_final, y_train):\n",
    "    X_train_concat = np.concatenate(\n",
    "        [\n",
    "            X_train_final[\"X_genres\"].cpu().numpy(),\n",
    "            X_train_final[\"X_instruments\"].cpu().numpy(),\n",
    "            X_train_final[\"X_moods\"].cpu().numpy(),\n",
    "            X_train_final[\"X_genres_categories\"].cpu().numpy(),\n",
    "            X_train_final[\"X_instruments_categories\"].cpu().numpy(),\n",
    "            X_train_final[\"X_moods_categories\"].cpu().numpy(),\n",
    "            X_train_final[\"y_genres_pred\"].cpu().numpy(),\n",
    "            X_train_final[\"y_instruments_pred\"].cpu().numpy(),\n",
    "            X_train_final[\"y_moods_pred\"].cpu().numpy(),\n",
    "            X_train_final[\"y_genres_instruments_pred\"].cpu().numpy(),\n",
    "            X_train_final[\"y_genres_moods_pred\"].cpu().numpy(),\n",
    "            X_train_final[\"y_instruments_moods_pred\"].cpu().numpy(),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    # Normaliser les données\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_concat)\n",
    "    \n",
    "    return X_train_scaled, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Random Forest: 100%|██████████| 50/50 [08:00<00:00,  9.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.0076, Validation F1 Score: 0.3418\n",
      "Test Accuracy: 0.0074, Test F1 Score: 0.3397\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Préparer les données pour le Random Forest\n",
    "X_train_rf, y_train_rf = prepare_data_for_rf(\n",
    "    X_train_final_train, train_targets.cpu().numpy()\n",
    ")\n",
    "X_val_rf, y_val_rf = prepare_data_for_rf(X_train_final_val, val_targets.cpu().numpy())\n",
    "X_test_rf, y_test_rf = prepare_data_for_rf(X_test_final, y_test_tensor.cpu().numpy())\n",
    "\n",
    "# Réduire la dimensionnalité des données avec PCA\n",
    "pca = PCA(n_components=100)  # Ajustez le nombre de composants selon vos besoins\n",
    "X_train_pca = pca.fit_transform(X_train_rf)\n",
    "X_val_pca = pca.transform(X_val_rf)\n",
    "X_test_pca = pca.transform(X_test_rf)\n",
    "\n",
    "# Initialiser le Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=50, random_state=42, warm_start=True)\n",
    "\n",
    "# Entraîner le modèle avec une barre de progression\n",
    "for i in tqdm(range(rf_model.n_estimators), desc=\"Training Random Forest\"):\n",
    "    rf_model.n_estimators = i + 1\n",
    "    rf_model.fit(X_train_pca, y_train_rf)\n",
    "\n",
    "# Évaluer le modèle sur l'ensemble de validation\n",
    "y_val_pred_rf = rf_model.predict(X_val_pca)\n",
    "val_accuracy = accuracy_score(y_val_rf, y_val_pred_rf)\n",
    "val_f1 = f1_score(y_val_rf, y_val_pred_rf, average='weighted')\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# Évaluer le modèle sur l'ensemble de test\n",
    "y_test_pred_rf = rf_model.predict(X_test_pca)\n",
    "test_accuracy = accuracy_score(y_test_rf, y_test_pred_rf)\n",
    "test_f1 = f1_score(y_test_rf, y_test_pred_rf, average='weighted')\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.0071, Validation F1 Score: 0.3313\n",
      "Test Accuracy: 0.0052, Test F1 Score: 0.3294\n"
     ]
    }
   ],
   "source": [
    "# Évaluer le modèle sur l'ensemble de validation\n",
    "y_val_pred_rf = rf_model.predict(X_val_pca)\n",
    "val_accuracy = accuracy_score(y_val_rf, y_val_pred_rf)\n",
    "val_f1 = f1_score(y_val_rf, y_val_pred_rf, average='weighted')\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# Évaluer le modèle sur l'ensemble de test\n",
    "y_test_pred_rf = rf_model.predict(X_test_pca)\n",
    "test_accuracy = accuracy_score(y_test_rf, y_test_pred_rf)\n",
    "test_f1 = f1_score(y_test_rf, y_test_pred_rf, average='weighted')\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de composants principaux initialement: 1033\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Supposons que X_train_rf soit votre matrice de données d'origine\n",
    "# X_train_rf est un tableau NumPy ou une structure similaire\n",
    "\n",
    "# Initialiser PCA\n",
    "pca = PCA()\n",
    "\n",
    "# Ajuster PCA sur les données d'entraînement\n",
    "pca.fit(X_train_rf)\n",
    "\n",
    "# Obtenir le nombre de composants principaux\n",
    "n_components_total = pca.n_components_\n",
    "\n",
    "print(f\"Nombre total de composants principaux initialement: {n_components_total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 569, number of negative: 70377\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013160 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.008020 -> initscore=-4.817741\n",
      "[LightGBM] [Info] Start training from score -4.817741\n",
      "[LightGBM] [Info] Number of positive: 3983, number of negative: 66963\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.018031 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.056141 -> initscore=-2.822105\n",
      "[LightGBM] [Info] Start training from score -2.822105\n",
      "[LightGBM] [Info] Number of positive: 812, number of negative: 70134\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014127 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011445 -> initscore=-4.458663\n",
      "[LightGBM] [Info] Start training from score -4.458663\n",
      "[LightGBM] [Info] Number of positive: 255, number of negative: 70691\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.018146 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003594 -> initscore=-5.624810\n",
      "[LightGBM] [Info] Start training from score -5.624810\n",
      "[LightGBM] [Info] Number of positive: 200, number of negative: 70746\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014919 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002819 -> initscore=-5.868534\n",
      "[LightGBM] [Info] Start training from score -5.868534\n",
      "[LightGBM] [Info] Number of positive: 938, number of negative: 70008\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014070 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.013221 -> initscore=-4.312615\n",
      "[LightGBM] [Info] Start training from score -4.312615\n",
      "[LightGBM] [Info] Number of positive: 402, number of negative: 70544\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014095 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005666 -> initscore=-5.167540\n",
      "[LightGBM] [Info] Start training from score -5.167540\n",
      "[LightGBM] [Info] Number of positive: 1606, number of negative: 69340\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014228 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.022637 -> initscore=-3.765275\n",
      "[LightGBM] [Info] Start training from score -3.765275\n",
      "[LightGBM] [Info] Number of positive: 3237, number of negative: 67709\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015220 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.045626 -> initscore=-3.040572\n",
      "[LightGBM] [Info] Start training from score -3.040572\n",
      "[LightGBM] [Info] Number of positive: 714, number of negative: 70232\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013514 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010064 -> initscore=-4.588676\n",
      "[LightGBM] [Info] Start training from score -4.588676\n",
      "[LightGBM] [Info] Number of positive: 2392, number of negative: 68554\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014653 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.033716 -> initscore=-3.355492\n",
      "[LightGBM] [Info] Start training from score -3.355492\n",
      "[LightGBM] [Info] Number of positive: 6648, number of negative: 64298\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014430 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.093705 -> initscore=-2.269212\n",
      "[LightGBM] [Info] Start training from score -2.269212\n",
      "[LightGBM] [Info] Number of positive: 1447, number of negative: 69499\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015292 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020396 -> initscore=-3.871820\n",
      "[LightGBM] [Info] Start training from score -3.871820\n",
      "[LightGBM] [Info] Number of positive: 4505, number of negative: 66441\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.025374 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063499 -> initscore=-2.691126\n",
      "[LightGBM] [Info] Start training from score -2.691126\n",
      "[LightGBM] [Info] Number of positive: 2925, number of negative: 68021\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014257 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.041229 -> initscore=-3.146522\n",
      "[LightGBM] [Info] Start training from score -3.146522\n",
      "[LightGBM] [Info] Number of positive: 1165, number of negative: 69781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.019315 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.016421 -> initscore=-4.092641\n",
      "[LightGBM] [Info] Start training from score -4.092641\n",
      "[LightGBM] [Info] Number of positive: 221, number of negative: 70725\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013057 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003115 -> initscore=-5.768392\n",
      "[LightGBM] [Info] Start training from score -5.768392\n",
      "[LightGBM] [Info] Number of positive: 460, number of negative: 70486\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012726 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006484 -> initscore=-5.031943\n",
      "[LightGBM] [Info] Start training from score -5.031943\n",
      "[LightGBM] [Info] Number of positive: 481, number of negative: 70465\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012771 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006780 -> initscore=-4.987004\n",
      "[LightGBM] [Info] Start training from score -4.987004\n",
      "[LightGBM] [Info] Number of positive: 764, number of negative: 70182\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013087 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010769 -> initscore=-4.520279\n",
      "[LightGBM] [Info] Start training from score -4.520279\n",
      "[LightGBM] [Info] Number of positive: 2989, number of negative: 67957\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013002 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.042131 -> initscore=-3.123936\n",
      "[LightGBM] [Info] Start training from score -3.123936\n",
      "[LightGBM] [Info] Number of positive: 510, number of negative: 70436\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014973 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007189 -> initscore=-4.928049\n",
      "[LightGBM] [Info] Start training from score -4.928049\n",
      "[LightGBM] [Info] Number of positive: 430, number of negative: 70516\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014991 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006061 -> initscore=-5.099810\n",
      "[LightGBM] [Info] Start training from score -5.099810\n",
      "[LightGBM] [Info] Number of positive: 3149, number of negative: 67797\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016641 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.044386 -> initscore=-3.069433\n",
      "[LightGBM] [Info] Start training from score -3.069433\n",
      "[LightGBM] [Info] Number of positive: 2713, number of negative: 68233\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015421 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038240 -> initscore=-3.224873\n",
      "[LightGBM] [Info] Start training from score -3.224873\n",
      "[LightGBM] [Info] Number of positive: 261, number of negative: 70685\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014984 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003679 -> initscore=-5.601468\n",
      "[LightGBM] [Info] Start training from score -5.601468\n",
      "[LightGBM] [Info] Number of positive: 1189, number of negative: 69757\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013692 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.016759 -> initscore=-4.071905\n",
      "[LightGBM] [Info] Start training from score -4.071905\n",
      "[LightGBM] [Info] Number of positive: 10840, number of negative: 60106\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013024 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.152792 -> initscore=-1.712867\n",
      "[LightGBM] [Info] Start training from score -1.712867\n",
      "[LightGBM] [Info] Number of positive: 2825, number of negative: 68121\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012448 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039819 -> initscore=-3.182777\n",
      "[LightGBM] [Info] Start training from score -3.182777\n",
      "[LightGBM] [Info] Number of positive: 1253, number of negative: 69693\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012816 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.017661 -> initscore=-4.018559\n",
      "[LightGBM] [Info] Start training from score -4.018559\n",
      "[LightGBM] [Info] Number of positive: 458, number of negative: 70488\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013564 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006456 -> initscore=-5.036329\n",
      "[LightGBM] [Info] Start training from score -5.036329\n",
      "[LightGBM] [Info] Number of positive: 731, number of negative: 70215\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015161 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010304 -> initscore=-4.564904\n",
      "[LightGBM] [Info] Start training from score -4.564904\n",
      "[LightGBM] [Info] Number of positive: 273, number of negative: 70673\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013679 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003848 -> initscore=-5.556347\n",
      "[LightGBM] [Info] Start training from score -5.556347\n",
      "[LightGBM] [Info] Number of positive: 1936, number of negative: 69010\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.028733 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.027288 -> initscore=-3.573627\n",
      "[LightGBM] [Info] Start training from score -3.573627\n",
      "[LightGBM] [Info] Number of positive: 1087, number of negative: 69859\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012602 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.015322 -> initscore=-4.163057\n",
      "[LightGBM] [Info] Start training from score -4.163057\n",
      "[LightGBM] [Info] Number of positive: 2555, number of negative: 68391\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012728 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036013 -> initscore=-3.287189\n",
      "[LightGBM] [Info] Start training from score -3.287189\n",
      "[LightGBM] [Info] Number of positive: 1928, number of negative: 69018\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014243 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.027176 -> initscore=-3.577884\n",
      "[LightGBM] [Info] Start training from score -3.577884\n",
      "[LightGBM] [Info] Number of positive: 1237, number of negative: 69709\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014938 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.017436 -> initscore=-4.031640\n",
      "[LightGBM] [Info] Start training from score -4.031640\n",
      "[LightGBM] [Info] Number of positive: 1764, number of negative: 69182\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014391 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.024864 -> initscore=-3.669157\n",
      "[LightGBM] [Info] Start training from score -3.669157\n",
      "[LightGBM] [Info] Number of positive: 4949, number of negative: 65997\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014290 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.069757 -> initscore=-2.590424\n",
      "[LightGBM] [Info] Start training from score -2.590424\n",
      "[LightGBM] [Info] Number of positive: 302, number of negative: 70644\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013876 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004257 -> initscore=-5.454981\n",
      "[LightGBM] [Info] Start training from score -5.454981\n",
      "[LightGBM] [Info] Number of positive: 717, number of negative: 70229\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014532 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010106 -> initscore=-4.584441\n",
      "[LightGBM] [Info] Start training from score -4.584441\n",
      "[LightGBM] [Info] Number of positive: 1018, number of negative: 69928\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014706 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014349 -> initscore=-4.229626\n",
      "[LightGBM] [Info] Start training from score -4.229626\n",
      "[LightGBM] [Info] Number of positive: 342, number of negative: 70604\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014728 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004821 -> initscore=-5.330031\n",
      "[LightGBM] [Info] Start training from score -5.330031\n",
      "[LightGBM] [Info] Number of positive: 580, number of negative: 70366\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013029 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.008175 -> initscore=-4.798437\n",
      "[LightGBM] [Info] Start training from score -4.798437\n",
      "[LightGBM] [Info] Number of positive: 2097, number of negative: 68849\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013286 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.029558 -> initscore=-3.491408\n",
      "[LightGBM] [Info] Start training from score -3.491408\n",
      "[LightGBM] [Info] Number of positive: 191, number of negative: 70755\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013229 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002692 -> initscore=-5.914705\n",
      "[LightGBM] [Info] Start training from score -5.914705\n",
      "[LightGBM] [Info] Number of positive: 175, number of negative: 70771\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013261 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002467 -> initscore=-6.002419\n",
      "[LightGBM] [Info] Start training from score -6.002419\n",
      "[LightGBM] [Info] Number of positive: 373, number of negative: 70573\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013909 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005258 -> initscore=-5.242824\n",
      "[LightGBM] [Info] Start training from score -5.242824\n",
      "[LightGBM] [Info] Number of positive: 196, number of negative: 70750\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.022657 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002763 -> initscore=-5.888793\n",
      "[LightGBM] [Info] Start training from score -5.888793\n",
      "[LightGBM] [Info] Number of positive: 188, number of negative: 70758\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013156 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002650 -> initscore=-5.930579\n",
      "[LightGBM] [Info] Start training from score -5.930579\n",
      "[LightGBM] [Info] Number of positive: 267, number of negative: 70679\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.029202 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003763 -> initscore=-5.578655\n",
      "[LightGBM] [Info] Start training from score -5.578655\n",
      "[LightGBM] [Info] Number of positive: 130, number of negative: 70816\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.017238 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001832 -> initscore=-6.300306\n",
      "[LightGBM] [Info] Start training from score -6.300306\n",
      "[LightGBM] [Info] Number of positive: 785, number of negative: 70161\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.029502 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011065 -> initscore=-4.492864\n",
      "[LightGBM] [Info] Start training from score -4.492864\n",
      "[LightGBM] [Info] Number of positive: 1107, number of negative: 69839\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014025 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.015603 -> initscore=-4.144539\n",
      "[LightGBM] [Info] Start training from score -4.144539\n",
      "[LightGBM] [Info] Number of positive: 1218, number of negative: 69728\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012930 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.017168 -> initscore=-4.047392\n",
      "[LightGBM] [Info] Start training from score -4.047392\n",
      "[LightGBM] [Info] Number of positive: 194, number of negative: 70752\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013907 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002734 -> initscore=-5.899078\n",
      "[LightGBM] [Info] Start training from score -5.899078\n",
      "[LightGBM] [Info] Number of positive: 393, number of negative: 70553\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013111 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005539 -> initscore=-5.190310\n",
      "[LightGBM] [Info] Start training from score -5.190310\n",
      "[LightGBM] [Info] Number of positive: 359, number of negative: 70587\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015380 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005060 -> initscore=-5.281279\n",
      "[LightGBM] [Info] Start training from score -5.281279\n",
      "[LightGBM] [Info] Number of positive: 546, number of negative: 70400\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015057 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007696 -> initscore=-4.859330\n",
      "[LightGBM] [Info] Start training from score -4.859330\n",
      "[LightGBM] [Info] Number of positive: 265, number of negative: 70681\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016146 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003735 -> initscore=-5.586202\n",
      "[LightGBM] [Info] Start training from score -5.586202\n",
      "[LightGBM] [Info] Number of positive: 1217, number of negative: 69729\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.029715 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.017154 -> initscore=-4.048227\n",
      "[LightGBM] [Info] Start training from score -4.048227\n",
      "[LightGBM] [Info] Number of positive: 113, number of negative: 70833\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013837 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001593 -> initscore=-6.440692\n",
      "[LightGBM] [Info] Start training from score -6.440692\n",
      "[LightGBM] [Info] Number of positive: 782, number of negative: 70164\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014203 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011022 -> initscore=-4.496736\n",
      "[LightGBM] [Info] Start training from score -4.496736\n",
      "[LightGBM] [Info] Number of positive: 398, number of negative: 70548\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015116 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005610 -> initscore=-5.177597\n",
      "[LightGBM] [Info] Start training from score -5.177597\n",
      "[LightGBM] [Info] Number of positive: 452, number of negative: 70494\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012572 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006371 -> initscore=-5.049601\n",
      "[LightGBM] [Info] Start training from score -5.049601\n",
      "[LightGBM] [Info] Number of positive: 1510, number of negative: 69436\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014132 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.021284 -> initscore=-3.828296\n",
      "[LightGBM] [Info] Start training from score -3.828296\n",
      "[LightGBM] [Info] Number of positive: 563, number of negative: 70383\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.018480 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007936 -> initscore=-4.828427\n",
      "[LightGBM] [Info] Start training from score -4.828427\n",
      "[LightGBM] [Info] Number of positive: 413, number of negative: 70533\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016320 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005821 -> initscore=-5.140388\n",
      "[LightGBM] [Info] Start training from score -5.140388\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 70349\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014482 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.008415 -> initscore=-4.769307\n",
      "[LightGBM] [Info] Start training from score -4.769307\n",
      "[LightGBM] [Info] Number of positive: 1503, number of negative: 69443\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013065 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.021185 -> initscore=-3.833043\n",
      "[LightGBM] [Info] Start training from score -3.833043\n",
      "[LightGBM] [Info] Number of positive: 80, number of negative: 70866\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013202 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001128 -> initscore=-6.786519\n",
      "[LightGBM] [Info] Start training from score -6.786519\n",
      "[LightGBM] [Info] Number of positive: 116, number of negative: 70830\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013770 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001635 -> initscore=-6.414448\n",
      "[LightGBM] [Info] Start training from score -6.414448\n",
      "[LightGBM] [Info] Number of positive: 599, number of negative: 70347\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013027 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.008443 -> initscore=-4.765934\n",
      "[LightGBM] [Info] Start training from score -4.765934\n",
      "[LightGBM] [Info] Number of positive: 312, number of negative: 70634\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013908 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004398 -> initscore=-5.422264\n",
      "[LightGBM] [Info] Start training from score -5.422264\n",
      "[LightGBM] [Info] Number of positive: 223, number of negative: 70723\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014311 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003143 -> initscore=-5.759354\n",
      "[LightGBM] [Info] Start training from score -5.759354\n",
      "[LightGBM] [Info] Number of positive: 276, number of negative: 70670\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015129 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003890 -> initscore=-5.545376\n",
      "[LightGBM] [Info] Start training from score -5.545376\n",
      "[LightGBM] [Info] Number of positive: 1169, number of negative: 69777\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015041 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.016477 -> initscore=-4.089156\n",
      "[LightGBM] [Info] Start training from score -4.089156\n",
      "[LightGBM] [Info] Number of positive: 372, number of negative: 70574\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.019151 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005243 -> initscore=-5.245523\n",
      "[LightGBM] [Info] Start training from score -5.245523\n",
      "[LightGBM] [Info] Number of positive: 285, number of negative: 70661\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015034 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004017 -> initscore=-5.513160\n",
      "[LightGBM] [Info] Start training from score -5.513160\n",
      "[LightGBM] [Info] Number of positive: 1240, number of negative: 69706\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012689 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.017478 -> initscore=-4.029175\n",
      "[LightGBM] [Info] Start training from score -4.029175\n",
      "[LightGBM] [Info] Number of positive: 4730, number of negative: 66216\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013760 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.066670 -> initscore=-2.638997\n",
      "[LightGBM] [Info] Start training from score -2.638997\n",
      "[LightGBM] [Info] Number of positive: 1036, number of negative: 69910\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012824 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.014603 -> initscore=-4.211842\n",
      "[LightGBM] [Info] Start training from score -4.211842\n",
      "[LightGBM] [Info] Number of positive: 247, number of negative: 70699\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014597 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003482 -> initscore=-5.656798\n",
      "[LightGBM] [Info] Start training from score -5.656798\n",
      "[LightGBM] [Info] Number of positive: 1172, number of negative: 69774\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013580 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.016520 -> initscore=-4.086550\n",
      "[LightGBM] [Info] Start training from score -4.086550\n",
      "[LightGBM] [Info] Number of positive: 345, number of negative: 70601\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014165 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004863 -> initscore=-5.321255\n",
      "[LightGBM] [Info] Start training from score -5.321255\n",
      "[LightGBM] [Info] Number of positive: 2243, number of negative: 68703\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014099 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031616 -> initscore=-3.421979\n",
      "[LightGBM] [Info] Start training from score -3.421979\n",
      "[LightGBM] [Info] Number of positive: 3114, number of negative: 67832\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013929 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.043893 -> initscore=-3.081126\n",
      "[LightGBM] [Info] Start training from score -3.081126\n",
      "[LightGBM] [Info] Number of positive: 132, number of negative: 70814\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013849 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001861 -> initscore=-6.285010\n",
      "[LightGBM] [Info] Start training from score -6.285010\n",
      "[LightGBM] [Info] Number of positive: 1776, number of negative: 69170\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012650 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.025033 -> initscore=-3.662204\n",
      "[LightGBM] [Info] Start training from score -3.662204\n",
      "[LightGBM] [Info] Number of positive: 327, number of negative: 70619\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014492 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004609 -> initscore=-5.375094\n",
      "[LightGBM] [Info] Start training from score -5.375094\n",
      "[LightGBM] [Info] Number of positive: 1724, number of negative: 69222\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014653 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.024300 -> initscore=-3.692672\n",
      "[LightGBM] [Info] Start training from score -3.692672\n",
      "[LightGBM] [Info] Number of positive: 10851, number of negative: 60095\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014471 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.152947 -> initscore=-1.711669\n",
      "[LightGBM] [Info] Start training from score -1.711669\n",
      "[LightGBM] [Info] Number of positive: 1599, number of negative: 69347\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014324 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.022538 -> initscore=-3.769744\n",
      "[LightGBM] [Info] Start training from score -3.769744\n",
      "[LightGBM] [Info] Number of positive: 821, number of negative: 70125\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012903 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011572 -> initscore=-4.447512\n",
      "[LightGBM] [Info] Start training from score -4.447512\n",
      "[LightGBM] [Info] Number of positive: 15828, number of negative: 55118\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.023966 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.223099 -> initscore=-1.247696\n",
      "[LightGBM] [Info] Start training from score -1.247696\n",
      "[LightGBM] [Info] Number of positive: 359, number of negative: 70587\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012879 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005060 -> initscore=-5.281279\n",
      "[LightGBM] [Info] Start training from score -5.281279\n",
      "[LightGBM] [Info] Number of positive: 663, number of negative: 70283\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.026140 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.009345 -> initscore=-4.663510\n",
      "[LightGBM] [Info] Start training from score -4.663510\n",
      "[LightGBM] [Info] Number of positive: 299, number of negative: 70647\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014391 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004214 -> initscore=-5.465007\n",
      "[LightGBM] [Info] Start training from score -5.465007\n",
      "[LightGBM] [Info] Number of positive: 863, number of negative: 70083\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013499 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.012164 -> initscore=-4.397021\n",
      "[LightGBM] [Info] Start training from score -4.397021\n",
      "[LightGBM] [Info] Number of positive: 9132, number of negative: 61814\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014982 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.128718 -> initscore=-1.912345\n",
      "[LightGBM] [Info] Start training from score -1.912345\n",
      "[LightGBM] [Info] Number of positive: 1286, number of negative: 69660\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016109 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.018126 -> initscore=-3.992090\n",
      "[LightGBM] [Info] Start training from score -3.992090\n",
      "[LightGBM] [Info] Number of positive: 3730, number of negative: 67216\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013835 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.052575 -> initscore=-2.891503\n",
      "[LightGBM] [Info] Start training from score -2.891503\n",
      "[LightGBM] [Info] Number of positive: 94, number of negative: 70852\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.020070 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001325 -> initscore=-6.625054\n",
      "[LightGBM] [Info] Start training from score -6.625054\n",
      "[LightGBM] [Info] Number of positive: 56, number of negative: 70890\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.022521 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000789 -> initscore=-7.143533\n",
      "[LightGBM] [Info] Start training from score -7.143533\n",
      "[LightGBM] [Info] Number of positive: 364, number of negative: 70582\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013285 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005131 -> initscore=-5.267377\n",
      "[LightGBM] [Info] Start training from score -5.267377\n",
      "[LightGBM] [Info] Number of positive: 1692, number of negative: 69254\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013278 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.023849 -> initscore=-3.711870\n",
      "[LightGBM] [Info] Start training from score -3.711870\n",
      "[LightGBM] [Info] Number of positive: 1590, number of negative: 69356\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013019 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.022411 -> initscore=-3.775519\n",
      "[LightGBM] [Info] Start training from score -3.775519\n",
      "[LightGBM] [Info] Number of positive: 428, number of negative: 70518\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012807 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006033 -> initscore=-5.104500\n",
      "[LightGBM] [Info] Start training from score -5.104500\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 70148\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014309 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011248 -> initscore=-4.476254\n",
      "[LightGBM] [Info] Start training from score -4.476254\n",
      "[LightGBM] [Info] Number of positive: 266, number of negative: 70680\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013350 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003749 -> initscore=-5.582422\n",
      "[LightGBM] [Info] Start training from score -5.582422\n",
      "[LightGBM] [Info] Number of positive: 913, number of negative: 70033\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013890 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.012869 -> initscore=-4.339986\n",
      "[LightGBM] [Info] Start training from score -4.339986\n",
      "[LightGBM] [Info] Number of positive: 218, number of negative: 70728\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013898 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003073 -> initscore=-5.782102\n",
      "[LightGBM] [Info] Start training from score -5.782102\n",
      "[LightGBM] [Info] Number of positive: 3594, number of negative: 67352\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013298 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.050658 -> initscore=-2.930667\n",
      "[LightGBM] [Info] Start training from score -2.930667\n",
      "[LightGBM] [Info] Number of positive: 315, number of negative: 70631\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014162 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004440 -> initscore=-5.412652\n",
      "[LightGBM] [Info] Start training from score -5.412652\n",
      "[LightGBM] [Info] Number of positive: 16047, number of negative: 54899\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015435 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.226186 -> initscore=-1.229973\n",
      "[LightGBM] [Info] Start training from score -1.229973\n",
      "[LightGBM] [Info] Number of positive: 14078, number of negative: 56868\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014936 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.198433 -> initscore=-1.396119\n",
      "[LightGBM] [Info] Start training from score -1.396119\n",
      "[LightGBM] [Info] Number of positive: 374, number of negative: 70572\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013410 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005272 -> initscore=-5.240133\n",
      "[LightGBM] [Info] Start training from score -5.240133\n",
      "[LightGBM] [Info] Number of positive: 20185, number of negative: 50761\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015480 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.284512 -> initscore=-0.922189\n",
      "[LightGBM] [Info] Start training from score -0.922189\n",
      "[LightGBM] [Info] Number of positive: 785, number of negative: 70161\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013511 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011065 -> initscore=-4.492864\n",
      "[LightGBM] [Info] Start training from score -4.492864\n",
      "[LightGBM] [Info] Number of positive: 612, number of negative: 70334\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012431 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.008626 -> initscore=-4.744278\n",
      "[LightGBM] [Info] Start training from score -4.744278\n",
      "[LightGBM] [Info] Number of positive: 501, number of negative: 70445\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.018551 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007062 -> initscore=-4.945981\n",
      "[LightGBM] [Info] Start training from score -4.945981\n",
      "[LightGBM] [Info] Number of positive: 4673, number of negative: 66273\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014892 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.065867 -> initscore=-2.651981\n",
      "[LightGBM] [Info] Start training from score -2.651981\n",
      "[LightGBM] [Info] Number of positive: 1391, number of negative: 69555\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014680 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.019606 -> initscore=-3.912095\n",
      "[LightGBM] [Info] Start training from score -3.912095\n",
      "[LightGBM] [Info] Number of positive: 237, number of negative: 70709\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015006 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003341 -> initscore=-5.698268\n",
      "[LightGBM] [Info] Start training from score -5.698268\n",
      "[LightGBM] [Info] Number of positive: 3645, number of negative: 67301\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013320 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.051377 -> initscore=-2.915819\n",
      "[LightGBM] [Info] Start training from score -2.915819\n",
      "[LightGBM] [Info] Number of positive: 1911, number of negative: 69035\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014723 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026936 -> initscore=-3.586987\n",
      "[LightGBM] [Info] Start training from score -3.586987\n",
      "[LightGBM] [Info] Number of positive: 633, number of negative: 70313\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014496 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.008922 -> initscore=-4.710242\n",
      "[LightGBM] [Info] Start training from score -4.710242\n",
      "[LightGBM] [Info] Number of positive: 1903, number of negative: 69043\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.021832 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026823 -> initscore=-3.591298\n",
      "[LightGBM] [Info] Start training from score -3.591298\n",
      "[LightGBM] [Info] Number of positive: 1961, number of negative: 68985\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013106 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.027641 -> initscore=-3.560435\n",
      "[LightGBM] [Info] Start training from score -3.560435\n",
      "[LightGBM] [Info] Number of positive: 1795, number of negative: 69151\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.022186 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.025301 -> initscore=-3.651287\n",
      "[LightGBM] [Info] Start training from score -3.651287\n",
      "[LightGBM] [Info] Number of positive: 701, number of negative: 70245\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013306 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.009881 -> initscore=-4.607237\n",
      "[LightGBM] [Info] Start training from score -4.607237\n",
      "[LightGBM] [Info] Number of positive: 1908, number of negative: 69038\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.019714 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.026894 -> initscore=-3.588602\n",
      "[LightGBM] [Info] Start training from score -3.588602\n",
      "[LightGBM] [Info] Number of positive: 562, number of negative: 70384\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014936 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007922 -> initscore=-4.830219\n",
      "[LightGBM] [Info] Start training from score -4.830219\n",
      "[LightGBM] [Info] Number of positive: 228, number of negative: 70718\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013559 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003214 -> initscore=-5.737110\n",
      "[LightGBM] [Info] Start training from score -5.737110\n",
      "[LightGBM] [Info] Number of positive: 2140, number of negative: 68806\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014535 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.030164 -> initscore=-3.470485\n",
      "[LightGBM] [Info] Start training from score -3.470485\n",
      "[LightGBM] [Info] Number of positive: 316, number of negative: 70630\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012841 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004454 -> initscore=-5.409468\n",
      "[LightGBM] [Info] Start training from score -5.409468\n",
      "[LightGBM] [Info] Number of positive: 408, number of negative: 70538\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012549 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005751 -> initscore=-5.152640\n",
      "[LightGBM] [Info] Start training from score -5.152640\n",
      "[LightGBM] [Info] Number of positive: 2343, number of negative: 68603\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014495 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.033025 -> initscore=-3.376904\n",
      "[LightGBM] [Info] Start training from score -3.376904\n",
      "[LightGBM] [Info] Number of positive: 498, number of negative: 70448\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013899 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007019 -> initscore=-4.952030\n",
      "[LightGBM] [Info] Start training from score -4.952030\n",
      "[LightGBM] [Info] Number of positive: 218, number of negative: 70728\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.020687 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003073 -> initscore=-5.782102\n",
      "[LightGBM] [Info] Start training from score -5.782102\n",
      "[LightGBM] [Info] Number of positive: 6148, number of negative: 64798\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.019992 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.086657 -> initscore=-2.355148\n",
      "[LightGBM] [Info] Start training from score -2.355148\n",
      "[LightGBM] [Info] Number of positive: 830, number of negative: 70116\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013621 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011699 -> initscore=-4.436481\n",
      "[LightGBM] [Info] Start training from score -4.436481\n",
      "[LightGBM] [Info] Number of positive: 684, number of negative: 70262\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013356 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.009641 -> initscore=-4.632028\n",
      "[LightGBM] [Info] Start training from score -4.632028\n",
      "[LightGBM] [Info] Number of positive: 1826, number of negative: 69120\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013526 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.025738 -> initscore=-3.633716\n",
      "[LightGBM] [Info] Start training from score -3.633716\n",
      "[LightGBM] [Info] Number of positive: 73, number of negative: 70873\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013159 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001029 -> initscore=-6.878185\n",
      "[LightGBM] [Info] Start training from score -6.878185\n",
      "[LightGBM] [Info] Number of positive: 207, number of negative: 70739\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015945 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002918 -> initscore=-5.834034\n",
      "[LightGBM] [Info] Start training from score -5.834034\n",
      "[LightGBM] [Info] Number of positive: 119, number of negative: 70827\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014271 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001677 -> initscore=-6.388872\n",
      "[LightGBM] [Info] Start training from score -6.388872\n",
      "[LightGBM] [Info] Number of positive: 830, number of negative: 70116\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014543 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011699 -> initscore=-4.436481\n",
      "[LightGBM] [Info] Start training from score -4.436481\n",
      "[LightGBM] [Info] Number of positive: 1098, number of negative: 69848\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013199 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.015477 -> initscore=-4.152831\n",
      "[LightGBM] [Info] Start training from score -4.152831\n",
      "[LightGBM] [Info] Number of positive: 942, number of negative: 70004\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015311 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.013278 -> initscore=-4.308302\n",
      "[LightGBM] [Info] Start training from score -4.308302\n",
      "[LightGBM] [Info] Number of positive: 245, number of negative: 70701\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.024527 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003453 -> initscore=-5.664957\n",
      "[LightGBM] [Info] Start training from score -5.664957\n",
      "[LightGBM] [Info] Number of positive: 178, number of negative: 70768\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014481 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002509 -> initscore=-5.985379\n",
      "[LightGBM] [Info] Start training from score -5.985379\n",
      "[LightGBM] [Info] Number of positive: 419, number of negative: 70527\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012860 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005906 -> initscore=-5.125880\n",
      "[LightGBM] [Info] Start training from score -5.125880\n",
      "[LightGBM] [Info] Number of positive: 1165, number of negative: 69781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013609 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.016421 -> initscore=-4.092641\n",
      "[LightGBM] [Info] Start training from score -4.092641\n",
      "[LightGBM] [Info] Number of positive: 17116, number of negative: 53830\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012227 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.241254 -> initscore=-1.145817\n",
      "[LightGBM] [Info] Start training from score -1.145817\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 70781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013676 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.002326 -> initscore=-6.061400\n",
      "[LightGBM] [Info] Start training from score -6.061400\n",
      "[LightGBM] [Info] Number of positive: 1844, number of negative: 69102\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013394 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.025992 -> initscore=-3.623647\n",
      "[LightGBM] [Info] Start training from score -3.623647\n",
      "[LightGBM] [Info] Number of positive: 5439, number of negative: 65507\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013689 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.076664 -> initscore=-2.488562\n",
      "[LightGBM] [Info] Start training from score -2.488562\n",
      "[LightGBM] [Info] Number of positive: 834, number of negative: 70112\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012642 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011755 -> initscore=-4.431616\n",
      "[LightGBM] [Info] Start training from score -4.431616\n",
      "[LightGBM] [Info] Number of positive: 239, number of negative: 70707\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012855 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003369 -> initscore=-5.689836\n",
      "[LightGBM] [Info] Start training from score -5.689836\n",
      "[LightGBM] [Info] Number of positive: 7125, number of negative: 63821\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013192 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100428 -> initscore=-2.192473\n",
      "[LightGBM] [Info] Start training from score -2.192473\n",
      "[LightGBM] [Info] Number of positive: 3261, number of negative: 67685\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014677 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.045965 -> initscore=-3.032831\n",
      "[LightGBM] [Info] Start training from score -3.032831\n",
      "[LightGBM] [Info] Number of positive: 760, number of negative: 70186\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012733 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010712 -> initscore=-4.525586\n",
      "[LightGBM] [Info] Start training from score -4.525586\n",
      "[LightGBM] [Info] Number of positive: 2836, number of negative: 68110\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014333 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039974 -> initscore=-3.178729\n",
      "[LightGBM] [Info] Start training from score -3.178729\n",
      "[LightGBM] [Info] Number of positive: 342, number of negative: 70604\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013923 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004821 -> initscore=-5.330031\n",
      "[LightGBM] [Info] Start training from score -5.330031\n",
      "[LightGBM] [Info] Number of positive: 467, number of negative: 70479\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012479 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.006582 -> initscore=-5.016741\n",
      "[LightGBM] [Info] Start training from score -5.016741\n",
      "[LightGBM] [Info] Number of positive: 522, number of negative: 70424\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013952 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007358 -> initscore=-4.904622\n",
      "[LightGBM] [Info] Start training from score -4.904622\n",
      "[LightGBM] [Info] Number of positive: 697, number of negative: 70249\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013934 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.009824 -> initscore=-4.613016\n",
      "[LightGBM] [Info] Start training from score -4.613016\n",
      "[LightGBM] [Info] Number of positive: 2950, number of negative: 67996\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013256 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.041581 -> initscore=-3.137644\n",
      "[LightGBM] [Info] Start training from score -3.137644\n",
      "[LightGBM] [Info] Number of positive: 216, number of negative: 70730\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013967 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003045 -> initscore=-5.791347\n",
      "[LightGBM] [Info] Start training from score -5.791347\n",
      "[LightGBM] [Info] Number of positive: 92, number of negative: 70854\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014084 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.001297 -> initscore=-6.646588\n",
      "[LightGBM] [Info] Start training from score -6.646588\n",
      "[LightGBM] [Info] Number of positive: 11921, number of negative: 59025\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013184 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.168029 -> initscore=-1.599660\n",
      "[LightGBM] [Info] Start training from score -1.599660\n",
      "[LightGBM] [Info] Number of positive: 3493, number of negative: 67453\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013313 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.049235 -> initscore=-2.960670\n",
      "[LightGBM] [Info] Start training from score -2.960670\n",
      "[LightGBM] [Info] Number of positive: 4608, number of negative: 66338\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.019485 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.064951 -> initscore=-2.666969\n",
      "[LightGBM] [Info] Start training from score -2.666969\n",
      "[LightGBM] [Info] Number of positive: 4504, number of negative: 66442\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013941 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.063485 -> initscore=-2.691363\n",
      "[LightGBM] [Info] Start training from score -2.691363\n",
      "[LightGBM] [Info] Number of positive: 1315, number of negative: 69631\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013026 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.018535 -> initscore=-3.969373\n",
      "[LightGBM] [Info] Start training from score -3.969373\n",
      "[LightGBM] [Info] Number of positive: 21933, number of negative: 49013\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012755 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.309151 -> initscore=-0.804093\n",
      "[LightGBM] [Info] Start training from score -0.804093\n",
      "[LightGBM] [Info] Number of positive: 225, number of negative: 70721\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013095 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003171 -> initscore=-5.750397\n",
      "[LightGBM] [Info] Start training from score -5.750397\n",
      "[LightGBM] [Info] Number of positive: 334, number of negative: 70612\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015103 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004708 -> initscore=-5.353814\n",
      "[LightGBM] [Info] Start training from score -5.353814\n",
      "[LightGBM] [Info] Number of positive: 1748, number of negative: 69198\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014438 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.024638 -> initscore=-3.678500\n",
      "[LightGBM] [Info] Start training from score -3.678500\n",
      "[LightGBM] [Info] Number of positive: 528, number of negative: 70418\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012886 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007442 -> initscore=-4.893108\n",
      "[LightGBM] [Info] Start training from score -4.893108\n",
      "[LightGBM] [Info] Number of positive: 1304, number of negative: 69642\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012427 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.018380 -> initscore=-3.977931\n",
      "[LightGBM] [Info] Start training from score -3.977931\n",
      "[LightGBM] [Info] Number of positive: 344, number of negative: 70602\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.023543 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004849 -> initscore=-5.324172\n",
      "[LightGBM] [Info] Start training from score -5.324172\n",
      "[LightGBM] [Info] Number of positive: 596, number of negative: 70350\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014631 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.008401 -> initscore=-4.770997\n",
      "[LightGBM] [Info] Start training from score -4.770997\n",
      "[LightGBM] [Info] Number of positive: 2624, number of negative: 68322\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016742 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.036986 -> initscore=-3.259532\n",
      "[LightGBM] [Info] Start training from score -3.259532\n",
      "[LightGBM] [Info] Number of positive: 325, number of negative: 70621\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015903 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004581 -> initscore=-5.381258\n",
      "[LightGBM] [Info] Start training from score -5.381258\n",
      "[LightGBM] [Info] Number of positive: 302, number of negative: 70644\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016219 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004257 -> initscore=-5.454981\n",
      "[LightGBM] [Info] Start training from score -5.454981\n",
      "[LightGBM] [Info] Number of positive: 1600, number of negative: 69346\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012143 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.022552 -> initscore=-3.769105\n",
      "[LightGBM] [Info] Start training from score -3.769105\n",
      "[LightGBM] [Info] Number of positive: 578, number of negative: 70368\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014093 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.008147 -> initscore=-4.801920\n",
      "[LightGBM] [Info] Start training from score -4.801920\n",
      "[LightGBM] [Info] Number of positive: 1212, number of negative: 69734\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014506 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.017083 -> initscore=-4.052416\n",
      "[LightGBM] [Info] Start training from score -4.052416\n",
      "[LightGBM] [Info] Number of positive: 335, number of negative: 70611\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014471 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.004722 -> initscore=-5.350811\n",
      "[LightGBM] [Info] Start training from score -5.350811\n",
      "[LightGBM] [Info] Number of positive: 3004, number of negative: 67942\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013727 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.042342 -> initscore=-3.118710\n",
      "[LightGBM] [Info] Start training from score -3.118710\n",
      "[LightGBM] [Info] Number of positive: 16363, number of negative: 54583\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013116 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.230640 -> initscore=-1.204700\n",
      "[LightGBM] [Info] Start training from score -1.204700\n",
      "[LightGBM] [Info] Number of positive: 756, number of negative: 70190\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.018905 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.010656 -> initscore=-4.530920\n",
      "[LightGBM] [Info] Start training from score -4.530920\n",
      "[LightGBM] [Info] Number of positive: 240, number of negative: 70706\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012293 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.003383 -> initscore=-5.685647\n",
      "[LightGBM] [Info] Start training from score -5.685647\n",
      "[LightGBM] [Info] Number of positive: 1587, number of negative: 69359\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015385 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.022369 -> initscore=-3.777450\n",
      "[LightGBM] [Info] Start training from score -3.777450\n",
      "[LightGBM] [Info] Number of positive: 355, number of negative: 70591\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014222 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005004 -> initscore=-5.292540\n",
      "[LightGBM] [Info] Start training from score -5.292540\n",
      "[LightGBM] [Info] Number of positive: 529, number of negative: 70417\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012438 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.007456 -> initscore=-4.891202\n",
      "[LightGBM] [Info] Start training from score -4.891202\n",
      "[LightGBM] [Info] Number of positive: 1728, number of negative: 69218\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015080 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.024357 -> initscore=-3.690296\n",
      "[LightGBM] [Info] Start training from score -3.690296\n",
      "[LightGBM] [Info] Number of positive: 1772, number of negative: 69174\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013340 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.024977 -> initscore=-3.664516\n",
      "[LightGBM] [Info] Start training from score -3.664516\n",
      "[LightGBM] [Info] Number of positive: 834, number of negative: 70112\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014884 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011755 -> initscore=-4.431616\n",
      "[LightGBM] [Info] Start training from score -4.431616\n",
      "[LightGBM] [Info] Number of positive: 2759, number of negative: 68187\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013747 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038889 -> initscore=-3.207386\n",
      "[LightGBM] [Info] Start training from score -3.207386\n",
      "[LightGBM] [Info] Number of positive: 2227, number of negative: 68719\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012973 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031390 -> initscore=-3.429370\n",
      "[LightGBM] [Info] Start training from score -3.429370\n",
      "[LightGBM] [Info] Number of positive: 4294, number of negative: 66652\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012507 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.060525 -> initscore=-2.742266\n",
      "[LightGBM] [Info] Start training from score -2.742266\n",
      "[LightGBM] [Info] Number of positive: 6642, number of negative: 64304\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013805 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.093621 -> initscore=-2.270209\n",
      "[LightGBM] [Info] Start training from score -2.270209\n",
      "[LightGBM] [Info] Number of positive: 8139, number of negative: 62807\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013354 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114721 -> initscore=-2.043399\n",
      "[LightGBM] [Info] Start training from score -2.043399\n",
      "[LightGBM] [Info] Number of positive: 1253, number of negative: 69693\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013807 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.017661 -> initscore=-4.018559\n",
      "[LightGBM] [Info] Start training from score -4.018559\n",
      "[LightGBM] [Info] Number of positive: 17756, number of negative: 53190\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012505 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.250275 -> initscore=-1.097147\n",
      "[LightGBM] [Info] Start training from score -1.097147\n",
      "[LightGBM] [Info] Number of positive: 2028, number of negative: 68918\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014980 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.028585 -> initscore=-3.525867\n",
      "[LightGBM] [Info] Start training from score -3.525867\n",
      "[LightGBM] [Info] Number of positive: 2449, number of negative: 68497\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012622 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.034519 -> initscore=-3.331110\n",
      "[LightGBM] [Info] Start training from score -3.331110\n",
      "[LightGBM] [Info] Number of positive: 13690, number of negative: 57256\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012657 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.192964 -> initscore=-1.430867\n",
      "[LightGBM] [Info] Start training from score -1.430867\n",
      "[LightGBM] [Info] Number of positive: 3480, number of negative: 67466\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012841 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.049051 -> initscore=-2.964591\n",
      "[LightGBM] [Info] Start training from score -2.964591\n",
      "[LightGBM] [Info] Number of positive: 1544, number of negative: 69402\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013268 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.021763 -> initscore=-3.805539\n",
      "[LightGBM] [Info] Start training from score -3.805539\n",
      "[LightGBM] [Info] Number of positive: 6645, number of negative: 64301\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013283 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.093663 -> initscore=-2.269710\n",
      "[LightGBM] [Info] Start training from score -2.269710\n",
      "[LightGBM] [Info] Number of positive: 1430, number of negative: 69516\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013226 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020156 -> initscore=-3.883882\n",
      "[LightGBM] [Info] Start training from score -3.883882\n",
      "[LightGBM] [Info] Number of positive: 4073, number of negative: 66873\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012334 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.057410 -> initscore=-2.798415\n",
      "[LightGBM] [Info] Start training from score -2.798415\n",
      "[LightGBM] [Info] Number of positive: 9364, number of negative: 61582\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016380 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.131988 -> initscore=-1.883497\n",
      "[LightGBM] [Info] Start training from score -1.883497\n",
      "[LightGBM] [Info] Number of positive: 1548, number of negative: 69398\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013416 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.021819 -> initscore=-3.802894\n",
      "[LightGBM] [Info] Start training from score -3.802894\n",
      "[LightGBM] [Info] Number of positive: 3904, number of negative: 67042\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015720 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.055028 -> initscore=-2.843318\n",
      "[LightGBM] [Info] Start training from score -2.843318\n",
      "[LightGBM] [Info] Number of positive: 2705, number of negative: 68241\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015595 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.038128 -> initscore=-3.227944\n",
      "[LightGBM] [Info] Start training from score -3.227944\n",
      "[LightGBM] [Info] Number of positive: 1643, number of negative: 69303\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012776 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.023158 -> initscore=-3.741964\n",
      "[LightGBM] [Info] Start training from score -3.741964\n",
      "[LightGBM] [Info] Number of positive: 6019, number of negative: 64927\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013014 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.084839 -> initscore=-2.378342\n",
      "[LightGBM] [Info] Start training from score -2.378342\n",
      "[LightGBM] [Info] Number of positive: 3042, number of negative: 67904\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016555 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.042878 -> initscore=-3.105580\n",
      "[LightGBM] [Info] Start training from score -3.105580\n",
      "[LightGBM] [Info] Number of positive: 5618, number of negative: 65328\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.021998 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.079187 -> initscore=-2.453445\n",
      "[LightGBM] [Info] Start training from score -2.453445\n",
      "[LightGBM] [Info] Number of positive: 1415, number of negative: 69531\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012645 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.019945 -> initscore=-3.894643\n",
      "[LightGBM] [Info] Start training from score -3.894643\n",
      "[LightGBM] [Info] Number of positive: 2126, number of negative: 68820\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014855 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.029966 -> initscore=-3.477252\n",
      "[LightGBM] [Info] Start training from score -3.477252\n",
      "[LightGBM] [Info] Number of positive: 3927, number of negative: 67019\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013537 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.055352 -> initscore=-2.837100\n",
      "[LightGBM] [Info] Start training from score -2.837100\n",
      "[LightGBM] [Info] Number of positive: 16618, number of negative: 54328\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012721 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.234234 -> initscore=-1.184553\n",
      "[LightGBM] [Info] Start training from score -1.184553\n",
      "[LightGBM] [Info] Number of positive: 6617, number of negative: 64329\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014430 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.093268 -> initscore=-2.274368\n",
      "[LightGBM] [Info] Start training from score -2.274368\n",
      "[LightGBM] [Info] Number of positive: 1794, number of negative: 69152\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013354 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.025287 -> initscore=-3.651859\n",
      "[LightGBM] [Info] Start training from score -3.651859\n",
      "[LightGBM] [Info] Number of positive: 2791, number of negative: 68155\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014705 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.039340 -> initscore=-3.195385\n",
      "[LightGBM] [Info] Start training from score -3.195385\n",
      "[LightGBM] [Info] Number of positive: 976, number of negative: 69970\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012827 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.013757 -> initscore=-4.272359\n",
      "[LightGBM] [Info] Start training from score -4.272359\n",
      "[LightGBM] [Info] Number of positive: 7589, number of negative: 63357\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015032 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.106969 -> initscore=-2.122086\n",
      "[LightGBM] [Info] Start training from score -2.122086\n",
      "[LightGBM] [Info] Number of positive: 13058, number of negative: 57888\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.024880 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.184055 -> initscore=-1.489109\n",
      "[LightGBM] [Info] Start training from score -1.489109\n",
      "[LightGBM] [Info] Number of positive: 6440, number of negative: 64506\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013095 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.090773 -> initscore=-2.304230\n",
      "[LightGBM] [Info] Start training from score -2.304230\n",
      "[LightGBM] [Info] Number of positive: 6241, number of negative: 64705\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013370 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.087968 -> initscore=-2.338698\n",
      "[LightGBM] [Info] Start training from score -2.338698\n",
      "[LightGBM] [Info] Number of positive: 2551, number of negative: 68395\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014653 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035957 -> initscore=-3.288814\n",
      "[LightGBM] [Info] Start training from score -3.288814\n",
      "[LightGBM] [Info] Number of positive: 3093, number of negative: 67853\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014562 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.043597 -> initscore=-3.088202\n",
      "[LightGBM] [Info] Start training from score -3.088202\n",
      "[LightGBM] [Info] Number of positive: 2204, number of negative: 68742\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013833 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031066 -> initscore=-3.440086\n",
      "[LightGBM] [Info] Start training from score -3.440086\n",
      "[LightGBM] [Info] Number of positive: 7839, number of negative: 63107\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013729 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.110492 -> initscore=-2.085720\n",
      "[LightGBM] [Info] Start training from score -2.085720\n",
      "[LightGBM] [Info] Number of positive: 642, number of negative: 70304\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013347 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.009049 -> initscore=-4.695996\n",
      "[LightGBM] [Info] Start training from score -4.695996\n",
      "[LightGBM] [Info] Number of positive: 10469, number of negative: 60477\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012855 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.147563 -> initscore=-1.753845\n",
      "[LightGBM] [Info] Start training from score -1.753845\n",
      "[LightGBM] [Info] Number of positive: 6454, number of negative: 64492\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013149 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.090971 -> initscore=-2.301841\n",
      "[LightGBM] [Info] Start training from score -2.301841\n",
      "[LightGBM] [Info] Number of positive: 369, number of negative: 70577\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013670 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.005201 -> initscore=-5.253663\n",
      "[LightGBM] [Info] Start training from score -5.253663\n",
      "[LightGBM] [Info] Number of positive: 8636, number of negative: 62310\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013092 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.121726 -> initscore=-1.976182\n",
      "[LightGBM] [Info] Start training from score -1.976182\n",
      "[LightGBM] [Info] Number of positive: 2353, number of negative: 68593\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013170 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.033166 -> initscore=-3.372499\n",
      "[LightGBM] [Info] Start training from score -3.372499\n",
      "[LightGBM] [Info] Number of positive: 9626, number of negative: 61320\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012571 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.135681 -> initscore=-1.851638\n",
      "[LightGBM] [Info] Start training from score -1.851638\n",
      "Validation Accuracy: 0.0019, Validation F1 Score: 0.4712\n",
      "Test Accuracy: 0.0026, Test F1 Score: 0.4658\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Préparer les données pour LightGBM\n",
    "X_train_lgb, y_train_lgb = prepare_data_for_rf(X_train_final_train, train_targets.cpu().numpy())\n",
    "X_val_lgb, y_val_lgb = prepare_data_for_rf(X_train_final_val, val_targets.cpu().numpy())\n",
    "X_test_lgb, y_test_lgb = prepare_data_for_rf(X_test_final, y_test_tensor.cpu().numpy())\n",
    "\n",
    "# Initialiser les modèles LightGBM pour chaque label\n",
    "num_labels = y_train_lgb.shape[1]\n",
    "lgb_models = [lgb.LGBMClassifier(objective='binary', n_estimators=100, random_state=42, device='gpu') for _ in range(num_labels)]\n",
    "\n",
    "# Entraîner les modèles\n",
    "for i in range(num_labels):\n",
    "    lgb_models[i].fit(X_train_lgb, y_train_lgb[:, i])\n",
    "\n",
    "# Évaluer les modèles sur l'ensemble de validation\n",
    "y_val_pred_lgb = np.zeros_like(y_val_lgb)\n",
    "for i in range(num_labels):\n",
    "    y_val_pred_lgb[:, i] = lgb_models[i].predict(X_val_lgb)\n",
    "y_val_pred_lgb = (y_val_pred_lgb > 0.5).astype(int)\n",
    "val_accuracy = accuracy_score(y_val_lgb, y_val_pred_lgb)\n",
    "val_f1 = f1_score(y_val_lgb, y_val_pred_lgb, average='weighted')\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# Évaluer les modèles sur l'ensemble de test\n",
    "y_test_pred_lgb = np.zeros_like(y_test_lgb)\n",
    "for i in range(num_labels):\n",
    "    y_test_pred_lgb[:, i] = lgb_models[i].predict(X_test_lgb)\n",
    "y_test_pred_lgb = (y_test_pred_lgb > 0.5).astype(int)\n",
    "test_accuracy = accuracy_score(y_test_lgb, y_test_pred_lgb)\n",
    "test_f1 = f1_score(y_test_lgb, y_test_pred_lgb, average='weighted')\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 569, number of negative: 70377\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014323 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360829\n",
      "[LightGBM] [Info] Start training from score -10.360829\n",
      "[LightGBM] [Info] Number of positive: 3983, number of negative: 66963\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013575 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.314452\n",
      "[LightGBM] [Info] Start training from score -10.314452\n",
      "[LightGBM] [Info] Number of positive: 812, number of negative: 70134\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014923 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358541\n",
      "[LightGBM] [Info] Start training from score -10.358541\n",
      "[LightGBM] [Info] Number of positive: 255, number of negative: 70691\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013963 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360449\n",
      "[LightGBM] [Info] Start training from score -10.360449\n",
      "[LightGBM] [Info] Number of positive: 200, number of negative: 70746\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013734 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358810\n",
      "[LightGBM] [Info] Start training from score -10.358810\n",
      "[LightGBM] [Info] Number of positive: 938, number of negative: 70008\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013979 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.357110\n",
      "[LightGBM] [Info] Start training from score -10.357110\n",
      "[LightGBM] [Info] Number of positive: 402, number of negative: 70544\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013963 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361571\n",
      "[LightGBM] [Info] Start training from score -10.361571\n",
      "[LightGBM] [Info] Number of positive: 1606, number of negative: 69340\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014667 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.348509\n",
      "[LightGBM] [Info] Start training from score -10.348509\n",
      "[LightGBM] [Info] Number of positive: 3237, number of negative: 67709\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014659 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.325403\n",
      "[LightGBM] [Info] Start training from score -10.325403\n",
      "[LightGBM] [Info] Number of positive: 714, number of negative: 70232\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012893 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359561\n",
      "[LightGBM] [Info] Start training from score -10.359561\n",
      "[LightGBM] [Info] Number of positive: 2392, number of negative: 68554\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013910 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.337563\n",
      "[LightGBM] [Info] Start training from score -10.337563\n",
      "[LightGBM] [Info] Number of positive: 6648, number of negative: 64298\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012690 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000035 -> initscore=-10.274064\n",
      "[LightGBM] [Info] Start training from score -10.274064\n",
      "[LightGBM] [Info] Number of positive: 1447, number of negative: 69499\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014852 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.350647\n",
      "[LightGBM] [Info] Start training from score -10.350647\n",
      "[LightGBM] [Info] Number of positive: 4505, number of negative: 66441\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013853 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.306691\n",
      "[LightGBM] [Info] Start training from score -10.306691\n",
      "[LightGBM] [Info] Number of positive: 2925, number of negative: 68021\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015188 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.329927\n",
      "[LightGBM] [Info] Start training from score -10.329927\n",
      "[LightGBM] [Info] Number of positive: 1165, number of negative: 69781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016603 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.354325\n",
      "[LightGBM] [Info] Start training from score -10.354325\n",
      "[LightGBM] [Info] Number of positive: 221, number of negative: 70725\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015327 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359578\n",
      "[LightGBM] [Info] Start training from score -10.359578\n",
      "[LightGBM] [Info] Number of positive: 460, number of negative: 70486\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013921 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361448\n",
      "[LightGBM] [Info] Start training from score -10.361448\n",
      "[LightGBM] [Info] Number of positive: 481, number of negative: 70465\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014975 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361362\n",
      "[LightGBM] [Info] Start training from score -10.361362\n",
      "[LightGBM] [Info] Number of positive: 764, number of negative: 70182\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014846 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359053\n",
      "[LightGBM] [Info] Start training from score -10.359053\n",
      "[LightGBM] [Info] Number of positive: 2989, number of negative: 67957\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015574 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.329002\n",
      "[LightGBM] [Info] Start training from score -10.329002\n",
      "[LightGBM] [Info] Number of positive: 510, number of negative: 70436\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014694 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361214\n",
      "[LightGBM] [Info] Start training from score -10.361214\n",
      "[LightGBM] [Info] Number of positive: 430, number of negative: 70516\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016573 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361536\n",
      "[LightGBM] [Info] Start training from score -10.361536\n",
      "[LightGBM] [Info] Number of positive: 3149, number of negative: 67797\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015023 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.326682\n",
      "[LightGBM] [Info] Start training from score -10.326682\n",
      "[LightGBM] [Info] Number of positive: 2713, number of negative: 68233\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015450 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.332979\n",
      "[LightGBM] [Info] Start training from score -10.332979\n",
      "[LightGBM] [Info] Number of positive: 261, number of negative: 70685\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.021841 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360566\n",
      "[LightGBM] [Info] Start training from score -10.360566\n",
      "[LightGBM] [Info] Number of positive: 1189, number of negative: 69757\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.018294 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.354019\n",
      "[LightGBM] [Info] Start training from score -10.354019\n",
      "[LightGBM] [Info] Number of positive: 10840, number of negative: 60106\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014986 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000037 -> initscore=-10.206774\n",
      "[LightGBM] [Info] Start training from score -10.206774\n",
      "[LightGBM] [Info] Number of positive: 2825, number of negative: 68121\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.022975 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.331369\n",
      "[LightGBM] [Info] Start training from score -10.331369\n",
      "[LightGBM] [Info] Number of positive: 1253, number of negative: 69693\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015514 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.353197\n",
      "[LightGBM] [Info] Start training from score -10.353197\n",
      "[LightGBM] [Info] Number of positive: 458, number of negative: 70488\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016407 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361456\n",
      "[LightGBM] [Info] Start training from score -10.361456\n",
      "[LightGBM] [Info] Number of positive: 731, number of negative: 70215\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013787 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359391\n",
      "[LightGBM] [Info] Start training from score -10.359391\n",
      "[LightGBM] [Info] Number of positive: 273, number of negative: 70673\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013825 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360773\n",
      "[LightGBM] [Info] Start training from score -10.360773\n",
      "[LightGBM] [Info] Number of positive: 1936, number of negative: 69010\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015634 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.343974\n",
      "[LightGBM] [Info] Start training from score -10.343974\n",
      "[LightGBM] [Info] Number of positive: 1087, number of negative: 69859\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015824 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.355305\n",
      "[LightGBM] [Info] Start training from score -10.355305\n",
      "[LightGBM] [Info] Number of positive: 2555, number of negative: 68391\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.025335 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.335242\n",
      "[LightGBM] [Info] Start training from score -10.335242\n",
      "[LightGBM] [Info] Number of positive: 1928, number of negative: 69018\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014570 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.344085\n",
      "[LightGBM] [Info] Start training from score -10.344085\n",
      "[LightGBM] [Info] Number of positive: 1237, number of negative: 69709\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014146 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.353403\n",
      "[LightGBM] [Info] Start training from score -10.353403\n",
      "[LightGBM] [Info] Number of positive: 1764, number of negative: 69182\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.017896 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.346351\n",
      "[LightGBM] [Info] Start training from score -10.346351\n",
      "[LightGBM] [Info] Number of positive: 4949, number of negative: 65997\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015089 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000034 -> initscore=-10.300030\n",
      "[LightGBM] [Info] Start training from score -10.300030\n",
      "[LightGBM] [Info] Number of positive: 302, number of negative: 70644\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014737 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361149\n",
      "[LightGBM] [Info] Start training from score -10.361149\n",
      "[LightGBM] [Info] Number of positive: 717, number of negative: 70229\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015373 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359531\n",
      "[LightGBM] [Info] Start training from score -10.359531\n",
      "[LightGBM] [Info] Number of positive: 1018, number of negative: 69928\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014127 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.356153\n",
      "[LightGBM] [Info] Start training from score -10.356153\n",
      "[LightGBM] [Info] Number of positive: 342, number of negative: 70604\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015154 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361448\n",
      "[LightGBM] [Info] Start training from score -10.361448\n",
      "[LightGBM] [Info] Number of positive: 580, number of negative: 70366\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014522 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360747\n",
      "[LightGBM] [Info] Start training from score -10.360747\n",
      "[LightGBM] [Info] Number of positive: 2097, number of negative: 68849\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013796 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.341726\n",
      "[LightGBM] [Info] Start training from score -10.341726\n",
      "[LightGBM] [Info] Number of positive: 191, number of negative: 70755\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013734 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358408\n",
      "[LightGBM] [Info] Start training from score -10.358408\n",
      "[LightGBM] [Info] Number of positive: 175, number of negative: 70771\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014636 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.357559\n",
      "[LightGBM] [Info] Start training from score -10.357559\n",
      "[LightGBM] [Info] Number of positive: 373, number of negative: 70573\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013581 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361551\n",
      "[LightGBM] [Info] Start training from score -10.361551\n",
      "[LightGBM] [Info] Number of positive: 196, number of negative: 70750\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012858 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358638\n",
      "[LightGBM] [Info] Start training from score -10.358638\n",
      "[LightGBM] [Info] Number of positive: 188, number of negative: 70758\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013584 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358263\n",
      "[LightGBM] [Info] Start training from score -10.358263\n",
      "[LightGBM] [Info] Number of positive: 267, number of negative: 70679\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013712 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360673\n",
      "[LightGBM] [Info] Start training from score -10.360673\n",
      "[LightGBM] [Info] Number of positive: 130, number of negative: 70816\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012871 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.353740\n",
      "[LightGBM] [Info] Start training from score -10.353740\n",
      "[LightGBM] [Info] Number of positive: 785, number of negative: 70161\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013825 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358831\n",
      "[LightGBM] [Info] Start training from score -10.358831\n",
      "[LightGBM] [Info] Number of positive: 1107, number of negative: 69839\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014217 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.355055\n",
      "[LightGBM] [Info] Start training from score -10.355055\n",
      "[LightGBM] [Info] Number of positive: 1218, number of negative: 69728\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013798 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.353648\n",
      "[LightGBM] [Info] Start training from score -10.353648\n",
      "[LightGBM] [Info] Number of positive: 194, number of negative: 70752\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013566 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358548\n",
      "[LightGBM] [Info] Start training from score -10.358548\n",
      "[LightGBM] [Info] Number of positive: 393, number of negative: 70553\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013480 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361572\n",
      "[LightGBM] [Info] Start training from score -10.361572\n",
      "[LightGBM] [Info] Number of positive: 359, number of negative: 70587\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013365 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361516\n",
      "[LightGBM] [Info] Start training from score -10.361516\n",
      "[LightGBM] [Info] Number of positive: 546, number of negative: 70400\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013536 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360991\n",
      "[LightGBM] [Info] Start training from score -10.360991\n",
      "[LightGBM] [Info] Number of positive: 265, number of negative: 70681\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014351 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360638\n",
      "[LightGBM] [Info] Start training from score -10.360638\n",
      "[LightGBM] [Info] Number of positive: 1217, number of negative: 69729\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013757 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.353661\n",
      "[LightGBM] [Info] Start training from score -10.353661\n",
      "[LightGBM] [Info] Number of positive: 113, number of negative: 70833\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015592 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.351365\n",
      "[LightGBM] [Info] Start training from score -10.351365\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 782, number of negative: 70164\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.019054 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358863\n",
      "[LightGBM] [Info] Start training from score -10.358863\n",
      "[LightGBM] [Info] Number of positive: 398, number of negative: 70548\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012874 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361572\n",
      "[LightGBM] [Info] Start training from score -10.361572\n",
      "[LightGBM] [Info] Number of positive: 452, number of negative: 70494\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015591 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361476\n",
      "[LightGBM] [Info] Start training from score -10.361476\n",
      "[LightGBM] [Info] Number of positive: 1510, number of negative: 69436\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014274 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.349804\n",
      "[LightGBM] [Info] Start training from score -10.349804\n",
      "[LightGBM] [Info] Number of positive: 563, number of negative: 70383\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014038 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360872\n",
      "[LightGBM] [Info] Start training from score -10.360872\n",
      "[LightGBM] [Info] Number of positive: 413, number of negative: 70533\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.023992 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361563\n",
      "[LightGBM] [Info] Start training from score -10.361563\n",
      "[LightGBM] [Info] Number of positive: 597, number of negative: 70349\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.024580 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360614\n",
      "[LightGBM] [Info] Start training from score -10.360614\n",
      "[LightGBM] [Info] Number of positive: 1503, number of negative: 69443\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016452 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.349898\n",
      "[LightGBM] [Info] Start training from score -10.349898\n",
      "[LightGBM] [Info] Number of positive: 80, number of negative: 70866\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015007 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.343535\n",
      "[LightGBM] [Info] Start training from score -10.343535\n",
      "[LightGBM] [Info] Number of positive: 116, number of negative: 70830\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014974 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.351840\n",
      "[LightGBM] [Info] Start training from score -10.351840\n",
      "[LightGBM] [Info] Number of positive: 599, number of negative: 70347\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014534 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360598\n",
      "[LightGBM] [Info] Start training from score -10.360598\n",
      "[LightGBM] [Info] Number of positive: 312, number of negative: 70634\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.017187 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361244\n",
      "[LightGBM] [Info] Start training from score -10.361244\n",
      "[LightGBM] [Info] Number of positive: 223, number of negative: 70723\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014617 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359641\n",
      "[LightGBM] [Info] Start training from score -10.359641\n",
      "[LightGBM] [Info] Number of positive: 276, number of negative: 70670\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013195 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360819\n",
      "[LightGBM] [Info] Start training from score -10.360819\n",
      "[LightGBM] [Info] Number of positive: 1169, number of negative: 69777\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013541 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.354274\n",
      "[LightGBM] [Info] Start training from score -10.354274\n",
      "[LightGBM] [Info] Number of positive: 372, number of negative: 70574\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012952 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361549\n",
      "[LightGBM] [Info] Start training from score -10.361549\n",
      "[LightGBM] [Info] Number of positive: 285, number of negative: 70661\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013163 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360948\n",
      "[LightGBM] [Info] Start training from score -10.360948\n",
      "[LightGBM] [Info] Number of positive: 1240, number of negative: 69706\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012487 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.353365\n",
      "[LightGBM] [Info] Start training from score -10.353365\n",
      "[LightGBM] [Info] Number of positive: 4730, number of negative: 66216\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015395 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000034 -> initscore=-10.303322\n",
      "[LightGBM] [Info] Start training from score -10.303322\n",
      "[LightGBM] [Info] Number of positive: 1036, number of negative: 69910\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015164 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.355934\n",
      "[LightGBM] [Info] Start training from score -10.355934\n",
      "[LightGBM] [Info] Number of positive: 247, number of negative: 70699\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015511 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360278\n",
      "[LightGBM] [Info] Start training from score -10.360278\n",
      "[LightGBM] [Info] Number of positive: 1172, number of negative: 69774\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014446 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.354236\n",
      "[LightGBM] [Info] Start training from score -10.354236\n",
      "[LightGBM] [Info] Number of positive: 345, number of negative: 70601\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014438 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361462\n",
      "[LightGBM] [Info] Start training from score -10.361462\n",
      "[LightGBM] [Info] Number of positive: 2243, number of negative: 68703\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.019718 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.339672\n",
      "[LightGBM] [Info] Start training from score -10.339672\n",
      "[LightGBM] [Info] Number of positive: 3114, number of negative: 67832\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013649 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.327190\n",
      "[LightGBM] [Info] Start training from score -10.327190\n",
      "[LightGBM] [Info] Number of positive: 132, number of negative: 70814\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013060 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.353975\n",
      "[LightGBM] [Info] Start training from score -10.353975\n",
      "[LightGBM] [Info] Number of positive: 1776, number of negative: 69170\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013303 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.346186\n",
      "[LightGBM] [Info] Start training from score -10.346186\n",
      "[LightGBM] [Info] Number of positive: 327, number of negative: 70619\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014149 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361360\n",
      "[LightGBM] [Info] Start training from score -10.361360\n",
      "[LightGBM] [Info] Number of positive: 1724, number of negative: 69222\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013342 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.346900\n",
      "[LightGBM] [Info] Start training from score -10.346900\n",
      "[LightGBM] [Info] Number of positive: 10851, number of negative: 60095\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013154 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000037 -> initscore=-10.206591\n",
      "[LightGBM] [Info] Start training from score -10.206591\n",
      "[LightGBM] [Info] Number of positive: 1599, number of negative: 69347\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014520 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.348603\n",
      "[LightGBM] [Info] Start training from score -10.348603\n",
      "[LightGBM] [Info] Number of positive: 821, number of negative: 70125\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.017886 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358442\n",
      "[LightGBM] [Info] Start training from score -10.358442\n",
      "[LightGBM] [Info] Number of positive: 15828, number of negative: 55118\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012950 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000040 -> initscore=-10.120205\n",
      "[LightGBM] [Info] Start training from score -10.120205\n",
      "[LightGBM] [Info] Number of positive: 359, number of negative: 70587\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.020778 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361516\n",
      "[LightGBM] [Info] Start training from score -10.361516\n",
      "[LightGBM] [Info] Number of positive: 663, number of negative: 70283\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013235 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360047\n",
      "[LightGBM] [Info] Start training from score -10.360047\n",
      "[LightGBM] [Info] Number of positive: 299, number of negative: 70647\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014584 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361117\n",
      "[LightGBM] [Info] Start training from score -10.361117\n",
      "[LightGBM] [Info] Number of positive: 863, number of negative: 70083\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.018323 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.357975\n",
      "[LightGBM] [Info] Start training from score -10.357975\n",
      "[LightGBM] [Info] Number of positive: 9132, number of negative: 61814\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015536 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000036 -> initscore=-10.234756\n",
      "[LightGBM] [Info] Start training from score -10.234756\n",
      "[LightGBM] [Info] Number of positive: 1286, number of negative: 69660\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013999 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.352769\n",
      "[LightGBM] [Info] Start training from score -10.352769\n",
      "[LightGBM] [Info] Number of positive: 3730, number of negative: 67216\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013075 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.318185\n",
      "[LightGBM] [Info] Start training from score -10.318185\n",
      "[LightGBM] [Info] Number of positive: 94, number of negative: 70852\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013772 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.347577\n",
      "[LightGBM] [Info] Start training from score -10.347577\n",
      "[LightGBM] [Info] Number of positive: 56, number of negative: 70890\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014279 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.331574\n",
      "[LightGBM] [Info] Start training from score -10.331574\n",
      "[LightGBM] [Info] Number of positive: 364, number of negative: 70582\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016007 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361530\n",
      "[LightGBM] [Info] Start training from score -10.361530\n",
      "[LightGBM] [Info] Number of positive: 1692, number of negative: 69254\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014448 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.347338\n",
      "[LightGBM] [Info] Start training from score -10.347338\n",
      "[LightGBM] [Info] Number of positive: 1590, number of negative: 69356\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013389 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.348725\n",
      "[LightGBM] [Info] Start training from score -10.348725\n",
      "[LightGBM] [Info] Number of positive: 428, number of negative: 70518\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012995 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361540\n",
      "[LightGBM] [Info] Start training from score -10.361540\n",
      "[LightGBM] [Info] Number of positive: 798, number of negative: 70148\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014568 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358692\n",
      "[LightGBM] [Info] Start training from score -10.358692\n",
      "[LightGBM] [Info] Number of positive: 266, number of negative: 70680\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012510 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360656\n",
      "[LightGBM] [Info] Start training from score -10.360656\n",
      "[LightGBM] [Info] Number of positive: 913, number of negative: 70033\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013638 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.357403\n",
      "[LightGBM] [Info] Start training from score -10.357403\n",
      "[LightGBM] [Info] Number of positive: 218, number of negative: 70728\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012836 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359481\n",
      "[LightGBM] [Info] Start training from score -10.359481\n",
      "[LightGBM] [Info] Number of positive: 3594, number of negative: 67352\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013534 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.320184\n",
      "[LightGBM] [Info] Start training from score -10.320184\n",
      "[LightGBM] [Info] Number of positive: 315, number of negative: 70631\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012874 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361270\n",
      "[LightGBM] [Info] Start training from score -10.361270\n",
      "[LightGBM] [Info] Number of positive: 16047, number of negative: 54899\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013871 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000040 -> initscore=-10.116226\n",
      "[LightGBM] [Info] Start training from score -10.116226\n",
      "[LightGBM] [Info] Number of positive: 14078, number of negative: 56868\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015582 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000039 -> initscore=-10.151444\n",
      "[LightGBM] [Info] Start training from score -10.151444\n",
      "[LightGBM] [Info] Number of positive: 374, number of negative: 70572\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.022072 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361553\n",
      "[LightGBM] [Info] Start training from score -10.361553\n",
      "[LightGBM] [Info] Number of positive: 20185, number of negative: 50761\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013071 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000044 -> initscore=-10.037887\n",
      "[LightGBM] [Info] Start training from score -10.037887\n",
      "[LightGBM] [Info] Number of positive: 785, number of negative: 70161\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014578 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358831\n",
      "[LightGBM] [Info] Start training from score -10.358831\n",
      "[LightGBM] [Info] Number of positive: 612, number of negative: 70334\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016005 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360493\n",
      "[LightGBM] [Info] Start training from score -10.360493\n",
      "[LightGBM] [Info] Number of positive: 501, number of negative: 70445\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014832 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361263\n",
      "[LightGBM] [Info] Start training from score -10.361263\n",
      "[LightGBM] [Info] Number of positive: 4673, number of negative: 66273\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014623 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.304177\n",
      "[LightGBM] [Info] Start training from score -10.304177\n",
      "[LightGBM] [Info] Number of positive: 1391, number of negative: 69555\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014112 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.351391\n",
      "[LightGBM] [Info] Start training from score -10.351391\n",
      "[LightGBM] [Info] Number of positive: 237, number of negative: 70709\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.020999 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360037\n",
      "[LightGBM] [Info] Start training from score -10.360037\n",
      "[LightGBM] [Info] Number of positive: 3645, number of negative: 67301\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013226 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.319435\n",
      "[LightGBM] [Info] Start training from score -10.319435\n",
      "[LightGBM] [Info] Number of positive: 1911, number of negative: 69035\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014759 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.344321\n",
      "[LightGBM] [Info] Start training from score -10.344321\n",
      "[LightGBM] [Info] Number of positive: 633, number of negative: 70313\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013915 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360315\n",
      "[LightGBM] [Info] Start training from score -10.360315\n",
      "[LightGBM] [Info] Number of positive: 1903, number of negative: 69043\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015192 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.344432\n",
      "[LightGBM] [Info] Start training from score -10.344432\n",
      "[LightGBM] [Info] Number of positive: 1961, number of negative: 68985\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014859 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.343626\n",
      "[LightGBM] [Info] Start training from score -10.343626\n",
      "[LightGBM] [Info] Number of positive: 1795, number of negative: 69151\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014577 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.345925\n",
      "[LightGBM] [Info] Start training from score -10.345925\n",
      "[LightGBM] [Info] Number of positive: 701, number of negative: 70245\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013297 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359688\n",
      "[LightGBM] [Info] Start training from score -10.359688\n",
      "[LightGBM] [Info] Number of positive: 1908, number of negative: 69038\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013276 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.344363\n",
      "[LightGBM] [Info] Start training from score -10.344363\n",
      "[LightGBM] [Info] Number of positive: 562, number of negative: 70384\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014644 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360879\n",
      "[LightGBM] [Info] Start training from score -10.360879\n",
      "[LightGBM] [Info] Number of positive: 228, number of negative: 70718\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013347 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359791\n",
      "[LightGBM] [Info] Start training from score -10.359791\n",
      "[LightGBM] [Info] Number of positive: 2140, number of negative: 68806\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012780 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.341123\n",
      "[LightGBM] [Info] Start training from score -10.341123\n",
      "[LightGBM] [Info] Number of positive: 316, number of negative: 70630\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013307 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361278\n",
      "[LightGBM] [Info] Start training from score -10.361278\n",
      "[LightGBM] [Info] Number of positive: 408, number of negative: 70538\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013692 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361568\n",
      "[LightGBM] [Info] Start training from score -10.361568\n",
      "[LightGBM] [Info] Number of positive: 2343, number of negative: 68603\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014274 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.338258\n",
      "[LightGBM] [Info] Start training from score -10.338258\n",
      "[LightGBM] [Info] Number of positive: 498, number of negative: 70448\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013602 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361279\n",
      "[LightGBM] [Info] Start training from score -10.361279\n",
      "[LightGBM] [Info] Number of positive: 218, number of negative: 70728\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013191 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359481\n",
      "[LightGBM] [Info] Start training from score -10.359481\n",
      "[LightGBM] [Info] Number of positive: 6148, number of negative: 64798\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015216 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000034 -> initscore=-10.281783\n",
      "[LightGBM] [Info] Start training from score -10.281783\n",
      "[LightGBM] [Info] Number of positive: 830, number of negative: 70116\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015135 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358343\n",
      "[LightGBM] [Info] Start training from score -10.358343\n",
      "[LightGBM] [Info] Number of positive: 684, number of negative: 70262\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014563 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359851\n",
      "[LightGBM] [Info] Start training from score -10.359851\n",
      "[LightGBM] [Info] Number of positive: 1826, number of negative: 69120\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012542 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.345497\n",
      "[LightGBM] [Info] Start training from score -10.345497\n",
      "[LightGBM] [Info] Number of positive: 73, number of negative: 70873\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015203 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.340895\n",
      "[LightGBM] [Info] Start training from score -10.340895\n",
      "[LightGBM] [Info] Number of positive: 207, number of negative: 70739\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015855 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359090\n",
      "[LightGBM] [Info] Start training from score -10.359090\n",
      "[LightGBM] [Info] Number of positive: 119, number of negative: 70827\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014818 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.352289\n",
      "[LightGBM] [Info] Start training from score -10.352289\n",
      "[LightGBM] [Info] Number of positive: 830, number of negative: 70116\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014536 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358343\n",
      "[LightGBM] [Info] Start training from score -10.358343\n",
      "[LightGBM] [Info] Number of positive: 1098, number of negative: 69848\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014168 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.355168\n",
      "[LightGBM] [Info] Start training from score -10.355168\n",
      "[LightGBM] [Info] Number of positive: 942, number of negative: 70004\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012429 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.357063\n",
      "[LightGBM] [Info] Start training from score -10.357063\n",
      "[LightGBM] [Info] Number of positive: 245, number of negative: 70701\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014550 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360232\n",
      "[LightGBM] [Info] Start training from score -10.360232\n",
      "[LightGBM] [Info] Number of positive: 178, number of negative: 70768\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015181 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.357734\n",
      "[LightGBM] [Info] Start training from score -10.357734\n",
      "[LightGBM] [Info] Number of positive: 419, number of negative: 70527\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013757 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361556\n",
      "[LightGBM] [Info] Start training from score -10.361556\n",
      "[LightGBM] [Info] Number of positive: 1165, number of negative: 69781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016273 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.354325\n",
      "[LightGBM] [Info] Start training from score -10.354325\n",
      "[LightGBM] [Info] Number of positive: 17116, number of negative: 53830\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014665 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000041 -> initscore=-10.096570\n",
      "[LightGBM] [Info] Start training from score -10.096570\n",
      "[LightGBM] [Info] Number of positive: 165, number of negative: 70781\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014892 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.356922\n",
      "[LightGBM] [Info] Start training from score -10.356922\n",
      "[LightGBM] [Info] Number of positive: 1844, number of negative: 69102\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012879 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.345249\n",
      "[LightGBM] [Info] Start training from score -10.345249\n",
      "[LightGBM] [Info] Number of positive: 5439, number of negative: 65507\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014084 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000034 -> initscore=-10.292618\n",
      "[LightGBM] [Info] Start training from score -10.292618\n",
      "[LightGBM] [Info] Number of positive: 834, number of negative: 70112\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014434 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358299\n",
      "[LightGBM] [Info] Start training from score -10.358299\n",
      "[LightGBM] [Info] Number of positive: 239, number of negative: 70707\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014627 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360087\n",
      "[LightGBM] [Info] Start training from score -10.360087\n",
      "[LightGBM] [Info] Number of positive: 7125, number of negative: 63821\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014412 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000035 -> initscore=-10.266640\n",
      "[LightGBM] [Info] Start training from score -10.266640\n",
      "[LightGBM] [Info] Number of positive: 3261, number of negative: 67685\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013288 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.325053\n",
      "[LightGBM] [Info] Start training from score -10.325053\n",
      "[LightGBM] [Info] Number of positive: 760, number of negative: 70186\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014053 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359094\n",
      "[LightGBM] [Info] Start training from score -10.359094\n",
      "[LightGBM] [Info] Number of positive: 2836, number of negative: 68110\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015182 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.331210\n",
      "[LightGBM] [Info] Start training from score -10.331210\n",
      "[LightGBM] [Info] Number of positive: 342, number of negative: 70604\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012810 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361448\n",
      "[LightGBM] [Info] Start training from score -10.361448\n",
      "[LightGBM] [Info] Number of positive: 467, number of negative: 70479\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014477 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361422\n",
      "[LightGBM] [Info] Start training from score -10.361422\n",
      "[LightGBM] [Info] Number of positive: 522, number of negative: 70424\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014747 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361144\n",
      "[LightGBM] [Info] Start training from score -10.361144\n",
      "[LightGBM] [Info] Number of positive: 697, number of negative: 70249\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014607 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359727\n",
      "[LightGBM] [Info] Start training from score -10.359727\n",
      "[LightGBM] [Info] Number of positive: 2950, number of negative: 67996\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015401 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.329566\n",
      "[LightGBM] [Info] Start training from score -10.329566\n",
      "[LightGBM] [Info] Number of positive: 216, number of negative: 70730\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015628 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359414\n",
      "[LightGBM] [Info] Start training from score -10.359414\n",
      "[LightGBM] [Info] Number of positive: 92, number of negative: 70854\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013980 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.347080\n",
      "[LightGBM] [Info] Start training from score -10.347080\n",
      "[LightGBM] [Info] Number of positive: 11921, number of negative: 59025\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013421 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000038 -> initscore=-10.188644\n",
      "[LightGBM] [Info] Start training from score -10.188644\n",
      "[LightGBM] [Info] Number of positive: 3493, number of negative: 67453\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014325 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.321665\n",
      "[LightGBM] [Info] Start training from score -10.321665\n",
      "[LightGBM] [Info] Number of positive: 4608, number of negative: 66338\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013378 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.305150\n",
      "[LightGBM] [Info] Start training from score -10.305150\n",
      "[LightGBM] [Info] Number of positive: 4504, number of negative: 66442\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012954 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.306706\n",
      "[LightGBM] [Info] Start training from score -10.306706\n",
      "[LightGBM] [Info] Number of positive: 1315, number of negative: 69631\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012194 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.352390\n",
      "[LightGBM] [Info] Start training from score -10.352390\n",
      "[LightGBM] [Info] Number of positive: 21933, number of negative: 49013\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.021555 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000045 -> initscore=-10.002853\n",
      "[LightGBM] [Info] Start training from score -10.002853\n",
      "[LightGBM] [Info] Number of positive: 225, number of negative: 70721\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013845 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359702\n",
      "[LightGBM] [Info] Start training from score -10.359702\n",
      "[LightGBM] [Info] Number of positive: 334, number of negative: 70612\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.023884 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361404\n",
      "[LightGBM] [Info] Start training from score -10.361404\n",
      "[LightGBM] [Info] Number of positive: 1748, number of negative: 69198\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016599 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.346571\n",
      "[LightGBM] [Info] Start training from score -10.346571\n",
      "[LightGBM] [Info] Number of positive: 528, number of negative: 70418\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012444 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361107\n",
      "[LightGBM] [Info] Start training from score -10.361107\n",
      "[LightGBM] [Info] Number of positive: 1304, number of negative: 69642\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014211 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.352534\n",
      "[LightGBM] [Info] Start training from score -10.352534\n",
      "[LightGBM] [Info] Number of positive: 344, number of negative: 70602\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014759 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361457\n",
      "[LightGBM] [Info] Start training from score -10.361457\n",
      "[LightGBM] [Info] Number of positive: 596, number of negative: 70350\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013964 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360622\n",
      "[LightGBM] [Info] Start training from score -10.360622\n",
      "[LightGBM] [Info] Number of positive: 2624, number of negative: 68322\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015150 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.334255\n",
      "[LightGBM] [Info] Start training from score -10.334255\n",
      "[LightGBM] [Info] Number of positive: 325, number of negative: 70621\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013492 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361347\n",
      "[LightGBM] [Info] Start training from score -10.361347\n",
      "[LightGBM] [Info] Number of positive: 302, number of negative: 70644\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.025151 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361149\n",
      "[LightGBM] [Info] Start training from score -10.361149\n",
      "[LightGBM] [Info] Number of positive: 1600, number of negative: 69346\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013398 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.348590\n",
      "[LightGBM] [Info] Start training from score -10.348590\n",
      "[LightGBM] [Info] Number of positive: 578, number of negative: 70368\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015106 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360762\n",
      "[LightGBM] [Info] Start training from score -10.360762\n",
      "[LightGBM] [Info] Number of positive: 1212, number of negative: 69734\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012659 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.353725\n",
      "[LightGBM] [Info] Start training from score -10.353725\n",
      "[LightGBM] [Info] Number of positive: 335, number of negative: 70611\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013349 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361410\n",
      "[LightGBM] [Info] Start training from score -10.361410\n",
      "[LightGBM] [Info] Number of positive: 3004, number of negative: 67942\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014319 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.328785\n",
      "[LightGBM] [Info] Start training from score -10.328785\n",
      "[LightGBM] [Info] Number of positive: 16363, number of negative: 54583\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013050 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000041 -> initscore=-10.110456\n",
      "[LightGBM] [Info] Start training from score -10.110456\n",
      "[LightGBM] [Info] Number of positive: 756, number of negative: 70190\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013346 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.359136\n",
      "[LightGBM] [Info] Start training from score -10.359136\n",
      "[LightGBM] [Info] Number of positive: 240, number of negative: 70706\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014706 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360112\n",
      "[LightGBM] [Info] Start training from score -10.360112\n",
      "[LightGBM] [Info] Number of positive: 1587, number of negative: 69359\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014470 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.348766\n",
      "[LightGBM] [Info] Start training from score -10.348766\n",
      "[LightGBM] [Info] Number of positive: 355, number of negative: 70591\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016118 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361502\n",
      "[LightGBM] [Info] Start training from score -10.361502\n",
      "[LightGBM] [Info] Number of positive: 529, number of negative: 70417\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013584 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361101\n",
      "[LightGBM] [Info] Start training from score -10.361101\n",
      "[LightGBM] [Info] Number of positive: 1728, number of negative: 69218\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012710 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.346845\n",
      "[LightGBM] [Info] Start training from score -10.346845\n",
      "[LightGBM] [Info] Number of positive: 1772, number of negative: 69174\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.025720 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.346241\n",
      "[LightGBM] [Info] Start training from score -10.346241\n",
      "[LightGBM] [Info] Number of positive: 834, number of negative: 70112\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013471 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.358299\n",
      "[LightGBM] [Info] Start training from score -10.358299\n",
      "[LightGBM] [Info] Number of positive: 2759, number of negative: 68187\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012620 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.332319\n",
      "[LightGBM] [Info] Start training from score -10.332319\n",
      "[LightGBM] [Info] Number of positive: 2227, number of negative: 68719\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013726 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.339898\n",
      "[LightGBM] [Info] Start training from score -10.339898\n",
      "[LightGBM] [Info] Number of positive: 4294, number of negative: 66652\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015392 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.309837\n",
      "[LightGBM] [Info] Start training from score -10.309837\n",
      "[LightGBM] [Info] Number of positive: 6642, number of negative: 64304\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014193 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000035 -> initscore=-10.274157\n",
      "[LightGBM] [Info] Start training from score -10.274157\n",
      "[LightGBM] [Info] Number of positive: 8139, number of negative: 62807\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014455 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000035 -> initscore=-10.250663\n",
      "[LightGBM] [Info] Start training from score -10.250663\n",
      "[LightGBM] [Info] Number of positive: 1253, number of negative: 69693\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014278 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.353197\n",
      "[LightGBM] [Info] Start training from score -10.353197\n",
      "[LightGBM] [Info] Number of positive: 17756, number of negative: 53190\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014075 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000042 -> initscore=-10.084615\n",
      "[LightGBM] [Info] Start training from score -10.084615\n",
      "[LightGBM] [Info] Number of positive: 2028, number of negative: 68918\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014270 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.342692\n",
      "[LightGBM] [Info] Start training from score -10.342692\n",
      "[LightGBM] [Info] Number of positive: 2449, number of negative: 68497\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015765 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.336753\n",
      "[LightGBM] [Info] Start training from score -10.336753\n",
      "[LightGBM] [Info] Number of positive: 13690, number of negative: 57256\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013664 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000039 -> initscore=-10.158239\n",
      "[LightGBM] [Info] Start training from score -10.158239\n",
      "[LightGBM] [Info] Number of positive: 3480, number of negative: 67466\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014868 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.321855\n",
      "[LightGBM] [Info] Start training from score -10.321855\n",
      "[LightGBM] [Info] Number of positive: 1544, number of negative: 69402\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014705 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.349347\n",
      "[LightGBM] [Info] Start training from score -10.349347\n",
      "[LightGBM] [Info] Number of positive: 6645, number of negative: 64301\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013927 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000035 -> initscore=-10.274110\n",
      "[LightGBM] [Info] Start training from score -10.274110\n",
      "[LightGBM] [Info] Number of positive: 1430, number of negative: 69516\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014010 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.350873\n",
      "[LightGBM] [Info] Start training from score -10.350873\n",
      "[LightGBM] [Info] Number of positive: 4073, number of negative: 66873\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016717 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.313119\n",
      "[LightGBM] [Info] Start training from score -10.313119\n",
      "[LightGBM] [Info] Number of positive: 9364, number of negative: 61582\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014616 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000036 -> initscore=-10.231002\n",
      "[LightGBM] [Info] Start training from score -10.231002\n",
      "[LightGBM] [Info] Number of positive: 1548, number of negative: 69398\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013542 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.349293\n",
      "[LightGBM] [Info] Start training from score -10.349293\n",
      "[LightGBM] [Info] Number of positive: 3904, number of negative: 67042\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013999 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.315620\n",
      "[LightGBM] [Info] Start training from score -10.315620\n",
      "[LightGBM] [Info] Number of positive: 2705, number of negative: 68241\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013636 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.333094\n",
      "[LightGBM] [Info] Start training from score -10.333094\n",
      "[LightGBM] [Info] Number of positive: 1643, number of negative: 69303\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012424 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.348006\n",
      "[LightGBM] [Info] Start training from score -10.348006\n",
      "[LightGBM] [Info] Number of positive: 6019, number of negative: 64927\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014339 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000034 -> initscore=-10.283764\n",
      "[LightGBM] [Info] Start training from score -10.283764\n",
      "[LightGBM] [Info] Number of positive: 3042, number of negative: 67904\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013730 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.328234\n",
      "[LightGBM] [Info] Start training from score -10.328234\n",
      "[LightGBM] [Info] Number of positive: 5618, number of negative: 65328\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012771 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000034 -> initscore=-10.289895\n",
      "[LightGBM] [Info] Start training from score -10.289895\n",
      "[LightGBM] [Info] Number of positive: 1415, number of negative: 69531\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015953 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.351073\n",
      "[LightGBM] [Info] Start training from score -10.351073\n",
      "[LightGBM] [Info] Number of positive: 2126, number of negative: 68820\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014290 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.341319\n",
      "[LightGBM] [Info] Start training from score -10.341319\n",
      "[LightGBM] [Info] Number of positive: 3927, number of negative: 67019\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015151 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.315280\n",
      "[LightGBM] [Info] Start training from score -10.315280\n",
      "[LightGBM] [Info] Number of positive: 16618, number of negative: 54328\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014447 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000041 -> initscore=-10.105775\n",
      "[LightGBM] [Info] Start training from score -10.105775\n",
      "[LightGBM] [Info] Number of positive: 6617, number of negative: 64329\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014294 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000034 -> initscore=-10.274544\n",
      "[LightGBM] [Info] Start training from score -10.274544\n",
      "[LightGBM] [Info] Number of positive: 1794, number of negative: 69152\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012869 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.345939\n",
      "[LightGBM] [Info] Start training from score -10.345939\n",
      "[LightGBM] [Info] Number of positive: 2791, number of negative: 68155\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013599 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.331858\n",
      "[LightGBM] [Info] Start training from score -10.331858\n",
      "[LightGBM] [Info] Number of positive: 976, number of negative: 69970\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013899 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.356660\n",
      "[LightGBM] [Info] Start training from score -10.356660\n",
      "[LightGBM] [Info] Number of positive: 7589, number of negative: 63357\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014127 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000035 -> initscore=-10.259362\n",
      "[LightGBM] [Info] Start training from score -10.259362\n",
      "[LightGBM] [Info] Number of positive: 13058, number of negative: 57888\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013676 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000038 -> initscore=-10.169209\n",
      "[LightGBM] [Info] Start training from score -10.169209\n",
      "[LightGBM] [Info] Number of positive: 6440, number of negative: 64506\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013091 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000034 -> initscore=-10.277283\n",
      "[LightGBM] [Info] Start training from score -10.277283\n",
      "[LightGBM] [Info] Number of positive: 6241, number of negative: 64705\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.021106 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000034 -> initscore=-10.280352\n",
      "[LightGBM] [Info] Start training from score -10.280352\n",
      "[LightGBM] [Info] Number of positive: 2551, number of negative: 68395\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012171 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.335299\n",
      "[LightGBM] [Info] Start training from score -10.335299\n",
      "[LightGBM] [Info] Number of positive: 3093, number of negative: 67853\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.015537 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000033 -> initscore=-10.327495\n",
      "[LightGBM] [Info] Start training from score -10.327495\n",
      "[LightGBM] [Info] Number of positive: 2204, number of negative: 68742\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.016255 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.340222\n",
      "[LightGBM] [Info] Start training from score -10.340222\n",
      "[LightGBM] [Info] Number of positive: 7839, number of negative: 63107\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013015 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000035 -> initscore=-10.255418\n",
      "[LightGBM] [Info] Start training from score -10.255418\n",
      "[LightGBM] [Info] Number of positive: 642, number of negative: 70304\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014969 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.360236\n",
      "[LightGBM] [Info] Start training from score -10.360236\n",
      "[LightGBM] [Info] Number of positive: 10469, number of negative: 60477\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.021831 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000037 -> initscore=-10.212920\n",
      "[LightGBM] [Info] Start training from score -10.212920\n",
      "[LightGBM] [Info] Number of positive: 6454, number of negative: 64492\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013662 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000034 -> initscore=-10.277066\n",
      "[LightGBM] [Info] Start training from score -10.277066\n",
      "[LightGBM] [Info] Number of positive: 369, number of negative: 70577\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.014477 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.361543\n",
      "[LightGBM] [Info] Start training from score -10.361543\n",
      "[LightGBM] [Info] Number of positive: 8636, number of negative: 62310\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013025 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000036 -> initscore=-10.242734\n",
      "[LightGBM] [Info] Start training from score -10.242734\n",
      "[LightGBM] [Info] Number of positive: 2353, number of negative: 68593\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.013724 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000032 -> initscore=-10.338116\n",
      "[LightGBM] [Info] Start training from score -10.338116\n",
      "[LightGBM] [Info] Number of positive: 9626, number of negative: 61320\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 263415\n",
      "[LightGBM] [Info] Number of data points in the train set: 70946, number of used features: 1033\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce RTX 3060, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 1033 dense feature groups (70.10 MB) transferred to GPU in 0.012480 secs. 0 sparse feature groups\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000036 -> initscore=-10.226745\n",
      "[LightGBM] [Info] Start training from score -10.226745\n",
      "Validation Accuracy: 0.0001, Validation F1 Score: 0.2312\n",
      "Test Accuracy: 0.0000, Test F1 Score: 0.2227\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Préparer les données pour LightGBM\n",
    "X_train_lgb, y_train_lgb = prepare_data_for_rf(X_train_final_train, train_targets.cpu().numpy())\n",
    "X_val_lgb, y_val_lgb = prepare_data_for_rf(X_train_final_val, val_targets.cpu().numpy())\n",
    "X_test_lgb, y_test_lgb = prepare_data_for_rf(X_test_final, y_test_tensor.cpu().numpy())\n",
    "\n",
    "# Initialiser les modèles LightGBM pour chaque label\n",
    "num_labels = y_train_lgb.shape[1]\n",
    "lgb_models = [lgb.LGBMClassifier(objective='binary', n_estimators=100, random_state=42, device='gpu') for _ in range(num_labels)]\n",
    "\n",
    "# Calculer les poids de classe\n",
    "class_weights = np.sum(y_train_lgb, axis=0) / y_train_lgb.shape[0]\n",
    "class_weights = 1 / class_weights\n",
    "class_weights = class_weights / np.sum(class_weights)\n",
    "\n",
    "# Entraîner les modèles\n",
    "for i in range(num_labels):\n",
    "    sample_weight = np.where(y_train_lgb[:, i] == 1, class_weights[i], 1 - class_weights[i])\n",
    "    lgb_models[i].fit(X_train_lgb, y_train_lgb[:, i], sample_weight=sample_weight)\n",
    "\n",
    "# Évaluer les modèles sur l'ensemble de validation\n",
    "y_val_pred_lgb = np.zeros_like(y_val_lgb)\n",
    "for i in range(num_labels):\n",
    "    y_val_pred_lgb[:, i] = lgb_models[i].predict(X_val_lgb)\n",
    "y_val_pred_lgb = (y_val_pred_lgb > 0.5).astype(int)\n",
    "val_accuracy = accuracy_score(y_val_lgb, y_val_pred_lgb)\n",
    "val_f1 = f1_score(y_val_lgb, y_val_pred_lgb, average='weighted')\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# Évaluer les modèles sur l'ensemble de test\n",
    "y_test_pred_lgb = np.zeros_like(y_test_lgb)\n",
    "for i in range(num_labels):\n",
    "    y_test_pred_lgb[:, i] = lgb_models[i].predict(X_test_lgb)\n",
    "y_test_pred_lgb = (y_test_pred_lgb > 0.5).astype(int)\n",
    "test_accuracy = accuracy_score(y_test_lgb, y_test_pred_lgb)\n",
    "test_f1 = f1_score(y_test_lgb, y_test_pred_lgb, average='weighted')\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def prepare_data_for_rf(X_train_final, y_train):\n",
    "    X_train_concat = np.concatenate(\n",
    "        [\n",
    "            X_train_final[\"X_genres\"].cpu().numpy(),\n",
    "            X_train_final[\"X_instruments\"].cpu().numpy(),\n",
    "            X_train_final[\"X_moods\"].cpu().numpy(),\n",
    "            X_train_final[\"X_genres_categories\"].cpu().numpy(),\n",
    "            X_train_final[\"X_instruments_categories\"].cpu().numpy(),\n",
    "            X_train_final[\"X_moods_categories\"].cpu().numpy(),\n",
    "            # X_train_final[\"y_genres_pred\"].cpu().numpy(),\n",
    "            # X_train_final[\"y_instruments_pred\"].cpu().numpy(),\n",
    "            # X_train_final[\"y_moods_pred\"].cpu().numpy(),\n",
    "            # X_train_final[\"y_genres_instruments_pred\"].cpu().numpy(),\n",
    "            # X_train_final[\"y_genres_moods_pred\"].cpu().numpy(),\n",
    "            # X_train_final[\"y_instruments_moods_pred\"].cpu().numpy(),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Normaliser les données\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_concat)\n",
    "\n",
    "    # Convertir y_train en format binaire\n",
    "    lb = LabelBinarizer()\n",
    "    y_train_bin = lb.fit_transform(y_train)\n",
    "\n",
    "    # Appliquer le rééchantillonnage pour chaque label binaire\n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled_list = []\n",
    "    y_train_resampled_list = []\n",
    "\n",
    "    for i in range(y_train_bin.shape[1]):\n",
    "        X_resampled, y_resampled = ros.fit_resample(X_train_scaled, y_train_bin[:, i])\n",
    "        X_train_resampled_list.append(X_resampled)\n",
    "        y_train_resampled_list.append(y_resampled.reshape(-1, 1))\n",
    "\n",
    "    # Concaténer les données rééchantillonnées\n",
    "    X_train_resampled = np.concatenate(X_train_resampled_list, axis=0)\n",
    "    y_train_resampled = np.concatenate(y_train_resampled_list, axis=0)\n",
    "\n",
    "    return X_train_resampled, y_train_resampled\n",
    "\n",
    "# Préparer les données pour LightGBM\n",
    "X_train_lgb, y_train_lgb = prepare_data_for_rf(X_train_final_train, train_targets.cpu().numpy())\n",
    "X_val_lgb, y_val_lgb = prepare_data_for_rf(X_train_final_val, val_targets.cpu().numpy())\n",
    "X_test_lgb, y_test_lgb = prepare_data_for_rf(X_test_final, y_test_tensor.cpu().numpy())\n",
    "\n",
    "# Initialiser les modèles LightGBM pour chaque label\n",
    "num_labels = y_train_lgb.shape[1]\n",
    "lgb_models = [lgb.LGBMClassifier(objective='binary', n_estimators=100, random_state=42) for _ in range(num_labels)]\n",
    "\n",
    "# Entraîner les modèles\n",
    "for i in range(num_labels):\n",
    "    lgb_models[i].fit(X_train_lgb, y_train_lgb[:, i])\n",
    "\n",
    "# Évaluer les modèles sur l'ensemble de validation\n",
    "y_val_pred_lgb = np.zeros_like(y_val_lgb)\n",
    "for i in range(num_labels):\n",
    "    y_val_pred_lgb[:, i] = lgb_models[i].predict(X_val_lgb)\n",
    "y_val_pred_lgb = (y_val_pred_lgb > 0.5).astype(int)\n",
    "val_accuracy = accuracy_score(y_val_lgb, y_val_pred_lgb)\n",
    "val_f1 = f1_score(y_val_lgb, y_val_pred_lgb, average='weighted')\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# Évaluer les modèles sur l'ensemble de test\n",
    "y_test_pred_lgb = np.zeros_like(y_test_lgb)\n",
    "for i in range(num_labels):\n",
    "    y_test_pred_lgb[:, i] = lgb_models[i].predict(X_test_lgb)\n",
    "y_test_pred_lgb = (y_test_pred_lgb > 0.5).astype(int)\n",
    "test_accuracy = accuracy_score(y_test_lgb, y_test_pred_lgb)\n",
    "test_f1 = f1_score(y_test_lgb, y_test_pred_lgb, average='weighted')\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_train_resampled, y_train_resampled\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Préparer les données pour LightGBM\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m X_train_lgb, y_train_lgb \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data_for_rf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_final_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_targets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m X_val_lgb, y_val_lgb \u001b[38;5;241m=\u001b[39m prepare_data_for_rf(X_train_final_val, val_targets\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     53\u001b[0m X_test_lgb, y_test_lgb \u001b[38;5;241m=\u001b[39m prepare_data_for_rf(X_test_final, y_test_tensor\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "Cell \u001b[0;32mIn[14], line 40\u001b[0m, in \u001b[0;36mprepare_data_for_rf\u001b[0;34m(X_train_final, y_train)\u001b[0m\n\u001b[1;32m     37\u001b[0m y_train_resampled_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(y_train_bin\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m---> 40\u001b[0m     X_resampled, y_resampled \u001b[38;5;241m=\u001b[39m \u001b[43msmote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_bin\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     X_train_resampled_list\u001b[38;5;241m.\u001b[39mappend(X_resampled)\n\u001b[1;32m     42\u001b[0m     y_train_resampled_list\u001b[38;5;241m.\u001b[39mappend(y_resampled\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/imblearn/base.py:202\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_resample\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m        The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/imblearn/base.py:105\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m     99\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[1;32m    103\u001b[0m )\n\u001b[0;32m--> 105\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    109\u001b[0m )\n\u001b[1;32m    111\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m arrays_transformer\u001b[38;5;241m.\u001b[39mtransform(output[\u001b[38;5;241m0\u001b[39m], y_)\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/imblearn/over_sampling/_smote/base.py:360\u001b[0m, in \u001b[0;36mSMOTE._fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_k_\u001b[38;5;241m.\u001b[39mfit(X_class)\n\u001b[1;32m    359\u001b[0m nns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_k_\u001b[38;5;241m.\u001b[39mkneighbors(X_class, return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 360\u001b[0m X_new, y_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_samples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m X_resampled\u001b[38;5;241m.\u001b[39mappend(X_new)\n\u001b[1;32m    364\u001b[0m y_resampled\u001b[38;5;241m.\u001b[39mappend(y_new)\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/imblearn/over_sampling/_smote/base.py:118\u001b[0m, in \u001b[0;36mBaseSMOTE._make_samples\u001b[0;34m(self, X, y_dtype, y_type, nn_data, nn_num, n_samples, step_size, y)\u001b[0m\n\u001b[1;32m    115\u001b[0m rows \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloor_divide(samples_indices, nn_num\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    116\u001b[0m cols \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmod(samples_indices, nn_num\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 118\u001b[0m X_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m y_new \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull(n_samples, fill_value\u001b[38;5;241m=\u001b[39my_type, dtype\u001b[38;5;241m=\u001b[39my_dtype)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_new, y_new\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/imblearn/over_sampling/_smote/base.py:187\u001b[0m, in \u001b[0;36mBaseSMOTE._generate_samples\u001b[0;34m(self, X, nn_data, nn_num, rows, cols, steps, y_type, y)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     X_new \u001b[38;5;241m=\u001b[39m X[rows] \u001b[38;5;241m+\u001b[39m steps \u001b[38;5;241m*\u001b[39m diffs\n\u001b[0;32m--> 187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mX_new\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def prepare_data_for_rf(X_train_final, y_train):\n",
    "    X_train_concat = np.concatenate(\n",
    "        [\n",
    "            X_train_final[\"X_genres\"].cpu().numpy(),\n",
    "            X_train_final[\"X_instruments\"].cpu().numpy(),\n",
    "            X_train_final[\"X_moods\"].cpu().numpy(),\n",
    "            X_train_final[\"X_genres_categories\"].cpu().numpy(),\n",
    "            X_train_final[\"X_instruments_categories\"].cpu().numpy(),\n",
    "            X_train_final[\"X_moods_categories\"].cpu().numpy(),\n",
    "            # X_train_final[\"y_genres_pred\"].cpu().numpy(),\n",
    "            # X_train_final[\"y_instruments_pred\"].cpu().numpy(),\n",
    "            # X_train_final[\"y_moods_pred\"].cpu().numpy(),\n",
    "            # X_train_final[\"y_genres_instruments_pred\"].cpu().numpy(),\n",
    "            # X_train_final[\"y_genres_moods_pred\"].cpu().numpy(),\n",
    "            # X_train_final[\"y_instruments_moods_pred\"].cpu().numpy(),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Normaliser les données\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_concat)\n",
    "\n",
    "    # Convertir y_train en format binaire\n",
    "    lb = LabelBinarizer()\n",
    "    y_train_bin = lb.fit_transform(y_train)\n",
    "\n",
    "    # Appliquer le rééchantillonnage pour chaque label binaire\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled_list = []\n",
    "    y_train_resampled_list = []\n",
    "\n",
    "    for i in range(y_train_bin.shape[1]):\n",
    "        X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train_bin[:, i])\n",
    "        X_train_resampled_list.append(X_resampled)\n",
    "        y_train_resampled_list.append(y_resampled.reshape(-1, 1))\n",
    "\n",
    "    # Concaténer les données rééchantillonnées\n",
    "    X_train_resampled = np.concatenate(X_train_resampled_list, axis=0)\n",
    "    y_train_resampled = np.concatenate(y_train_resampled_list, axis=0)\n",
    "\n",
    "    return X_train_resampled, y_train_resampled\n",
    "\n",
    "# Préparer les données pour LightGBM\n",
    "X_train_lgb, y_train_lgb = prepare_data_for_rf(X_train_final_train, train_targets.cpu().numpy())\n",
    "X_val_lgb, y_val_lgb = prepare_data_for_rf(X_train_final_val, val_targets.cpu().numpy())\n",
    "X_test_lgb, y_test_lgb = prepare_data_for_rf(X_test_final, y_test_tensor.cpu().numpy())\n",
    "\n",
    "# Initialiser les modèles LightGBM pour chaque label\n",
    "num_labels = y_train_lgb.shape[1]\n",
    "lgb_models = [lgb.LGBMClassifier(objective='binary', n_estimators=100, random_state=42) for _ in range(num_labels)]\n",
    "\n",
    "# Entraîner les modèles par mini-batchs\n",
    "batch_size = 1000\n",
    "for i in range(num_labels):\n",
    "    for start in range(0, len(X_train_lgb), batch_size):\n",
    "        end = start + batch_size\n",
    "        lgb_models[i].fit(X_train_lgb[start:end], y_train_lgb[start:end, i], init_score=lgb_models[i].best_score_)\n",
    "\n",
    "# Évaluer les modèles sur l'ensemble de validation\n",
    "y_val_pred_lgb = np.zeros_like(y_val_lgb)\n",
    "for i in range(num_labels):\n",
    "    y_val_pred_lgb[:, i] = lgb_models[i].predict(X_val_lgb)\n",
    "y_val_pred_lgb = (y_val_pred_lgb > 0.5).astype(int)\n",
    "val_accuracy = accuracy_score(y_val_lgb, y_val_pred_lgb)\n",
    "val_f1 = f1_score(y_val_lgb, y_val_pred_lgb, average='weighted')\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# Évaluer les modèles sur l'ensemble de test\n",
    "y_test_pred_lgb = np.zeros_like(y_test_lgb)\n",
    "for i in range(num_labels):\n",
    "    y_test_pred_lgb[:, i] = lgb_models[i].predict(X_test_lgb)\n",
    "y_test_pred_lgb = (y_test_pred_lgb > 0.5).astype(int)\n",
    "test_accuracy = accuracy_score(y_test_lgb, y_test_pred_lgb)\n",
    "test_f1 = f1_score(y_test_lgb, y_test_pred_lgb, average='weighted')\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Entraîner les modèles\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_labels):\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mxgb_models\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_xgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_xgb\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Évaluer les modèles sur l'ensemble de validation\u001b[39;00m\n\u001b[1;32m     19\u001b[0m y_val_pred_xgb \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(y_val_xgb)\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/xgboost/sklearn.py:1531\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[1;32m   1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1513\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1514\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1528\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1529\u001b[0m )\n\u001b[0;32m-> 1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/xgboost/core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2100\u001b[0m     _check_call(\n\u001b[0;32m-> 2101\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2104\u001b[0m     )\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Préparer les données pour XGBoost\n",
    "X_train_xgb, y_train_xgb = prepare_data_for_rf(X_train_final_train, train_targets.cpu().numpy())\n",
    "X_val_xgb, y_val_xgb = prepare_data_for_rf(X_train_final_val, val_targets.cpu().numpy())\n",
    "X_test_xgb, y_test_xgb = prepare_data_for_rf(X_test_final, y_test_tensor.cpu().numpy())\n",
    "\n",
    "# Initialiser les modèles XGBoost pour chaque label\n",
    "num_labels = y_train_xgb.shape[1]\n",
    "xgb_models = [xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, random_state=42) for _ in range(num_labels)]\n",
    "\n",
    "# Entraîner les modèles\n",
    "for i in range(num_labels):\n",
    "    xgb_models[i].fit(X_train_xgb, y_train_xgb[:, i])\n",
    "\n",
    "# Évaluer les modèles sur l'ensemble de validation\n",
    "y_val_pred_xgb = np.zeros_like(y_val_xgb)\n",
    "for i in range(num_labels):\n",
    "    y_val_pred_xgb[:, i] = xgb_models[i].predict(X_val_xgb)\n",
    "y_val_pred_xgb = (y_val_pred_xgb > 0.5).astype(int)\n",
    "val_accuracy = accuracy_score(y_val_xgb, y_val_pred_xgb)\n",
    "val_f1 = f1_score(y_val_xgb, y_val_pred_xgb, average='weighted')\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}, Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# Évaluer les modèles sur l'ensemble de test\n",
    "y_test_pred_xgb = np.zeros_like(y_test_xgb)\n",
    "for i in range(num_labels):\n",
    "    y_test_pred_xgb[:, i] = xgb_models[i].predict(X_test_xgb)\n",
    "y_test_pred_xgb = (y_test_pred_xgb > 0.5).astype(int)\n",
    "test_accuracy = accuracy_score(y_test_xgb, y_test_pred_xgb)\n",
    "test_f1 = f1_score(y_test_xgb, y_test_pred_xgb, average='weighted')\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}, Test F1 Score: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de paramètres : 10185080\n"
     ]
    }
   ],
   "source": [
    "# Modèle de Transformeur\n",
    "class MultiTaskTransformer(nn.Module):\n",
    "    def __init__(self, embedding_size, num_heads, num_layers, num_labels, dropout):\n",
    "        super(MultiTaskTransformer, self).__init__()\n",
    "        self.embedding_genres = nn.Linear(input_size_dict[\"X_genres\"], embedding_size)\n",
    "        self.embedding_instruments = nn.Linear(\n",
    "            input_size_dict[\"X_instruments\"], embedding_size\n",
    "        )\n",
    "        self.embedding_moods = nn.Linear(input_size_dict[\"X_moods\"], embedding_size)\n",
    "        self.embedding_y_genres_pred = nn.Linear(\n",
    "            input_size_dict[\"y_genres_pred\"], embedding_size\n",
    "        )\n",
    "        self.embedding_y_instruments_pred = nn.Linear(\n",
    "            input_size_dict[\"y_instruments_pred\"], embedding_size\n",
    "        )\n",
    "        self.embedding_y_moods_pred = nn.Linear(\n",
    "            input_size_dict[\"y_moods_pred\"], embedding_size\n",
    "        )\n",
    "        self.embedding_y_genres_instruments_pred = nn.Linear(\n",
    "            input_size_dict[\"y_genres_instruments_pred\"], embedding_size\n",
    "        )\n",
    "        self.embedding_y_genres_moods_pred = nn.Linear(\n",
    "            input_size_dict[\"y_genres_moods_pred\"], embedding_size\n",
    "        )\n",
    "        self.embedding_y_instruments_moods_pred = nn.Linear(\n",
    "            input_size_dict[\"y_instruments_moods_pred\"], embedding_size\n",
    "        )\n",
    "        self.embedding_genres_categories = nn.Linear(\n",
    "            input_size_dict[\"X_genres_categories\"], embedding_size\n",
    "        )\n",
    "        self.embedding_instruments_categories = nn.Linear(\n",
    "            input_size_dict[\"X_instruments_categories\"], embedding_size\n",
    "        )\n",
    "        self.embedding_moods_categories = nn.Linear(\n",
    "            input_size_dict[\"X_moods_categories\"], embedding_size\n",
    "        )\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embedding_size,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.classifier = nn.Linear(embedding_size, num_labels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_genres,\n",
    "        x_instruments,\n",
    "        x_moods,\n",
    "        x_genres_categories,\n",
    "        x_instruments_categories,\n",
    "        x_moods_categories,\n",
    "        y_genres_pred,\n",
    "        y_instruments_pred,\n",
    "        y_moods_pred,\n",
    "        y_genres_instruments_pred,\n",
    "        y_genres_moods_pred,\n",
    "        y_instruments_moods_pred,\n",
    "    ):\n",
    "        embedded_genres = self.embedding_genres(x_genres)\n",
    "        embedded_instruments = self.embedding_instruments(x_instruments)\n",
    "        embedded_moods = self.embedding_moods(x_moods)\n",
    "        embedded_y_genres_pred = self.embedding_y_genres_pred(y_genres_pred)\n",
    "        embedded_y_instruments_pred = self.embedding_y_instruments_pred(\n",
    "            y_instruments_pred\n",
    "        )\n",
    "        embedded_y_moods_pred = self.embedding_y_moods_pred(y_moods_pred)\n",
    "        embedded_y_genres_instruments_pred = self.embedding_y_genres_instruments_pred(\n",
    "            y_genres_instruments_pred\n",
    "        )\n",
    "        embedded_y_genres_moods_pred = self.embedding_y_genres_moods_pred(\n",
    "            y_genres_moods_pred\n",
    "        )\n",
    "        embedded_y_instruments_moods_pred = self.embedding_y_instruments_moods_pred(\n",
    "            y_instruments_moods_pred\n",
    "        )\n",
    "        embedded_genres_categories = self.embedding_genres_categories(\n",
    "            x_genres_categories\n",
    "        )\n",
    "        embedded_instruments_categories = self.embedding_instruments_categories(\n",
    "            x_instruments_categories\n",
    "        )\n",
    "        embedded_moods_categories = self.embedding_moods_categories(x_moods_categories)\n",
    "\n",
    "        sequence = torch.stack(\n",
    "            [\n",
    "                embedded_genres,\n",
    "                embedded_instruments,\n",
    "                embedded_moods,\n",
    "                embedded_genres_categories,\n",
    "                embedded_instruments_categories,\n",
    "                embedded_moods_categories,\n",
    "                embedded_y_genres_pred,\n",
    "                embedded_y_instruments_pred,\n",
    "                embedded_y_moods_pred,\n",
    "                embedded_y_genres_instruments_pred,\n",
    "                embedded_y_genres_moods_pred,\n",
    "                embedded_y_instruments_moods_pred,\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        transformer_output = self.transformer(sequence, sequence)\n",
    "        output = transformer_output.mean(dim=1)\n",
    "        predictions = self.classifier(output)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Initialiser le modèle\n",
    "model = MultiTaskTransformer(\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_labels=y_train.shape[1],\n",
    "    dropout=DROPOUT,\n",
    ").to(DEVICE)\n",
    "\n",
    "# Affichage du nombre de paramètres\n",
    "print(f\"Nombre total de paramètres : {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Optimiseur et fonction de perte\n",
    "criterion = nn.BCEWithLogitsLoss()  # Fonction de perte pour les étiquettes binaires\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "        self.attention_weights = nn.Linear(embedding_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, sequence_length, embedding_size)\n",
    "        weights = torch.softmax(\n",
    "            self.attention_weights(x), dim=1\n",
    "        )  # (batch_size, sequence_length, 1)\n",
    "        context = torch.sum(\n",
    "            weights * x, dim=1\n",
    "        )  # Weighted sum, (batch_size, embedding_size)\n",
    "        return context\n",
    "\n",
    "\n",
    "class GlobalPooling(nn.Module):\n",
    "    def __init__(self, embedding_size):\n",
    "        super(GlobalPooling, self).__init__()\n",
    "        self.attention_pooling = AttentionPooling(embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, sequence_length, embedding_size)\n",
    "        attention_output = self.attention_pooling(x)  # Attention pooling\n",
    "        max_output, _ = torch.max(x, dim=1)  # Max pooling\n",
    "        return attention_output + max_output  # Combine both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de paramètres : 4968761\n"
     ]
    }
   ],
   "source": [
    "class MultiTaskTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size_dict,\n",
    "        embedding_size,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        num_labels,\n",
    "        dropout,\n",
    "    ):\n",
    "        super(MultiTaskTransformer, self).__init__()\n",
    "\n",
    "        # Parallel input embeddings\n",
    "        self.embeddings = nn.ModuleDict(\n",
    "            {\n",
    "                \"genres\": nn.Sequential(\n",
    "                    nn.Linear(input_size_dict[\"X_genres\"], embedding_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                ),\n",
    "                \"instruments\": nn.Sequential(\n",
    "                    nn.Linear(input_size_dict[\"X_instruments\"], embedding_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                ),\n",
    "                \"moods\": nn.Sequential(\n",
    "                    nn.Linear(input_size_dict[\"X_moods\"], embedding_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Prediction embeddings\n",
    "        self.prediction_embeddings = nn.ModuleDict(\n",
    "            {\n",
    "                \"y_genres_pred\": nn.Sequential(\n",
    "                    nn.Linear(input_size_dict[\"y_genres_pred\"], embedding_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                ),\n",
    "                \"y_instruments_pred\": nn.Sequential(\n",
    "                    nn.Linear(input_size_dict[\"y_instruments_pred\"], embedding_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                ),\n",
    "                \"y_moods_pred\": nn.Sequential(\n",
    "                    nn.Linear(input_size_dict[\"y_moods_pred\"], embedding_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                ),\n",
    "                \"y_genres_instruments_pred\": nn.Sequential(\n",
    "                    nn.Linear(\n",
    "                        input_size_dict[\"y_genres_instruments_pred\"], embedding_size\n",
    "                    ),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                ),\n",
    "                \"y_genres_moods_pred\": nn.Sequential(\n",
    "                    nn.Linear(input_size_dict[\"y_genres_moods_pred\"], embedding_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                ),\n",
    "                \"y_instruments_moods_pred\": nn.Sequential(\n",
    "                    nn.Linear(\n",
    "                        input_size_dict[\"y_instruments_moods_pred\"], embedding_size\n",
    "                    ),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Categories embeddings\n",
    "        self.categories_embeddings = nn.ModuleDict(\n",
    "            {\n",
    "                \"genres\": nn.Sequential(\n",
    "                    nn.Linear(input_size_dict[\"X_genres_categories\"], embedding_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                ),\n",
    "                \"instruments\": nn.Sequential(\n",
    "                    nn.Linear(\n",
    "                        input_size_dict[\"X_instruments_categories\"], embedding_size\n",
    "                    ),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                ),\n",
    "                \"moods\": nn.Sequential(\n",
    "                    nn.Linear(input_size_dict[\"X_moods_categories\"], embedding_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Cross-attention mechanism\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=embedding_size,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        # Transformer encoder\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(\n",
    "                d_model=embedding_size,\n",
    "                nhead=num_heads,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            ),\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "\n",
    "        # LayerNorm\n",
    "        self.layer_norm = nn.LayerNorm(embedding_size)\n",
    "\n",
    "        # Global pooling\n",
    "        self.global_pooling = GlobalPooling(embedding_size)\n",
    "\n",
    "        # Final classification layers\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_size, embedding_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embedding_size // 2, num_labels),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_genres,\n",
    "        x_instruments,\n",
    "        x_moods,\n",
    "        x_genres_categories,\n",
    "        x_instruments_categories,\n",
    "        x_moods_categories,\n",
    "        y_genres_pred,\n",
    "        y_instruments_pred,\n",
    "        y_moods_pred,\n",
    "        y_genres_instruments_pred,\n",
    "        y_genres_moods_pred,\n",
    "        y_instruments_moods_pred,\n",
    "    ):\n",
    "        # Embed input features\n",
    "        embedded_inputs = torch.stack(\n",
    "            [\n",
    "                self.embeddings[\"genres\"](x_genres),\n",
    "                self.embeddings[\"instruments\"](x_instruments),\n",
    "                self.embeddings[\"moods\"](x_moods),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        # Embed prediction features\n",
    "        embedded_predictions = torch.stack(\n",
    "            [\n",
    "                self.prediction_embeddings[\"y_genres_pred\"](y_genres_pred),\n",
    "                self.prediction_embeddings[\"y_instruments_pred\"](y_instruments_pred),\n",
    "                self.prediction_embeddings[\"y_moods_pred\"](y_moods_pred),\n",
    "                self.prediction_embeddings[\"y_genres_instruments_pred\"](\n",
    "                    y_genres_instruments_pred\n",
    "                ),\n",
    "                self.prediction_embeddings[\"y_genres_moods_pred\"](y_genres_moods_pred),\n",
    "                self.prediction_embeddings[\"y_instruments_moods_pred\"](\n",
    "                    y_instruments_moods_pred\n",
    "                ),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        # Embed categories\n",
    "        embedded_categories = torch.stack(\n",
    "            [\n",
    "                self.categories_embeddings[\"genres\"](x_genres_categories),\n",
    "                self.categories_embeddings[\"instruments\"](x_instruments_categories),\n",
    "                self.categories_embeddings[\"moods\"](x_moods_categories),\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        # Combine inputs\n",
    "        combined_sequence = torch.cat(\n",
    "            [embedded_inputs, embedded_predictions, embedded_categories], dim=1\n",
    "        )\n",
    "        combined_sequence = self.layer_norm(combined_sequence)\n",
    "\n",
    "        # Apply cross-attention\n",
    "        attended_sequence, _ = self.cross_attention(\n",
    "            combined_sequence, combined_sequence, combined_sequence\n",
    "        )\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoded_sequence = self.transformer_encoder(attended_sequence)\n",
    "\n",
    "        # Global pooling\n",
    "        pooled_output = self.global_pooling(encoded_sequence)\n",
    "\n",
    "        # Final classification\n",
    "        predictions = self.classifier(pooled_output)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "\n",
    "model = MultiTaskTransformer(\n",
    "    input_size_dict=input_size_dict,\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_labels=y_train.shape[1],\n",
    "    dropout=DROPOUT,\n",
    ").to(DEVICE)\n",
    "\n",
    "# Affichage du nombre de paramètres\n",
    "print(f\"Nombre total de paramètres : {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Optimizer and loss\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, epochs, patience\n",
    "):\n",
    "    \"\"\"\n",
    "    Entraîne un modèle avec early stopping.\n",
    "\n",
    "    Args:\n",
    "        model: Le modèle à entraîner.\n",
    "        train_loader: DataLoader pour les données d'entraînement.\n",
    "        val_loader: DataLoader pour les données de validation.\n",
    "        criterion: Fonction de perte.\n",
    "        optimizer: Optimiseur.\n",
    "        epochs: Nombre maximum d'époques.\n",
    "        patience: Nombre d'époques à attendre pour une amélioration avant d'arrêter l'entraînement.\n",
    "\n",
    "    Returns:\n",
    "        Le meilleur modèle basé sur les performances sur l'ensemble de validation.\n",
    "    \"\"\"\n",
    "    best_loss = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Mode entraînement\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for (\n",
    "            x_genres_batch,\n",
    "            x_instruments_batch,\n",
    "            x_moods_batch,\n",
    "            x_genres_categories_batch,\n",
    "            x_instruments_categories_batch,\n",
    "            x_moods_categories_batch,\n",
    "            y_genres_pred_batch,\n",
    "            y_instruments_pred_batch,\n",
    "            y_moods_pred_batch,\n",
    "            y_genres_instruments_pred_batch,\n",
    "            y_genres_moods_pred_batch,\n",
    "            y_instruments_moods_pred_batch,\n",
    "            y_batch,\n",
    "        ) in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(\n",
    "                x_genres_batch,\n",
    "                x_instruments_batch,\n",
    "                x_moods_batch,\n",
    "                x_genres_categories_batch,\n",
    "                x_instruments_categories_batch,\n",
    "                x_moods_categories_batch,\n",
    "                y_genres_pred_batch,\n",
    "                y_instruments_pred_batch,\n",
    "                y_moods_pred_batch,\n",
    "                y_genres_instruments_pred_batch,\n",
    "                y_genres_moods_pred_batch,\n",
    "                y_instruments_moods_pred_batch,\n",
    "            )\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # Mode validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for (\n",
    "                x_genres_batch,\n",
    "                x_instruments_batch,\n",
    "                x_moods_batch,\n",
    "                x_genres_categories_batch,\n",
    "                x_instruments_categories_batch,\n",
    "                x_moods_categories_batch,\n",
    "                y_genres_pred_batch,\n",
    "                y_instruments_pred_batch,\n",
    "                y_moods_pred_batch,\n",
    "                y_genres_instruments_pred_batch,\n",
    "                y_genres_moods_pred_batch,\n",
    "                y_instruments_moods_pred_batch,\n",
    "                y_batch,\n",
    "            ) in val_loader:\n",
    "                predictions = model(\n",
    "                    x_genres_batch,\n",
    "                    x_instruments_batch,\n",
    "                    x_moods_batch,\n",
    "                    x_genres_categories_batch,\n",
    "                    x_instruments_categories_batch,\n",
    "                    x_moods_categories_batch,\n",
    "                    y_genres_pred_batch,\n",
    "                    y_instruments_pred_batch,\n",
    "                    y_moods_pred_batch,\n",
    "                    y_genres_instruments_pred_batch,\n",
    "                    y_genres_moods_pred_batch,\n",
    "                    y_instruments_moods_pred_batch,\n",
    "                )\n",
    "                loss = criterion(predictions, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\n",
    "                f\"Early stopping triggered at epoch {epoch + 1}. Best validation loss: {best_loss:.4f}\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "    # Charger le meilleur modèle\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Évaluation\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for (\n",
    "            x_genres_batch,\n",
    "            x_instruments_batch,\n",
    "            x_moods_batch,\n",
    "            x_genres_categories_batch,\n",
    "            x_instruments_categories_batch,\n",
    "            x_moods_categories_batch,\n",
    "            y_genres_pred_batch,\n",
    "            y_instruments_pred_batch,\n",
    "            y_moods_pred_batch,\n",
    "            y_genres_instruments_pred_batch,\n",
    "            y_genres_moods_pred_batch,\n",
    "            y_instruments_moods_pred_batch,\n",
    "            y_batch,\n",
    "        ) in test_loader:\n",
    "            predictions = model(\n",
    "                x_genres_batch,\n",
    "                x_instruments_batch,\n",
    "                x_moods_batch,\n",
    "                x_genres_categories_batch,\n",
    "                x_instruments_categories_batch,\n",
    "                x_moods_categories_batch,\n",
    "                y_genres_pred_batch,\n",
    "                y_instruments_pred_batch,\n",
    "                y_moods_pred_batch,\n",
    "                y_genres_instruments_pred_batch,\n",
    "                y_genres_moods_pred_batch,\n",
    "                y_instruments_moods_pred_batch,\n",
    "            )\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            total_loss += loss.item()\n",
    "    print(f\"Test Loss: {total_loss / len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Train Loss: 0.1699, Val Loss: 0.0981\n",
      "Epoch 2/30, Train Loss: 0.0994, Val Loss: 0.0915\n",
      "Epoch 3/30, Train Loss: 0.0943, Val Loss: 0.0884\n",
      "Epoch 4/30, Train Loss: 0.0917, Val Loss: 0.0876\n",
      "Epoch 5/30, Train Loss: 0.0900, Val Loss: 0.0864\n",
      "Epoch 6/30, Train Loss: 0.0887, Val Loss: 0.0856\n",
      "Epoch 7/30, Train Loss: 0.0877, Val Loss: 0.0848\n",
      "Epoch 8/30, Train Loss: 0.0868, Val Loss: 0.0842\n",
      "Epoch 9/30, Train Loss: 0.0860, Val Loss: 0.0840\n",
      "Epoch 10/30, Train Loss: 0.0854, Val Loss: 0.0837\n",
      "Epoch 11/30, Train Loss: 0.0848, Val Loss: 0.0834\n",
      "Epoch 12/30, Train Loss: 0.0842, Val Loss: 0.0828\n",
      "Epoch 13/30, Train Loss: 0.0837, Val Loss: 0.0827\n",
      "Epoch 14/30, Train Loss: 0.0833, Val Loss: 0.0825\n",
      "Epoch 15/30, Train Loss: 0.0828, Val Loss: 0.0823\n",
      "Epoch 16/30, Train Loss: 0.0824, Val Loss: 0.0820\n",
      "Epoch 17/30, Train Loss: 0.0820, Val Loss: 0.0820\n",
      "Epoch 18/30, Train Loss: 0.0816, Val Loss: 0.0820\n",
      "Epoch 19/30, Train Loss: 0.0812, Val Loss: 0.0820\n",
      "Epoch 20/30, Train Loss: 0.0809, Val Loss: 0.0816\n",
      "Epoch 21/30, Train Loss: 0.0805, Val Loss: 0.0821\n",
      "Epoch 22/30, Train Loss: 0.0801, Val Loss: 0.0818\n",
      "Epoch 23/30, Train Loss: 0.0797, Val Loss: 0.0815\n",
      "Epoch 24/30, Train Loss: 0.0794, Val Loss: 0.0815\n",
      "Epoch 25/30, Train Loss: 0.0789, Val Loss: 0.0815\n",
      "Epoch 26/30, Train Loss: 0.0785, Val Loss: 0.0820\n",
      "Epoch 27/30, Train Loss: 0.0782, Val Loss: 0.0817\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Entraîner le modèle\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATIENCE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Évaluer le modèle\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# evaluate_model(model, test_loader, criterion)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 61\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, epochs, patience)\u001b[0m\n\u001b[1;32m     59\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     60\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 61\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Mode validation\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Entraîner le modèle\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, EPOCHS, PATIENCE)\n",
    "\n",
    "# Évaluer le modèle\n",
    "# evaluate_model(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.08199064555961735\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data from the training log\n",
    "epochs = list(range(1, 21))\n",
    "train_loss = [\n",
    "    0.1281,\n",
    "    0.0886,\n",
    "    0.0842,\n",
    "    0.0820,\n",
    "    0.0807,\n",
    "    0.0797,\n",
    "    0.0789,\n",
    "    0.0782,\n",
    "    0.0775,\n",
    "    0.0769,\n",
    "    0.0762,\n",
    "    0.0756,\n",
    "    0.0749,\n",
    "    0.0743,\n",
    "    0.0736,\n",
    "    0.0730,\n",
    "    0.0723,\n",
    "    0.0716,\n",
    "    0.0709,\n",
    "    0.0702,\n",
    "]\n",
    "val_loss = [\n",
    "    0.0914,\n",
    "    0.0848,\n",
    "    0.0824,\n",
    "    0.0810,\n",
    "    0.0807,\n",
    "    0.0799,\n",
    "    0.0795,\n",
    "    0.0792,\n",
    "    0.0789,\n",
    "    0.0786,\n",
    "    0.0787,\n",
    "    0.0786,\n",
    "    0.0785,\n",
    "    0.0784,\n",
    "    0.0782,\n",
    "    0.0784,\n",
    "    0.0783,\n",
    "    0.0786,\n",
    "    0.0785,\n",
    "    0.0787,\n",
    "]\n",
    "\n",
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_loss, label=\"Training Loss\", marker=\"o\")\n",
    "plt.plot(epochs, val_loss, label=\"Validation Loss\", marker=\"x\")\n",
    "plt.axhline(y=0.0782, color=\"r\", linestyle=\"--\", label=\"Best Validation Loss\")\n",
    "plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation des performances (accuracy, precision, recall, f1-score)\n",
    "def evaluate_performance(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for (\n",
    "            x_genres_batch,\n",
    "            x_instruments_batch,\n",
    "            x_moods_batch,\n",
    "            x_genres_categories_batch,\n",
    "            x_instruments_categories_batch,\n",
    "            x_moods_categories_batch,\n",
    "            y_genres_pred_batch,\n",
    "            y_instruments_pred_batch,\n",
    "            y_moods_pred_batch,\n",
    "            y_genres_instruments_pred_batch,\n",
    "            y_genres_moods_pred_batch,\n",
    "            y_instruments_moods_pred_batch,\n",
    "            y_batch,\n",
    "        ) in test_loader:\n",
    "            predictions = model(\n",
    "                x_genres_batch,\n",
    "                x_instruments_batch,\n",
    "                x_moods_batch,\n",
    "                x_genres_categories_batch,\n",
    "                x_instruments_categories_batch,\n",
    "                x_moods_categories_batch,\n",
    "                y_genres_pred_batch,\n",
    "                y_instruments_pred_batch,\n",
    "                y_moods_pred_batch,\n",
    "                y_genres_instruments_pred_batch,\n",
    "                y_genres_moods_pred_batch,\n",
    "                y_instruments_moods_pred_batch,\n",
    "            )\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "            predictions = (predictions > 0.5).int()\n",
    "            y_true.append(y_batch.cpu().numpy())\n",
    "            y_pred.append(predictions.cpu().numpy())\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "    # Save 5% of the rows of the predictions as csv files in the data folder in predictions folder\n",
    "    # np.savetxt(\n",
    "    #     \"../data/predictions/train/y_true.csv\",\n",
    "    #     y_true[: int(0.05 * len(y_true))],\n",
    "    #     delimiter=\",\",\n",
    "    # )\n",
    "    # np.savetxt(\n",
    "    #     \"../data/predictions/train/y_pred.csv\",\n",
    "    #     y_pred[: int(0.05 * len(y_pred))],\n",
    "    #     delimiter=\",\",\n",
    "    # )\n",
    "\n",
    "    # Histograms plot of the predictions and true values for each tag\n",
    "    # for i in range(y_true.shape[1]):\n",
    "    #     plt.hist(y_true[:, i], bins=2, alpha=0.5, label=\"True\")\n",
    "    #     plt.hist(y_pred[:, i], bins=2, alpha=0.5, label=\"Predicted\")\n",
    "    #     plt.title(f\"Tag {i}\")\n",
    "    #     plt.legend()\n",
    "    #     plt.savefig(f\"../data/predictions/train/histogram_tag_{i}.png\")\n",
    "    #     plt.clf()\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # Precision, Recall, F1-Score\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    report = classification_report(y_true, y_pred, digits=4)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9709335865945197\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6190    0.0667    0.1204       195\n",
      "           1     0.6144    0.3686    0.4607      1202\n",
      "           2     1.0000    0.0120    0.0238       249\n",
      "           3     0.2500    0.0123    0.0235        81\n",
      "           4     0.5806    0.2687    0.3673        67\n",
      "           5     0.4954    0.2038    0.2888       265\n",
      "           6     0.6389    0.2072    0.3129       111\n",
      "           7     0.6061    0.0789    0.1396       507\n",
      "           8     0.6602    0.2244    0.3350      1056\n",
      "           9     0.6346    0.1422    0.2324       232\n",
      "          10     0.8262    0.5933    0.6906       713\n",
      "          11     0.7419    0.6590    0.6980      2041\n",
      "          12     0.6087    0.3318    0.4294       422\n",
      "          13     0.5801    0.1237    0.2039      1463\n",
      "          14     0.5783    0.1545    0.2438       861\n",
      "          15     0.6548    0.1440    0.2361       382\n",
      "          16     0.0000    0.0000    0.0000        77\n",
      "          17     0.4545    0.0316    0.0592       158\n",
      "          18     0.3818    0.1603    0.2258       131\n",
      "          19     0.6053    0.0858    0.1503       268\n",
      "          20     0.7786    0.5353    0.6344       992\n",
      "          21     0.6727    0.2202    0.3318       168\n",
      "          22     0.7241    0.1364    0.2295       154\n",
      "          23     0.6616    0.3566    0.4634       976\n",
      "          24     0.5309    0.2373    0.3280       906\n",
      "          25     0.0000    0.0000    0.0000        86\n",
      "          26     0.4091    0.1040    0.1659       346\n",
      "          27     0.6768    0.5036    0.5775      3463\n",
      "          28     0.7451    0.5056    0.6024       896\n",
      "          29     0.7000    0.1327    0.2231       422\n",
      "          30     0.5556    0.0455    0.0840       110\n",
      "          31     0.5000    0.0087    0.0171       230\n",
      "          32     0.7692    0.2632    0.3922        76\n",
      "          33     0.5767    0.3466    0.4330       629\n",
      "          34     0.6278    0.3714    0.4667       377\n",
      "          35     0.6925    0.4378    0.5365       756\n",
      "          36     0.5719    0.3248    0.4143       588\n",
      "          37     0.5699    0.1464    0.2330       362\n",
      "          38     0.5183    0.1543    0.2378       551\n",
      "          39     0.5603    0.3018    0.3923      1478\n",
      "          40     0.4286    0.0659    0.1143        91\n",
      "          41     0.4407    0.1106    0.1769       235\n",
      "          42     0.6316    0.0710    0.1277       338\n",
      "          43     0.0000    0.0000    0.0000       109\n",
      "          44     0.4783    0.0542    0.0973       203\n",
      "          45     0.6781    0.2427    0.3575       651\n",
      "          46     0.9310    0.5000    0.6506        54\n",
      "          47     0.5000    0.0426    0.0784        47\n",
      "          48     0.8276    0.3504    0.4923       137\n",
      "          49     0.9333    0.2692    0.4179        52\n",
      "          50     0.0000    0.0000    0.0000        67\n",
      "          51     0.4118    0.1573    0.2276        89\n",
      "          52     1.0000    0.0857    0.1579        35\n",
      "          53     0.8662    0.6415    0.7371       212\n",
      "          54     0.6575    0.1311    0.2187       366\n",
      "          55     0.7594    0.2590    0.3862       390\n",
      "          56     0.0000    0.0000    0.0000        59\n",
      "          57     0.0000    0.0000    0.0000       131\n",
      "          58     0.0000    0.0000    0.0000       112\n",
      "          59     0.6081    0.2695    0.3734       167\n",
      "          60     0.0000    0.0000    0.0000        94\n",
      "          61     0.4646    0.2577    0.3315       357\n",
      "          62     0.0000    0.0000    0.0000        42\n",
      "          63     0.6356    0.3165    0.4225       237\n",
      "          64     0.7619    0.1119    0.1951       143\n",
      "          65     0.5185    0.1037    0.1728       135\n",
      "          66     0.5284    0.2608    0.3492       464\n",
      "          67     0.6154    0.0426    0.0796       188\n",
      "          68     0.7333    0.3308    0.4560       133\n",
      "          69     0.6000    0.1415    0.2290       212\n",
      "          70     0.6000    0.1013    0.1733       474\n",
      "          71     0.0000    0.0000    0.0000        21\n",
      "          72     0.0000    0.0000    0.0000        36\n",
      "          73     0.6098    0.2632    0.3676       190\n",
      "          74     0.5000    0.0280    0.0531       107\n",
      "          75     0.6250    0.0676    0.1220        74\n",
      "          76     1.0000    0.0495    0.0943       101\n",
      "          77     0.6265    0.1444    0.2348       360\n",
      "          78     1.0000    0.0323    0.0625       124\n",
      "          79     0.4500    0.2222    0.2975        81\n",
      "          80     0.5385    0.1552    0.2409       406\n",
      "          81     0.7578    0.4835    0.5904      1398\n",
      "          82     0.5357    0.0831    0.1439       361\n",
      "          83     0.5000    0.0308    0.0580        65\n",
      "          84     0.4211    0.0428    0.0777       374\n",
      "          85     0.2222    0.0215    0.0392        93\n",
      "          86     0.7452    0.5118    0.6068       680\n",
      "          87     0.6947    0.3517    0.4669       964\n",
      "          88     0.7000    0.1556    0.2545        45\n",
      "          89     0.5374    0.2050    0.2968       561\n",
      "          90     0.7808    0.5816    0.6667        98\n",
      "          91     0.7619    0.5255    0.6220       548\n",
      "          92     0.7704    0.5933    0.6703      3477\n",
      "          93     0.5506    0.0972    0.1653       504\n",
      "          94     0.8105    0.2973    0.4350       259\n",
      "          95     0.7926    0.6102    0.6895      5097\n",
      "          96     0.6364    0.2947    0.4029        95\n",
      "          97     0.6923    0.0804    0.1440       224\n",
      "          98     0.0000    0.0000    0.0000        91\n",
      "          99     0.7634    0.3610    0.4902       277\n",
      "         100     0.7490    0.5153    0.6106      2907\n",
      "         101     0.7407    0.2036    0.3194       393\n",
      "         102     0.6890    0.4009    0.5068      1155\n",
      "         103     1.0000    0.0400    0.0769        25\n",
      "         104     0.0000    0.0000    0.0000        20\n",
      "         105     0.7500    0.0268    0.0517       112\n",
      "         106     0.7430    0.3571    0.4824       518\n",
      "         107     0.7529    0.2684    0.3958       488\n",
      "         108     0.5000    0.0136    0.0265       147\n",
      "         109     0.7021    0.1260    0.2136       262\n",
      "         110     0.4848    0.1702    0.2520        94\n",
      "         111     1.0000    0.0115    0.0228       347\n",
      "         112     0.7857    0.1467    0.2472        75\n",
      "         113     0.6804    0.3247    0.4396      1121\n",
      "         114     0.0000    0.0000    0.0000        87\n",
      "         115     0.8172    0.6323    0.7129      5232\n",
      "         116     0.6877    0.6582    0.6726      4459\n",
      "         117     0.4500    0.0744    0.1277       121\n",
      "         118     0.8048    0.7010    0.7493      6341\n",
      "         119     0.6697    0.2433    0.3570       300\n",
      "         120     0.6184    0.2626    0.3686       179\n",
      "         121     0.7143    0.0318    0.0610       157\n",
      "         122     0.7502    0.5864    0.6582      1378\n",
      "         123     0.5116    0.0554    0.1000       397\n",
      "         124     0.0000    0.0000    0.0000        71\n",
      "         125     0.6675    0.2253    0.3369      1176\n",
      "         126     0.6712    0.5237    0.5884       569\n",
      "         127     0.8333    0.0233    0.0452       215\n",
      "         128     0.5862    0.1735    0.2677       588\n",
      "         129     0.4924    0.1083    0.1776       600\n",
      "         130     0.7581    0.1646    0.2705       571\n",
      "         131     0.9107    0.2277    0.3643       224\n",
      "         132     0.5914    0.2521    0.3535       603\n",
      "         133     0.7442    0.2000    0.3153       160\n",
      "         134     0.6667    0.2121    0.3218        66\n",
      "         135     0.6794    0.4591    0.5480       734\n",
      "         136     0.8750    0.0631    0.1176       111\n",
      "         137     0.7231    0.3507    0.4724       134\n",
      "         138     0.6667    0.2100    0.3193       743\n",
      "         139     0.6250    0.1736    0.2717       144\n",
      "         140     0.5769    0.2419    0.3409        62\n",
      "         141     0.7816    0.6656    0.7189      1860\n",
      "         142     0.5077    0.1325    0.2102       249\n",
      "         143     0.8000    0.3048    0.4414       210\n",
      "         144     0.7083    0.2611    0.3815       586\n",
      "         145     0.0000    0.0000    0.0000        21\n",
      "         146     1.0000    0.0377    0.0727        53\n",
      "         147     0.5000    0.0263    0.0500        38\n",
      "         148     0.7917    0.0701    0.1288       271\n",
      "         149     0.8072    0.1851    0.3011       362\n",
      "         150     0.5147    0.2431    0.3302       288\n",
      "         151     0.8649    0.3902    0.5378        82\n",
      "         152     0.0000    0.0000    0.0000        71\n",
      "         153     0.6222    0.1918    0.2932       146\n",
      "         154     0.6275    0.1758    0.2747       364\n",
      "         155     0.7439    0.5494    0.6320      5413\n",
      "         156     0.0000    0.0000    0.0000        48\n",
      "         157     0.6881    0.3625    0.4749       560\n",
      "         158     0.6397    0.3288    0.4343      1755\n",
      "         159     0.6190    0.3237    0.4251       241\n",
      "         160     0.8462    0.1549    0.2619        71\n",
      "         161     0.7664    0.6505    0.7037      2335\n",
      "         162     0.6881    0.4123    0.5157      1038\n",
      "         163     0.5714    0.0329    0.0623       243\n",
      "         164     0.5504    0.0817    0.1423       869\n",
      "         165     0.6500    0.2500    0.3611       104\n",
      "         166     0.8611    0.2230    0.3543       139\n",
      "         167     0.4595    0.1062    0.1726       160\n",
      "         168     0.7143    0.1402    0.2344       214\n",
      "         169     0.8420    0.6033    0.7029       910\n",
      "         170     0.0000    0.0000    0.0000        71\n",
      "         171     0.0000    0.0000    0.0000        35\n",
      "         172     0.7236    0.5277    0.6103      3686\n",
      "         173     0.7742    0.5249    0.6256      1065\n",
      "         174     0.6727    0.4960    0.5710      1496\n",
      "         175     0.6711    0.3687    0.4759      1489\n",
      "         176     0.5000    0.0023    0.0045       443\n",
      "         177     0.7023    0.7244    0.7132      7002\n",
      "         178     0.0000    0.0000    0.0000        80\n",
      "         179     0.6341    0.2600    0.3688       100\n",
      "         180     0.4688    0.0295    0.0556       508\n",
      "         181     0.4375    0.1830    0.2581       153\n",
      "         182     0.7361    0.1253    0.2141       423\n",
      "         183     0.0000    0.0000    0.0000       140\n",
      "         184     0.6111    0.0629    0.1140       175\n",
      "         185     0.7480    0.2118    0.3301       883\n",
      "         186     0.6667    0.2326    0.3448        86\n",
      "         187     0.0000    0.0000    0.0000        84\n",
      "         188     0.5728    0.1099    0.1844       537\n",
      "         189     0.7778    0.4118    0.5385       187\n",
      "         190     0.7692    0.0469    0.0885       426\n",
      "         191     0.4286    0.0319    0.0594        94\n",
      "         192     0.7488    0.3567    0.4832       869\n",
      "         193     0.8106    0.7559    0.7823      4956\n",
      "         194     0.6897    0.0893    0.1581       224\n",
      "         195     0.0000    0.0000    0.0000        76\n",
      "         196     0.6364    0.0946    0.1647       518\n",
      "         197     0.8000    0.0792    0.1441       101\n",
      "         198     0.8955    0.3774    0.5310       159\n",
      "         199     0.6794    0.4076    0.5095       525\n",
      "         200     0.0000    0.0000    0.0000       566\n",
      "         201     0.6463    0.1949    0.2994       272\n",
      "         202     0.5441    0.0409    0.0761       905\n",
      "         203     0.4821    0.0717    0.1249       753\n",
      "         204     0.6116    0.3673    0.4590      1402\n",
      "         205     0.6240    0.2989    0.4042      2138\n",
      "         206     0.6259    0.1815    0.2814      2479\n",
      "         207     0.1667    0.0026    0.0052       379\n",
      "         208     0.5533    0.3680    0.4420      5503\n",
      "         209     0.4911    0.0847    0.1445       649\n",
      "         210     0.4792    0.0290    0.0547       793\n",
      "         211     0.5514    0.2828    0.3738      4364\n",
      "         212     0.7016    0.1953    0.3055      1096\n",
      "         213     0.3636    0.0086    0.0168       466\n",
      "         214     0.5374    0.0526    0.0959      2185\n",
      "         215     0.5915    0.0894    0.1553       470\n",
      "         216     0.6067    0.1144    0.1926      1267\n",
      "         217     0.5920    0.3566    0.4451      2877\n",
      "         218     0.3742    0.1247    0.1871       465\n",
      "         219     0.6065    0.2327    0.3363      1199\n",
      "         220     0.6429    0.0750    0.1343       840\n",
      "         221     0.6250    0.0101    0.0199       495\n",
      "         222     0.6372    0.1549    0.2492      1859\n",
      "         223     0.6906    0.1704    0.2733       904\n",
      "         224     0.6769    0.2097    0.3202      1688\n",
      "         225     0.0000    0.0000    0.0000       388\n",
      "         226     0.7000    0.0106    0.0210       658\n",
      "         227     0.5347    0.1371    0.2182      1291\n",
      "         228     0.5846    0.3974    0.4732      5236\n",
      "         229     0.5911    0.1534    0.2436      2073\n",
      "         230     0.4615    0.0122    0.0239       490\n",
      "         231     0.7816    0.0791    0.1436       860\n",
      "         232     0.4286    0.0293    0.0549       307\n",
      "         233     0.5825    0.1721    0.2656      2319\n",
      "         234     0.6805    0.3568    0.4682      4131\n",
      "         235     0.6497    0.2232    0.3322      2003\n",
      "         236     0.6517    0.1910    0.2954      1901\n",
      "         237     0.4815    0.0476    0.0867       819\n",
      "         238     0.7121    0.1034    0.1806       909\n",
      "         239     0.5000    0.0013    0.0026       758\n",
      "         240     0.6734    0.4661    0.5509      2446\n",
      "         241     0.5000    0.0089    0.0175       224\n",
      "         242     0.6391    0.2121    0.3185      3357\n",
      "         243     0.5865    0.2618    0.3620      2021\n",
      "         244     0.0000    0.0000    0.0000        87\n",
      "         245     0.6309    0.2341    0.3414      2790\n",
      "         246     0.6667    0.0303    0.0579       727\n",
      "         247     0.6138    0.1896    0.2897      3043\n",
      "\n",
      "   micro avg     0.6940    0.3594    0.4736    200012\n",
      "   macro avg     0.5769    0.2021    0.2725    200012\n",
      "weighted avg     0.6566    0.3594    0.4352    200012\n",
      " samples avg     0.6756    0.3601    0.4456    200012\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluer les performances pour test_loader combiné avec le train_loader\n",
    "\n",
    "# Combine les loaders\n",
    "# combined_loader = torch.utils.data.DataLoader(\n",
    "#     torch.utils.data.ConcatDataset([train_dataset, test_dataset, val_dataset]),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=False,\n",
    "# )\n",
    "\n",
    "evaluate_performance(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer et ESNs sauvegardés avec succès !\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder le modèle et les ESNs dans le dossier models et un sous dossier qui s'incrémente\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Créer un dossier models s'il n'existe pas\n",
    "if not os.path.exists(\"../models\"):\n",
    "    os.makedirs(\"../models\")\n",
    "\n",
    "# Créer un sous-dossier pour les modèles\n",
    "sub_folder = 0\n",
    "while os.path.exists(f\"../models/model_{sub_folder}\"):\n",
    "    sub_folder += 1\n",
    "os.makedirs(f\"../models/model_{sub_folder}\")\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "torch.save(model.state_dict(), f\"../models/model_{sub_folder}/transformer_weights.pth\")\n",
    "\n",
    "with open(f\"../models/model_{sub_folder}/transformer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Sauvegarder les ESNs\n",
    "with open(f\"../models/model_{sub_folder}/esn_Genre.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_Genre, f)\n",
    "\n",
    "with open(f\"../models/model_{sub_folder}/esn_Instrument.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_Instrument, f)\n",
    "\n",
    "with open(f\"../models/model_{sub_folder}/esn_Mood.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_Mood, f)\n",
    "\n",
    "with open(f\"../models/model_{sub_folder}/esn_Genre_Instrument.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_Genre_Instrument, f)\n",
    "\n",
    "with open(f\"../models/model_{sub_folder}/esn_Genre_Mood.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_Genre_Mood, f)\n",
    "\n",
    "with open(f\"../models/model_{sub_folder}/esn_Instrument_Mood.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_Instrument_Mood, f)\n",
    "\n",
    "\n",
    "print(\"Transformer et ESNs sauvegardés avec succès !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
