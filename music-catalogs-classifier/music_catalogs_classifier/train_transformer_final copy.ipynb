{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from reservoirpy.nodes import Reservoir, Ridge, ESN\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "# EMBEDDING_SIZE = 256  # Taille des embeddings pour tous les inputs\n",
    "# NUM_HEADS = 8\n",
    "# NUM_LAYERS = 4\n",
    "# DROPOUT = 0.1\n",
    "# LEARNING_RATE = 1e-4\n",
    "EMBEDDING_SIZE = 285\n",
    "NUM_HEADS = 15\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.2618574215322765\n",
    "LEARNING_RATE = 0.00015775441092497324\n",
    "EPOCHS = 20\n",
    "PATIENCE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    partOfData = 1\n",
    "    X_genres = pd.read_csv(\"../data/train/input_genres_tags_data.csv\")\n",
    "    X_instruments = pd.read_csv(\"../data/train/input_instruments_tags_data.csv\")\n",
    "    X_moods = pd.read_csv(\"../data/train/input_moods_tags_data.csv\")\n",
    "    X_genres_categories = pd.read_csv(\"../data/train/input_genres_categories_data.csv\")\n",
    "    X_instruments_categories = pd.read_csv(\n",
    "        \"../data/train/input_instruments_categories_data.csv\"\n",
    "    )\n",
    "    X_moods_categories = pd.read_csv(\"../data/train/input_moods_categories_data.csv\")\n",
    "\n",
    "    y_genres = pd.read_csv(\"../data/train/output_genres_tags_data.csv\")\n",
    "    y_instruments = pd.read_csv(\"../data/train/output_instruments_tags_data.csv\")\n",
    "    y_moods = pd.read_csv(\"../data/train/output_moods_tags_data.csv\")\n",
    "\n",
    "    # On peut garder seulement une partie des données\n",
    "    X_genres = X_genres[: int(partOfData * len(X_genres))]\n",
    "    X_instruments = X_instruments[: int(partOfData * len(X_instruments))]\n",
    "    X_moods = X_moods[: int(partOfData * len(X_moods))]\n",
    "    y_genres = y_genres[: int(partOfData * len(y_genres))]\n",
    "    y_instruments = y_instruments[: int(partOfData * len(y_instruments))]\n",
    "    y_moods = y_moods[: int(partOfData * len(y_moods))]\n",
    "    X_genres_categories = X_genres_categories[\n",
    "        : int(partOfData * len(X_genres_categories))\n",
    "    ]\n",
    "    X_instruments_categories = X_instruments_categories[\n",
    "        : int(partOfData * len(X_instruments_categories))\n",
    "    ]\n",
    "    X_moods_categories = X_moods_categories[: int(partOfData * len(X_moods_categories))]\n",
    "\n",
    "    return (\n",
    "        X_genres,\n",
    "        X_instruments,\n",
    "        X_moods,\n",
    "        X_genres_categories,\n",
    "        X_instruments_categories,\n",
    "        X_moods_categories,\n",
    "    ), (y_genres, y_instruments, y_moods)\n",
    "\n",
    "\n",
    "# Ensure the input data is in the correct format\n",
    "def reshape_input(X):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        return X.values.reshape(-1, 1, X.shape[1])  # Handles pandas DataFrame\n",
    "    elif isinstance(X, np.ndarray):\n",
    "        return X.reshape(-1, 1, X.shape[1])  # Handles numpy ndarray\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a pandas DataFrame or a numpy ndarray\")\n",
    "\n",
    "\n",
    "def format_predictions(predictions):\n",
    "    # Convert the list to a NumPy array\n",
    "    predictions_array = np.array(predictions)\n",
    "\n",
    "    # Reshape the array to 2-dimensional\n",
    "    predictions_reshaped = predictions_array.reshape(-1, predictions_array.shape[-1])\n",
    "\n",
    "    return predictions_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les données\n",
    "(\n",
    "    (\n",
    "        X_genres,\n",
    "        X_instruments,\n",
    "        X_moods,\n",
    "        X_genres_categories,\n",
    "        X_instruments_categories,\n",
    "        X_moods_categories,\n",
    "    ),\n",
    "    (y_genres, y_instruments, y_moods),\n",
    ") = load_data()\n",
    "\n",
    "# Train-test split\n",
    "X_genres_train, X_genres_test, y_genres_train, y_genres_test = train_test_split(\n",
    "    X_genres, y_genres, test_size=0.2, random_state=42\n",
    ")\n",
    "X_instruments_train, X_instruments_test, y_instruments_train, y_instruments_test = (\n",
    "    train_test_split(X_instruments, y_instruments, test_size=0.2, random_state=42)\n",
    ")\n",
    "X_moods_train, X_moods_test, y_moods_train, y_moods_test = train_test_split(\n",
    "    X_moods, y_moods, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train-test split for categories\n",
    "X_genres_categories_train, X_genres_categories_test = train_test_split(\n",
    "    X_genres_categories, test_size=0.2, random_state=42\n",
    ")\n",
    "X_instruments_categories_train, X_instruments_categories_test = train_test_split(\n",
    "    X_instruments_categories, test_size=0.2, random_state=42\n",
    ")\n",
    "X_moods_categories_train, X_moods_categories_test = train_test_split(\n",
    "    X_moods_categories, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation des données\n",
    "X_genres_train = X_genres_train.drop(columns=[\"ChallengeID\"])\n",
    "X_instruments_train = X_instruments_train.drop(columns=[\"ChallengeID\"])\n",
    "X_moods_train = X_moods_train.drop(columns=[\"ChallengeID\"])\n",
    "y_genres_train = y_genres_train.drop(columns=[\"ChallengeID\"])\n",
    "y_instruments_train = y_instruments_train.drop(columns=[\"ChallengeID\"])\n",
    "y_moods_train = y_moods_train.drop(columns=[\"ChallengeID\"])\n",
    "X_genres_categories_train = X_genres_categories_train.drop(columns=[\"ChallengeID\"])\n",
    "X_instruments_categories_train = X_instruments_categories_train.drop(\n",
    "    columns=[\"ChallengeID\"]\n",
    ")\n",
    "X_moods_categories_train = X_moods_categories_train.drop(columns=[\"ChallengeID\"])\n",
    "\n",
    "X_genres_test = X_genres_test.drop(columns=[\"ChallengeID\"])\n",
    "X_instruments_test = X_instruments_test.drop(columns=[\"ChallengeID\"])\n",
    "X_moods_test = X_moods_test.drop(columns=[\"ChallengeID\"])\n",
    "y_genres_test = y_genres_test.drop(columns=[\"ChallengeID\"])\n",
    "y_instruments_test = y_instruments_test.drop(columns=[\"ChallengeID\"])\n",
    "y_moods_test = y_moods_test.drop(columns=[\"ChallengeID\"])\n",
    "X_genres_categories_test = X_genres_categories_test.drop(columns=[\"ChallengeID\"])\n",
    "X_instruments_categories_test = X_instruments_categories_test.drop(\n",
    "    columns=[\"ChallengeID\"]\n",
    ")\n",
    "X_moods_categories_test = X_moods_categories_test.drop(columns=[\"ChallengeID\"])\n",
    "\n",
    "\n",
    "X_train = np.concatenate(\n",
    "    [\n",
    "        X_genres_train,\n",
    "        X_instruments_train,\n",
    "        X_moods_train,\n",
    "        X_genres_categories_train,\n",
    "        X_instruments_categories_train,\n",
    "        X_moods_categories_train,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "X_test = np.concatenate(\n",
    "    [\n",
    "        X_genres_test,\n",
    "        X_instruments_test,\n",
    "        X_moods_test,\n",
    "        X_genres_categories_test,\n",
    "        X_instruments_categories_test,\n",
    "        X_moods_categories_test,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "y_train = np.concatenate([y_genres_train, y_instruments_train, y_moods_train], axis=1)\n",
    "y_test = np.concatenate([y_genres_test, y_instruments_test, y_moods_test], axis=1)\n",
    "\n",
    "# Convertir les données en tensors PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(DEVICE)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(DEVICE)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation des réservoirs (genre, instrument, mood)\n",
    "reservoir_Genre = Reservoir(\n",
    "    units=100,\n",
    "    sr=0,  # Spectral radius\n",
    "    lr=1,  # Leak rate\n",
    "    input_scaling=1.0,\n",
    ")\n",
    "\n",
    "reservoir_Instrument = Reservoir(units=100, sr=0, lr=1, input_scaling=1.0)\n",
    "\n",
    "reservoir_Mood = Reservoir(units=100, sr=0, lr=1, input_scaling=1.0)\n",
    "\n",
    "# Readout pour chaque réservoir\n",
    "readout_Genre = Ridge(ridge=1e-4)\n",
    "readout_Instrument = Ridge(ridge=1e-4)\n",
    "readout_Mood = Ridge(ridge=1e-4)\n",
    "\n",
    "# Création des modèles avec ESN (Echo State Network)\n",
    "model_Genre = ESN(reservoir=reservoir_Genre, readout=readout_Genre, workers=-1)\n",
    "model_Instrument = ESN(\n",
    "    reservoir=reservoir_Instrument, readout=readout_Instrument, workers=-1\n",
    ")\n",
    "model_Mood = ESN(reservoir=reservoir_Mood, readout=readout_Mood, workers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-3: 100%|██████████| 88683/88683 [00:09<00:00, 9244.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-4: 100%|██████████| 88683/88683 [00:09<00:00, 8970.70it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-5: 100%|██████████| 88683/88683 [00:08<00:00, 9944.76it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-5...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ESN-5': ESN('Reservoir-5', 'Ridge-5')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_genres_train_reshaped = reshape_input(X_genres_train)\n",
    "X_instruments_train_reshaped = reshape_input(X_instruments_train)\n",
    "X_moods_train_reshaped = reshape_input(X_moods_train)\n",
    "\n",
    "y_genres_train_reshaped = reshape_input(y_genres_train)\n",
    "y_instruments_train_reshaped = reshape_input(y_instruments_train)\n",
    "y_moods_train_reshaped = reshape_input(y_moods_train)\n",
    "\n",
    "X_genres_test_reshaped = reshape_input(X_genres_test)\n",
    "X_instruments_test_reshaped = reshape_input(X_instruments_test)\n",
    "X_moods_test_reshaped = reshape_input(X_moods_test)\n",
    "\n",
    "y_genres_test_reshaped = reshape_input(y_genres_test)\n",
    "y_instruments_test_reshaped = reshape_input(y_instruments_test)\n",
    "y_moods_test_reshaped = reshape_input(y_moods_test)\n",
    "\n",
    "\n",
    "# Entraîner les réservoirs\n",
    "# Train the models with one line for single timestep\n",
    "model_Genre.fit(X_genres_train_reshaped, y_genres_train_reshaped)\n",
    "model_Instrument.fit(X_instruments_train_reshaped, y_instruments_train_reshaped)\n",
    "model_Mood.fit(X_moods_train_reshaped, y_moods_train_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Obtenir les sorties des réservoirs\n",
    "# y_genres_train_pred = model_Genre.run(X_genres_train_reshaped)\n",
    "# y_instruments_train_pred = model_Instrument.run(X_instruments_train_reshaped)\n",
    "# y_moods_train_pred = model_Mood.run(X_moods_train_reshaped)\n",
    "\n",
    "# y_genres_test_pred = model_Genre.run(X_genres_test_reshaped)\n",
    "# y_instruments_test_pred = model_Instrument.run(X_instruments_test_reshaped)\n",
    "# y_moods_test_pred = model_Mood.run(X_moods_test_reshaped)\n",
    "\n",
    "# # Formater les prédictions\n",
    "# y_genres_train_pred = format_predictions(y_genres_train_pred)\n",
    "# y_instruments_train_pred = format_predictions(y_instruments_train_pred)\n",
    "# y_moods_train_pred = format_predictions(y_moods_train_pred)\n",
    "\n",
    "# y_genres_test_pred = format_predictions(y_genres_test_pred)\n",
    "# y_instruments_test_pred = format_predictions(y_instruments_test_pred)\n",
    "# y_moods_test_pred = format_predictions(y_moods_test_pred)\n",
    "\n",
    "\n",
    "# Combine toutes les sorties (individuelles et croisées)\n",
    "X_train_final = np.concatenate(\n",
    "    [\n",
    "        X_genres_train,\n",
    "        X_instruments_train,\n",
    "        X_moods_train,\n",
    "        X_genres_categories_train,\n",
    "        X_instruments_categories_train,\n",
    "        X_moods_categories_train,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "X_test_final = np.concatenate(\n",
    "    [\n",
    "        X_genres_test,\n",
    "        X_instruments_test,\n",
    "        X_moods_test,\n",
    "        X_genres_categories_test,\n",
    "        X_instruments_categories_test,\n",
    "        X_moods_categories_test,\n",
    "    ],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraction de données pour la validation\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# Division des données en entraînement et validation\n",
    "(\n",
    "    X_train_final_train,\n",
    "    X_train_final_val,\n",
    "    y_train_tensor_train,\n",
    "    y_train_tensor_val,\n",
    ") = train_test_split(\n",
    "    X_train_final, y_train_tensor, test_size=VALIDATION_SPLIT, random_state=42\n",
    ")\n",
    "\n",
    "# Création des datasets\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(X_train_final_train, dtype=torch.float32).to(DEVICE),\n",
    "    y_train_tensor_train.clone().detach().to(DEVICE),\n",
    ")\n",
    "val_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(X_train_final_val, dtype=torch.float32).to(DEVICE),\n",
    "    y_train_tensor_val.clone().detach().to(DEVICE),\n",
    ")\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(X_test_final, dtype=torch.float32).to(DEVICE),\n",
    "    y_test_tensor.clone().detach().to(DEVICE),\n",
    ")\n",
    "\n",
    "# Création des loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([70946, 289]), torch.Size([70946, 248]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.tensors[0].shape, train_dataset.tensors[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de paramètres : 6795430\n"
     ]
    }
   ],
   "source": [
    "# Modèle de Transformeur\n",
    "class MultiTaskTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, embedding_size, num_heads, num_layers, num_labels, dropout\n",
    "    ):\n",
    "        super(MultiTaskTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, embedding_size)  # Embedding Layer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embedding_size,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.classifier = nn.Linear(embedding_size, num_labels)  # Final Classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the embedding layer\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Add a positional encoding (if needed)\n",
    "        embedded = embedded.unsqueeze(1)  # Add sequence dimension\n",
    "\n",
    "        # Transformer expects (batch, seq_len, embedding_size)\n",
    "        transformer_output = self.transformer(embedded, embedded)\n",
    "\n",
    "        # Take only the output of the first token (classification token equivalent)\n",
    "        output = transformer_output[:, 0, :]  # Extract first token\n",
    "\n",
    "        # Pass through the classifier\n",
    "        predictions = self.classifier(output)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Initialiser le modèle\n",
    "model = MultiTaskTransformer(\n",
    "    input_size=X_train_final.shape[\n",
    "        1\n",
    "    ],  # Taille totale des sorties concaténées des réservoirs\n",
    "    embedding_size=EMBEDDING_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_labels=y_train.shape[1],  # Nombre total de catégories en sortie\n",
    "    dropout=DROPOUT,\n",
    ").to(DEVICE)\n",
    "\n",
    "# Affichage du nombre de paramètres\n",
    "print(f\"Nombre total de paramètres : {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Optimiseur et fonction de perte\n",
    "criterion = nn.BCEWithLogitsLoss()  # Fonction de perte pour les étiquettes binaires\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entraînement\n",
    "def train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, epochs, patience\n",
    "):\n",
    "    \"\"\"\n",
    "        Entraîne un modèle avec early stopping.\n",
    "\n",
    "        Args:\n",
    "            model: Le modèle à entraîner.\n",
    "            train_loader: DataLoader pour les données d'entraînement.\n",
    "            val_loader: DataLoader pour les données de validation.\n",
    "            criterion: Fonction de perte.\n",
    "            optimizer: Optimiseur.\n",
    "            epochs: Nombre maximum d'époques.\n",
    "            patience: Nombre d'époques à attendre pour une amélioration avant d'arrêter l'entraînement.\n",
    "\n",
    "        Returns:\n",
    "            Le meilleur modèle basé sur les performances sur l'ensemble    macro avg     0.0000    0.0000    0.0000    636962\n",
    "    weighted avg     0.0000    0.0000    0.0000    636962\n",
    "     samples avg     0.0000    0.0000    0.0000  de validation.\n",
    "    \"\"\"\n",
    "    best_loss = float(\"inf\")\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Mode entraînement\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # Mode validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                predictions = model(X_batch)\n",
    "                loss = criterion(predictions, y_batch)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\n",
    "                f\"Early stopping triggered at epoch {epoch + 1}. Best validation loss: {best_loss:.4f}\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "    # Charger le meilleur modèle\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Évaluation\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            total_loss += loss.item()\n",
    "    print(f\"Test Loss: {total_loss / len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.1168, Val Loss: 0.0880\n",
      "Epoch 2/20, Train Loss: 0.0874, Val Loss: 0.0834\n",
      "Epoch 3/20, Train Loss: 0.0839, Val Loss: 0.0818\n",
      "Epoch 4/20, Train Loss: 0.0822, Val Loss: 0.0807\n",
      "Epoch 5/20, Train Loss: 0.0810, Val Loss: 0.0801\n",
      "Epoch 6/20, Train Loss: 0.0800, Val Loss: 0.0795\n",
      "Epoch 7/20, Train Loss: 0.0793, Val Loss: 0.0788\n",
      "Epoch 8/20, Train Loss: 0.0786, Val Loss: 0.0786\n",
      "Epoch 9/20, Train Loss: 0.0780, Val Loss: 0.0784\n",
      "Epoch 10/20, Train Loss: 0.0775, Val Loss: 0.0781\n",
      "Epoch 11/20, Train Loss: 0.0770, Val Loss: 0.0779\n",
      "Epoch 12/20, Train Loss: 0.0765, Val Loss: 0.0778\n",
      "Epoch 13/20, Train Loss: 0.0760, Val Loss: 0.0777\n",
      "Epoch 14/20, Train Loss: 0.0755, Val Loss: 0.0775\n",
      "Epoch 15/20, Train Loss: 0.0751, Val Loss: 0.0774\n",
      "Epoch 16/20, Train Loss: 0.0746, Val Loss: 0.0774\n",
      "Epoch 17/20, Train Loss: 0.0742, Val Loss: 0.0773\n",
      "Epoch 18/20, Train Loss: 0.0737, Val Loss: 0.0772\n",
      "Epoch 19/20, Train Loss: 0.0734, Val Loss: 0.0772\n",
      "Epoch 20/20, Train Loss: 0.0729, Val Loss: 0.0773\n",
      "Test Loss: 0.07752533686435875\n"
     ]
    }
   ],
   "source": [
    "# Entraîner le modèle\n",
    "model = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, EPOCHS, PATIENCE\n",
    ")\n",
    "\n",
    "# Évaluer le modèle\n",
    "evaluate_model(model, test_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation des performances (accuracy, precision, recall, f1-score)\n",
    "def evaluate_performance(model, test_loader):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            predictions = model(X_batch)\n",
    "            predictions = torch.sigmoid(predictions)\n",
    "            predictions = (predictions > 0.5).int()\n",
    "            y_true.append(y_batch.cpu().numpy())\n",
    "            y_pred.append(predictions.cpu().numpy())\n",
    "    y_true = np.concatenate(y_true, axis=0)\n",
    "    y_pred = np.concatenate(y_pred, axis=0)\n",
    "\n",
    "    # Save 5% of the rows of the predictions as csv files in the data folder in predictions folder\n",
    "    np.savetxt(\n",
    "        \"../data/predictions/train/y_true.csv\",\n",
    "        y_true[: int(0.05 * len(y_true))],\n",
    "        delimiter=\",\",\n",
    "    )\n",
    "    np.savetxt(\n",
    "        \"../data/predictions/train/y_pred.csv\",\n",
    "        y_pred[: int(0.05 * len(y_pred))],\n",
    "        delimiter=\",\",\n",
    "    )\n",
    "\n",
    "    # Histograms plot of the predictions and true values for each tag\n",
    "    # for i in range(y_true.shape[1]):\n",
    "    #     plt.hist(y_true[:, i], bins=2, alpha=0.5, label=\"True\")\n",
    "    #     plt.hist(y_pred[:, i], bins=2, alpha=0.5, label=\"Predicted\")\n",
    "    #     plt.title(f\"Tag {i}\")\n",
    "    #     plt.legend()\n",
    "    #     plt.savefig(f\"../data/predictions/train/histogram_tag_{i}.png\")\n",
    "    #     plt.clf()\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    # Precision, Recall, F1-Score\n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    report = classification_report(y_true, y_pred, digits=4)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9719198720793364\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4444    0.1026    0.1667       195\n",
      "           1     0.6010    0.3935    0.4756      1202\n",
      "           2     0.4808    0.1004    0.1661       249\n",
      "           3     1.0000    0.0370    0.0714        81\n",
      "           4     0.6327    0.4627    0.5345        67\n",
      "           5     0.5376    0.1887    0.2793       265\n",
      "           6     0.5882    0.2703    0.3704       111\n",
      "           7     0.5734    0.1617    0.2523       507\n",
      "           8     0.5965    0.3920    0.4731      1056\n",
      "           9     0.5702    0.2974    0.3909       232\n",
      "          10     0.8229    0.6255    0.7108       713\n",
      "          11     0.7547    0.6859    0.7187      2041\n",
      "          12     0.6020    0.4194    0.4944       422\n",
      "          13     0.6006    0.1490    0.2388      1463\n",
      "          14     0.5884    0.2009    0.2996       861\n",
      "          15     0.5484    0.2225    0.3166       382\n",
      "          16     0.0000    0.0000    0.0000        77\n",
      "          17     0.4512    0.2342    0.3083       158\n",
      "          18     0.4808    0.1908    0.2732       131\n",
      "          19     0.6220    0.1903    0.2914       268\n",
      "          20     0.7890    0.5655    0.6588       992\n",
      "          21     0.6226    0.3929    0.4818       168\n",
      "          22     0.7708    0.2403    0.3663       154\n",
      "          23     0.6672    0.4068    0.5054       976\n",
      "          24     0.5329    0.2506    0.3408       906\n",
      "          25     0.0000    0.0000    0.0000        86\n",
      "          26     0.4052    0.1358    0.2035       346\n",
      "          27     0.6959    0.5565    0.6184      3463\n",
      "          28     0.7057    0.6317    0.6667       896\n",
      "          29     0.6446    0.1848    0.2873       422\n",
      "          30     0.5385    0.1273    0.2059       110\n",
      "          31     0.4565    0.0913    0.1522       230\n",
      "          32     0.6053    0.3026    0.4035        76\n",
      "          33     0.6176    0.4134    0.4952       629\n",
      "          34     0.6413    0.3793    0.4767       377\n",
      "          35     0.6471    0.5384    0.5877       756\n",
      "          36     0.5491    0.3520    0.4290       588\n",
      "          37     0.5391    0.3425    0.4189       362\n",
      "          38     0.4980    0.2269    0.3117       551\n",
      "          39     0.5397    0.4235    0.4746      1478\n",
      "          40     0.5294    0.0989    0.1667        91\n",
      "          41     0.5342    0.1660    0.2532       235\n",
      "          42     0.5484    0.1006    0.1700       338\n",
      "          43     0.6154    0.0734    0.1311       109\n",
      "          44     0.6200    0.1527    0.2451       203\n",
      "          45     0.6588    0.3441    0.4521       651\n",
      "          46     0.9375    0.5556    0.6977        54\n",
      "          47     0.5625    0.1915    0.2857        47\n",
      "          48     0.6923    0.4599    0.5526       137\n",
      "          49     0.8333    0.3846    0.5263        52\n",
      "          50     0.0000    0.0000    0.0000        67\n",
      "          51     0.5238    0.1236    0.2000        89\n",
      "          52     1.0000    0.1714    0.2927        35\n",
      "          53     0.8757    0.6981    0.7769       212\n",
      "          54     0.6338    0.2459    0.3543       366\n",
      "          55     0.7198    0.3359    0.4580       390\n",
      "          56     0.4000    0.0339    0.0625        59\n",
      "          57     0.0000    0.0000    0.0000       131\n",
      "          58     0.7500    0.0268    0.0517       112\n",
      "          59     0.6000    0.3054    0.4048       167\n",
      "          60     0.5000    0.0638    0.1132        94\n",
      "          61     0.5321    0.2325    0.3236       357\n",
      "          62     0.5000    0.0952    0.1600        42\n",
      "          63     0.5860    0.3882    0.4670       237\n",
      "          64     0.6000    0.2308    0.3333       143\n",
      "          65     0.5306    0.1926    0.2826       135\n",
      "          66     0.5815    0.3384    0.4278       464\n",
      "          67     0.5217    0.1915    0.2802       188\n",
      "          68     0.7397    0.4060    0.5243       133\n",
      "          69     0.6301    0.2170    0.3228       212\n",
      "          70     0.6061    0.1688    0.2640       474\n",
      "          71     0.0000    0.0000    0.0000        21\n",
      "          72     1.0000    0.0278    0.0541        36\n",
      "          73     0.6063    0.4053    0.4858       190\n",
      "          74     0.3750    0.0561    0.0976       107\n",
      "          75     0.5806    0.2432    0.3429        74\n",
      "          76     0.5600    0.1386    0.2222       101\n",
      "          77     0.6505    0.1861    0.2894       360\n",
      "          78     0.5909    0.1048    0.1781       124\n",
      "          79     0.5682    0.3086    0.4000        81\n",
      "          80     0.5399    0.2167    0.3093       406\n",
      "          81     0.7853    0.5258    0.6298      1398\n",
      "          82     0.5895    0.1551    0.2456       361\n",
      "          83     0.6389    0.3538    0.4554        65\n",
      "          84     0.4545    0.1471    0.2222       374\n",
      "          85     0.4000    0.0430    0.0777        93\n",
      "          86     0.6431    0.6544    0.6487       680\n",
      "          87     0.6357    0.4544    0.5299       964\n",
      "          88     0.6154    0.1778    0.2759        45\n",
      "          89     0.5444    0.2406    0.3337       561\n",
      "          90     0.8193    0.6939    0.7514        98\n",
      "          91     0.7931    0.5876    0.6751       548\n",
      "          92     0.7915    0.6083    0.6879      3477\n",
      "          93     0.6875    0.1528    0.2500       504\n",
      "          94     0.7226    0.3822    0.5000       259\n",
      "          95     0.8086    0.6264    0.7059      5097\n",
      "          96     0.6207    0.3789    0.4706        95\n",
      "          97     0.6207    0.1607    0.2553       224\n",
      "          98     0.9231    0.1319    0.2308        91\n",
      "          99     0.7384    0.4585    0.5657       277\n",
      "         100     0.7138    0.5869    0.6441      2907\n",
      "         101     0.7537    0.2570    0.3833       393\n",
      "         102     0.6909    0.4779    0.5650      1155\n",
      "         103     0.8000    0.1600    0.2667        25\n",
      "         104     1.0000    0.1000    0.1818        20\n",
      "         105     0.5000    0.0982    0.1642       112\n",
      "         106     0.7211    0.4691    0.5684       518\n",
      "         107     0.6697    0.4488    0.5374       488\n",
      "         108     0.5385    0.0476    0.0875       147\n",
      "         109     0.6932    0.2328    0.3486       262\n",
      "         110     0.5283    0.2979    0.3810        94\n",
      "         111     0.6321    0.1931    0.2958       347\n",
      "         112     0.6486    0.3200    0.4286        75\n",
      "         113     0.7483    0.3818    0.5056      1121\n",
      "         114     0.8333    0.0575    0.1075        87\n",
      "         115     0.8372    0.6525    0.7334      5232\n",
      "         116     0.6988    0.7033    0.7010      4459\n",
      "         117     0.5532    0.2149    0.3095       121\n",
      "         118     0.8129    0.7182    0.7626      6341\n",
      "         119     0.6625    0.3533    0.4609       300\n",
      "         120     0.7234    0.3799    0.4982       179\n",
      "         121     0.5789    0.1401    0.2256       157\n",
      "         122     0.7804    0.5827    0.6672      1378\n",
      "         123     0.5481    0.1436    0.2275       397\n",
      "         124     1.0000    0.0141    0.0278        71\n",
      "         125     0.6373    0.3810    0.4768      1176\n",
      "         126     0.7123    0.5395    0.6140       569\n",
      "         127     0.5000    0.0977    0.1634       215\n",
      "         128     0.6615    0.2925    0.4057       588\n",
      "         129     0.5951    0.1617    0.2543       600\n",
      "         130     0.6535    0.2907    0.4024       571\n",
      "         131     0.7692    0.3125    0.4444       224\n",
      "         132     0.5812    0.4096    0.4805       603\n",
      "         133     0.6707    0.3438    0.4545       160\n",
      "         134     0.6957    0.2424    0.3596        66\n",
      "         135     0.7084    0.5395    0.6125       734\n",
      "         136     0.5366    0.3964    0.4560       111\n",
      "         137     0.7763    0.4403    0.5619       134\n",
      "         138     0.6982    0.3082    0.4276       743\n",
      "         139     0.5645    0.2431    0.3398       144\n",
      "         140     0.5556    0.3226    0.4082        62\n",
      "         141     0.7919    0.6608    0.7204      1860\n",
      "         142     0.5784    0.2369    0.3362       249\n",
      "         143     0.6791    0.4333    0.5291       210\n",
      "         144     0.6514    0.3891    0.4872       586\n",
      "         145     0.6667    0.0952    0.1667        21\n",
      "         146     0.6800    0.3208    0.4359        53\n",
      "         147     0.4000    0.0526    0.0930        38\n",
      "         148     0.5578    0.3026    0.3923       271\n",
      "         149     0.8087    0.2569    0.3899       362\n",
      "         150     0.5784    0.3715    0.4524       288\n",
      "         151     0.8696    0.4878    0.6250        82\n",
      "         152     0.2000    0.0141    0.0263        71\n",
      "         153     0.6909    0.2603    0.3781       146\n",
      "         154     0.5782    0.2335    0.3327       364\n",
      "         155     0.7750    0.5548    0.6466      5413\n",
      "         156     0.7778    0.1458    0.2456        48\n",
      "         157     0.7526    0.3857    0.5100       560\n",
      "         158     0.7008    0.3470    0.4642      1755\n",
      "         159     0.6615    0.3568    0.4636       241\n",
      "         160     0.7059    0.1690    0.2727        71\n",
      "         161     0.7951    0.6681    0.7261      2335\n",
      "         162     0.7561    0.4480    0.5626      1038\n",
      "         163     0.5814    0.1029    0.1748       243\n",
      "         164     0.5708    0.1530    0.2414       869\n",
      "         165     0.6744    0.2788    0.3946       104\n",
      "         166     0.7273    0.4029    0.5185       139\n",
      "         167     0.5185    0.1750    0.2617       160\n",
      "         168     0.7101    0.2290    0.3463       214\n",
      "         169     0.8295    0.6791    0.7468       910\n",
      "         170     0.4000    0.0282    0.0526        71\n",
      "         171     0.0000    0.0000    0.0000        35\n",
      "         172     0.7409    0.5803    0.6508      3686\n",
      "         173     0.7253    0.6423    0.6813      1065\n",
      "         174     0.7185    0.5033    0.5920      1496\n",
      "         175     0.6686    0.4621    0.5465      1489\n",
      "         176     0.5862    0.0384    0.0720       443\n",
      "         177     0.7091    0.7439    0.7261      7002\n",
      "         178     0.0000    0.0000    0.0000        80\n",
      "         179     0.6000    0.3300    0.4258       100\n",
      "         180     0.5726    0.1319    0.2144       508\n",
      "         181     0.5758    0.2484    0.3470       153\n",
      "         182     0.6646    0.2530    0.3664       423\n",
      "         183     0.0000    0.0000    0.0000       140\n",
      "         184     0.5385    0.1600    0.2467       175\n",
      "         185     0.7121    0.2661    0.3875       883\n",
      "         186     0.6140    0.4070    0.4895        86\n",
      "         187     0.6667    0.0238    0.0460        84\n",
      "         188     0.5648    0.2272    0.3240       537\n",
      "         189     0.7229    0.6417    0.6799       187\n",
      "         190     0.7083    0.1596    0.2605       426\n",
      "         191     0.4118    0.0745    0.1261        94\n",
      "         192     0.7398    0.4384    0.5506       869\n",
      "         193     0.8380    0.7554    0.7946      4956\n",
      "         194     0.6087    0.1250    0.2074       224\n",
      "         195     0.0000    0.0000    0.0000        76\n",
      "         196     0.6748    0.1602    0.2590       518\n",
      "         197     0.8636    0.1881    0.3089       101\n",
      "         198     0.8776    0.5409    0.6693       159\n",
      "         199     0.6310    0.5505    0.5880       525\n",
      "         200     0.5641    0.0389    0.0727       566\n",
      "         201     0.6181    0.3272    0.4279       272\n",
      "         202     0.6053    0.0762    0.1354       905\n",
      "         203     0.6058    0.1102    0.1865       753\n",
      "         204     0.5982    0.4194    0.4931      1402\n",
      "         205     0.5888    0.4247    0.4935      2138\n",
      "         206     0.5811    0.2485    0.3481      2479\n",
      "         207     0.1667    0.0079    0.0151       379\n",
      "         208     0.5716    0.3940    0.4664      5503\n",
      "         209     0.5493    0.1803    0.2715       649\n",
      "         210     0.5053    0.0605    0.1081       793\n",
      "         211     0.6156    0.2404    0.3457      4364\n",
      "         212     0.6360    0.3157    0.4220      1096\n",
      "         213     0.6000    0.0451    0.0838       466\n",
      "         214     0.5182    0.1565    0.2404      2185\n",
      "         215     0.6292    0.1191    0.2004       470\n",
      "         216     0.5436    0.2463    0.3389      1267\n",
      "         217     0.5972    0.4195    0.4929      2877\n",
      "         218     0.4382    0.1677    0.2426       465\n",
      "         219     0.6133    0.2777    0.3823      1199\n",
      "         220     0.6455    0.1452    0.2371       840\n",
      "         221     0.5455    0.0606    0.1091       495\n",
      "         222     0.6445    0.2351    0.3445      1859\n",
      "         223     0.5851    0.2699    0.3694       904\n",
      "         224     0.6142    0.3235    0.4237      1688\n",
      "         225     0.0667    0.0026    0.0050       388\n",
      "         226     0.4267    0.0486    0.0873       658\n",
      "         227     0.5355    0.2510    0.3418      1291\n",
      "         228     0.6223    0.3692    0.4634      5236\n",
      "         229     0.5906    0.2422    0.3435      2073\n",
      "         230     0.4179    0.0571    0.1005       490\n",
      "         231     0.6441    0.1326    0.2199       860\n",
      "         232     0.7143    0.0814    0.1462       307\n",
      "         233     0.5679    0.2613    0.3579      2319\n",
      "         234     0.6714    0.4500    0.5388      4131\n",
      "         235     0.6114    0.4234    0.5003      2003\n",
      "         236     0.6233    0.2725    0.3792      1901\n",
      "         237     0.5984    0.0928    0.1607       819\n",
      "         238     0.5826    0.2211    0.3206       909\n",
      "         239     0.5200    0.0343    0.0644       758\n",
      "         240     0.6541    0.5180    0.5781      2446\n",
      "         241     0.3333    0.0223    0.0418       224\n",
      "         242     0.5861    0.3256    0.4186      3357\n",
      "         243     0.5852    0.2820    0.3806      2021\n",
      "         244     0.0000    0.0000    0.0000        87\n",
      "         245     0.6067    0.2914    0.3937      2790\n",
      "         246     0.6325    0.1018    0.1754       727\n",
      "         247     0.6107    0.3000    0.4024      3043\n",
      "\n",
      "   micro avg     0.6891    0.4156    0.5185    200012\n",
      "   macro avg     0.6064    0.2776    0.3571    200012\n",
      "weighted avg     0.6611    0.4156    0.4914    200012\n",
      " samples avg     0.6752    0.4170    0.4927    200012\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluer les performances pour test_loader combiné avec le train_loader\n",
    "\n",
    "# Combine les loaders\n",
    "# combined_loader = torch.utils.data.DataLoader(\n",
    "#     torch.utils.data.ConcatDataset([train_dataset, test_dataset, val_dataset]),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=False,\n",
    "# )\n",
    "\n",
    "evaluate_performance(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-28 15:29:52,126] A new study created in RDB with name: no-name-e82af600-5504-486b-95cb-12b9e9a1ded9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.2204, Val Loss: 0.1301\n",
      "Epoch 2/20, Train Loss: 0.1264, Val Loss: 0.1105\n",
      "Epoch 3/20, Train Loss: 0.1115, Val Loss: 0.1004\n",
      "Epoch 4/20, Train Loss: 0.1033, Val Loss: 0.0957\n",
      "Epoch 5/20, Train Loss: 0.0985, Val Loss: 0.0933\n",
      "Epoch 6/20, Train Loss: 0.0954, Val Loss: 0.0918\n",
      "Epoch 7/20, Train Loss: 0.0932, Val Loss: 0.0907\n",
      "Epoch 8/20, Train Loss: 0.0916, Val Loss: 0.0895\n",
      "Epoch 9/20, Train Loss: 0.0904, Val Loss: 0.0890\n",
      "Epoch 10/20, Train Loss: 0.0895, Val Loss: 0.0882\n",
      "Epoch 11/20, Train Loss: 0.0887, Val Loss: 0.0879\n",
      "Epoch 12/20, Train Loss: 0.0880, Val Loss: 0.0874\n",
      "Epoch 13/20, Train Loss: 0.0874, Val Loss: 0.0865\n",
      "Epoch 14/20, Train Loss: 0.0869, Val Loss: 0.0865\n",
      "Epoch 15/20, Train Loss: 0.0864, Val Loss: 0.0863\n",
      "Epoch 16/20, Train Loss: 0.0860, Val Loss: 0.0859\n",
      "Epoch 17/20, Train Loss: 0.0857, Val Loss: 0.0855\n",
      "Epoch 18/20, Train Loss: 0.0853, Val Loss: 0.0851\n",
      "Epoch 19/20, Train Loss: 0.0849, Val Loss: 0.0846\n",
      "Epoch 20/20, Train Loss: 0.0847, Val Loss: 0.0847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-28 15:32:46,452] Trial 0 finished with value: 0.0847018271041431 and parameters: {'num_heads': 10, 'embedding_size': 490, 'num_layers': 2, 'dropout': 0.422162743964535, 'learning_rate': 1.2704484884665267e-05}. Best is trial 0 with value: 0.0847018271041431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.1579, Val Loss: 0.1163\n",
      "Epoch 2/20, Train Loss: 0.1112, Val Loss: 0.1004\n",
      "Epoch 3/20, Train Loss: 0.1013, Val Loss: 0.0961\n",
      "Epoch 4/20, Train Loss: 0.0976, Val Loss: 0.0934\n",
      "Epoch 5/20, Train Loss: 0.0954, Val Loss: 0.0915\n",
      "Epoch 6/20, Train Loss: 0.0939, Val Loss: 0.0906\n",
      "Epoch 7/20, Train Loss: 0.0928, Val Loss: 0.0899\n",
      "Epoch 8/20, Train Loss: 0.0921, Val Loss: 0.0892\n",
      "Epoch 9/20, Train Loss: 0.0915, Val Loss: 0.0885\n",
      "Epoch 10/20, Train Loss: 0.0908, Val Loss: 0.0883\n",
      "Epoch 11/20, Train Loss: 0.0904, Val Loss: 0.0874\n",
      "Epoch 12/20, Train Loss: 0.0899, Val Loss: 0.0870\n",
      "Epoch 13/20, Train Loss: 0.0894, Val Loss: 0.0868\n",
      "Epoch 14/20, Train Loss: 0.0891, Val Loss: 0.0863\n",
      "Epoch 15/20, Train Loss: 0.0888, Val Loss: 0.0860\n",
      "Epoch 16/20, Train Loss: 0.0885, Val Loss: 0.0857\n",
      "Epoch 17/20, Train Loss: 0.0883, Val Loss: 0.0856\n",
      "Epoch 18/20, Train Loss: 0.0880, Val Loss: 0.0854\n",
      "Epoch 19/20, Train Loss: 0.0878, Val Loss: 0.0851\n",
      "Epoch 20/20, Train Loss: 0.0876, Val Loss: 0.0850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-28 15:37:21,613] Trial 1 finished with value: 0.08504132875542847 and parameters: {'num_heads': 8, 'embedding_size': 376, 'num_layers': 4, 'dropout': 0.43435812817180464, 'learning_rate': 5.560237877967069e-05}. Best is trial 0 with value: 0.0847018271041431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.1116, Val Loss: 0.0892\n",
      "Epoch 2/20, Train Loss: 0.0888, Val Loss: 0.0854\n",
      "Epoch 3/20, Train Loss: 0.0863, Val Loss: 0.0839\n",
      "Epoch 4/20, Train Loss: 0.0849, Val Loss: 0.0830\n",
      "Epoch 5/20, Train Loss: 0.0840, Val Loss: 0.0825\n",
      "Epoch 6/20, Train Loss: 0.0833, Val Loss: 0.0821\n",
      "Epoch 7/20, Train Loss: 0.0827, Val Loss: 0.0818\n",
      "Epoch 8/20, Train Loss: 0.0821, Val Loss: 0.0810\n",
      "Epoch 9/20, Train Loss: 0.0817, Val Loss: 0.0810\n",
      "Epoch 10/20, Train Loss: 0.0812, Val Loss: 0.0806\n",
      "Epoch 11/20, Train Loss: 0.0808, Val Loss: 0.0804\n",
      "Epoch 12/20, Train Loss: 0.0804, Val Loss: 0.0805\n",
      "Epoch 13/20, Train Loss: 0.0800, Val Loss: 0.0801\n",
      "Epoch 14/20, Train Loss: 0.0797, Val Loss: 0.0800\n",
      "Epoch 15/20, Train Loss: 0.0793, Val Loss: 0.0798\n",
      "Epoch 16/20, Train Loss: 0.0790, Val Loss: 0.0796\n",
      "Epoch 17/20, Train Loss: 0.0786, Val Loss: 0.0792\n",
      "Epoch 18/20, Train Loss: 0.0784, Val Loss: 0.0793\n",
      "Epoch 19/20, Train Loss: 0.0780, Val Loss: 0.0792\n",
      "Epoch 20/20, Train Loss: 0.0777, Val Loss: 0.0792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-28 15:42:36,075] Trial 2 finished with value: 0.07919551542015385 and parameters: {'num_heads': 9, 'embedding_size': 477, 'num_layers': 4, 'dropout': 0.22808229001597416, 'learning_rate': 0.00014564181656591507}. Best is trial 2 with value: 0.07919551542015385.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.1903, Val Loss: 0.1301\n",
      "Epoch 2/20, Train Loss: 0.1268, Val Loss: 0.1153\n",
      "Epoch 3/20, Train Loss: 0.1141, Val Loss: 0.1060\n",
      "Epoch 4/20, Train Loss: 0.1071, Val Loss: 0.1005\n",
      "Epoch 5/20, Train Loss: 0.1022, Val Loss: 0.0970\n",
      "Epoch 6/20, Train Loss: 0.0992, Val Loss: 0.0950\n",
      "Epoch 7/20, Train Loss: 0.0973, Val Loss: 0.0937\n",
      "Epoch 8/20, Train Loss: 0.0961, Val Loss: 0.0928\n",
      "Epoch 9/20, Train Loss: 0.0951, Val Loss: 0.0921\n",
      "Epoch 10/20, Train Loss: 0.0944, Val Loss: 0.0911\n",
      "Epoch 11/20, Train Loss: 0.0936, Val Loss: 0.0907\n",
      "Epoch 12/20, Train Loss: 0.0931, Val Loss: 0.0902\n",
      "Epoch 13/20, Train Loss: 0.0927, Val Loss: 0.0898\n",
      "Epoch 14/20, Train Loss: 0.0922, Val Loss: 0.0895\n",
      "Epoch 15/20, Train Loss: 0.0919, Val Loss: 0.0892\n",
      "Epoch 16/20, Train Loss: 0.0915, Val Loss: 0.0887\n",
      "Epoch 17/20, Train Loss: 0.0912, Val Loss: 0.0884\n",
      "Epoch 18/20, Train Loss: 0.0909, Val Loss: 0.0882\n",
      "Epoch 19/20, Train Loss: 0.0907, Val Loss: 0.0880\n",
      "Epoch 20/20, Train Loss: 0.0905, Val Loss: 0.0878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-28 15:46:43,474] Trial 3 finished with value: 0.08775576769555216 and parameters: {'num_heads': 11, 'embedding_size': 297, 'num_layers': 4, 'dropout': 0.4505155841165638, 'learning_rate': 3.4630127467468254e-05}. Best is trial 2 with value: 0.07919551542015385.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.2758, Val Loss: 0.1528\n",
      "Epoch 2/20, Train Loss: 0.1360, Val Loss: 0.1182\n",
      "Epoch 3/20, Train Loss: 0.1146, Val Loss: 0.1056\n",
      "Epoch 4/20, Train Loss: 0.1048, Val Loss: 0.0985\n",
      "Epoch 5/20, Train Loss: 0.0987, Val Loss: 0.0938\n",
      "Epoch 6/20, Train Loss: 0.0945, Val Loss: 0.0905\n",
      "Epoch 7/20, Train Loss: 0.0915, Val Loss: 0.0882\n",
      "Epoch 8/20, Train Loss: 0.0893, Val Loss: 0.0865\n",
      "Epoch 9/20, Train Loss: 0.0877, Val Loss: 0.0853\n",
      "Epoch 10/20, Train Loss: 0.0865, Val Loss: 0.0844\n",
      "Epoch 11/20, Train Loss: 0.0856, Val Loss: 0.0836\n",
      "Epoch 12/20, Train Loss: 0.0847, Val Loss: 0.0833\n",
      "Epoch 13/20, Train Loss: 0.0841, Val Loss: 0.0826\n",
      "Epoch 14/20, Train Loss: 0.0836, Val Loss: 0.0822\n",
      "Epoch 15/20, Train Loss: 0.0831, Val Loss: 0.0818\n",
      "Epoch 16/20, Train Loss: 0.0827, Val Loss: 0.0816\n",
      "Epoch 17/20, Train Loss: 0.0823, Val Loss: 0.0814\n",
      "Epoch 18/20, Train Loss: 0.0820, Val Loss: 0.0810\n",
      "Epoch 19/20, Train Loss: 0.0817, Val Loss: 0.0808\n",
      "Epoch 20/20, Train Loss: 0.0814, Val Loss: 0.0807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-28 15:49:49,395] Trial 4 finished with value: 0.08069136896900994 and parameters: {'num_heads': 7, 'embedding_size': 259, 'num_layers': 3, 'dropout': 0.14530502458490924, 'learning_rate': 1.1147418052979788e-05}. Best is trial 2 with value: 0.07919551542015385.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.1164, Val Loss: 0.0871\n",
      "Epoch 2/20, Train Loss: 0.0868, Val Loss: 0.0829\n",
      "Epoch 3/20, Train Loss: 0.0833, Val Loss: 0.0814\n",
      "Epoch 4/20, Train Loss: 0.0816, Val Loss: 0.0801\n",
      "Epoch 5/20, Train Loss: 0.0804, Val Loss: 0.0796\n",
      "Epoch 6/20, Train Loss: 0.0795, Val Loss: 0.0790\n",
      "Epoch 7/20, Train Loss: 0.0788, Val Loss: 0.0787\n",
      "Epoch 8/20, Train Loss: 0.0781, Val Loss: 0.0785\n",
      "Epoch 9/20, Train Loss: 0.0776, Val Loss: 0.0781\n",
      "Epoch 10/20, Train Loss: 0.0770, Val Loss: 0.0779\n",
      "Epoch 11/20, Train Loss: 0.0765, Val Loss: 0.0777\n",
      "Epoch 12/20, Train Loss: 0.0760, Val Loss: 0.0775\n",
      "Epoch 13/20, Train Loss: 0.0755, Val Loss: 0.0776\n",
      "Epoch 14/20, Train Loss: 0.0750, Val Loss: 0.0773\n",
      "Epoch 15/20, Train Loss: 0.0746, Val Loss: 0.0773\n",
      "Epoch 16/20, Train Loss: 0.0741, Val Loss: 0.0772\n",
      "Epoch 17/20, Train Loss: 0.0737, Val Loss: 0.0770\n",
      "Epoch 18/20, Train Loss: 0.0732, Val Loss: 0.0771\n",
      "Epoch 19/20, Train Loss: 0.0728, Val Loss: 0.0772\n",
      "Epoch 20/20, Train Loss: 0.0723, Val Loss: 0.0773\n",
      "Early stopping triggered at epoch 20. Best validation loss: 0.0770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-28 15:52:04,848] Trial 5 finished with value: 0.07725657431556167 and parameters: {'num_heads': 15, 'embedding_size': 285, 'num_layers': 2, 'dropout': 0.2618574215322765, 'learning_rate': 0.00015775441092497324}. Best is trial 5 with value: 0.07725657431556167.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.2493, Val Loss: 0.1379\n",
      "Epoch 2/20, Train Loss: 0.1300, Val Loss: 0.1131\n",
      "Epoch 3/20, Train Loss: 0.1129, Val Loss: 0.1028\n",
      "Epoch 4/20, Train Loss: 0.1043, Val Loss: 0.0967\n",
      "Epoch 5/20, Train Loss: 0.0989, Val Loss: 0.0930\n",
      "Epoch 6/20, Train Loss: 0.0952, Val Loss: 0.0906\n",
      "Epoch 7/20, Train Loss: 0.0927, Val Loss: 0.0890\n",
      "Epoch 8/20, Train Loss: 0.0908, Val Loss: 0.0875\n",
      "Epoch 9/20, Train Loss: 0.0895, Val Loss: 0.0867\n",
      "Epoch 10/20, Train Loss: 0.0884, Val Loss: 0.0859\n",
      "Epoch 11/20, Train Loss: 0.0875, Val Loss: 0.0854\n",
      "Epoch 12/20, Train Loss: 0.0869, Val Loss: 0.0848\n",
      "Epoch 13/20, Train Loss: 0.0862, Val Loss: 0.0845\n",
      "Epoch 14/20, Train Loss: 0.0858, Val Loss: 0.0839\n",
      "Epoch 15/20, Train Loss: 0.0853, Val Loss: 0.0836\n",
      "Epoch 16/20, Train Loss: 0.0849, Val Loss: 0.0834\n",
      "Epoch 17/20, Train Loss: 0.0846, Val Loss: 0.0829\n",
      "Epoch 18/20, Train Loss: 0.0842, Val Loss: 0.0829\n",
      "Epoch 19/20, Train Loss: 0.0840, Val Loss: 0.0826\n",
      "Epoch 20/20, Train Loss: 0.0837, Val Loss: 0.0824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-28 15:56:14,683] Trial 6 finished with value: 0.0824053144658641 and parameters: {'num_heads': 7, 'embedding_size': 364, 'num_layers': 3, 'dropout': 0.27089571731073847, 'learning_rate': 1.0633027518029303e-05}. Best is trial 5 with value: 0.07725657431556167.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.1513, Val Loss: 0.0992\n",
      "Epoch 2/20, Train Loss: 0.0960, Val Loss: 0.0890\n",
      "Epoch 3/20, Train Loss: 0.0890, Val Loss: 0.0854\n",
      "Epoch 4/20, Train Loss: 0.0858, Val Loss: 0.0835\n",
      "Epoch 5/20, Train Loss: 0.0838, Val Loss: 0.0822\n",
      "Epoch 6/20, Train Loss: 0.0826, Val Loss: 0.0813\n",
      "Epoch 7/20, Train Loss: 0.0816, Val Loss: 0.0807\n",
      "Epoch 8/20, Train Loss: 0.0809, Val Loss: 0.0804\n",
      "Epoch 9/20, Train Loss: 0.0803, Val Loss: 0.0799\n",
      "Epoch 10/20, Train Loss: 0.0798, Val Loss: 0.0796\n",
      "Epoch 11/20, Train Loss: 0.0793, Val Loss: 0.0794\n",
      "Epoch 12/20, Train Loss: 0.0789, Val Loss: 0.0790\n",
      "Epoch 13/20, Train Loss: 0.0785, Val Loss: 0.0789\n",
      "Epoch 14/20, Train Loss: 0.0781, Val Loss: 0.0788\n",
      "Epoch 15/20, Train Loss: 0.0778, Val Loss: 0.0785\n",
      "Epoch 16/20, Train Loss: 0.0775, Val Loss: 0.0784\n",
      "Epoch 17/20, Train Loss: 0.0771, Val Loss: 0.0783\n",
      "Epoch 18/20, Train Loss: 0.0768, Val Loss: 0.0782\n",
      "Epoch 19/20, Train Loss: 0.0765, Val Loss: 0.0781\n",
      "Epoch 20/20, Train Loss: 0.0762, Val Loss: 0.0781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-28 16:05:21,383] Trial 7 finished with value: 0.07814515282889065 and parameters: {'num_heads': 9, 'embedding_size': 504, 'num_layers': 5, 'dropout': 0.13337936290597635, 'learning_rate': 2.530806289740109e-05}. Best is trial 5 with value: 0.07725657431556167.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.1173, Val Loss: 0.0925\n",
      "Epoch 2/20, Train Loss: 0.0926, Val Loss: 0.0882\n",
      "Epoch 3/20, Train Loss: 0.0899, Val Loss: 0.0868\n",
      "Epoch 4/20, Train Loss: 0.0885, Val Loss: 0.0856\n",
      "Epoch 5/20, Train Loss: 0.0875, Val Loss: 0.0850\n",
      "Epoch 6/20, Train Loss: 0.0868, Val Loss: 0.0843\n",
      "Epoch 7/20, Train Loss: 0.0861, Val Loss: 0.0843\n",
      "Epoch 8/20, Train Loss: 0.0856, Val Loss: 0.0838\n",
      "Epoch 9/20, Train Loss: 0.0850, Val Loss: 0.0833\n",
      "Epoch 10/20, Train Loss: 0.0846, Val Loss: 0.0830\n",
      "Epoch 11/20, Train Loss: 0.0842, Val Loss: 0.0827\n",
      "Epoch 12/20, Train Loss: 0.0838, Val Loss: 0.0823\n",
      "Epoch 13/20, Train Loss: 0.0834, Val Loss: 0.0820\n",
      "Epoch 14/20, Train Loss: 0.0831, Val Loss: 0.0818\n",
      "Epoch 15/20, Train Loss: 0.0827, Val Loss: 0.0818\n",
      "Epoch 16/20, Train Loss: 0.0824, Val Loss: 0.0815\n",
      "Epoch 17/20, Train Loss: 0.0820, Val Loss: 0.0813\n",
      "Epoch 18/20, Train Loss: 0.0816, Val Loss: 0.0808\n",
      "Epoch 19/20, Train Loss: 0.0811, Val Loss: 0.0807\n",
      "Epoch 20/20, Train Loss: 0.0806, Val Loss: 0.0805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-28 16:08:50,784] Trial 8 finished with value: 0.08048821087899825 and parameters: {'num_heads': 9, 'embedding_size': 162, 'num_layers': 4, 'dropout': 0.2536116624411341, 'learning_rate': 0.00037704316085736175}. Best is trial 5 with value: 0.07725657431556167.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.2115, Val Loss: 0.1216\n",
      "Epoch 2/20, Train Loss: 0.1145, Val Loss: 0.1013\n",
      "Epoch 3/20, Train Loss: 0.1009, Val Loss: 0.0934\n",
      "Epoch 4/20, Train Loss: 0.0942, Val Loss: 0.0890\n",
      "Epoch 5/20, Train Loss: 0.0903, Val Loss: 0.0864\n",
      "Epoch 6/20, Train Loss: 0.0877, Val Loss: 0.0848\n",
      "Epoch 7/20, Train Loss: 0.0859, Val Loss: 0.0836\n",
      "Epoch 8/20, Train Loss: 0.0847, Val Loss: 0.0829\n",
      "Epoch 9/20, Train Loss: 0.0838, Val Loss: 0.0822\n",
      "Epoch 10/20, Train Loss: 0.0831, Val Loss: 0.0816\n",
      "Epoch 11/20, Train Loss: 0.0825, Val Loss: 0.0813\n",
      "Epoch 12/20, Train Loss: 0.0820, Val Loss: 0.0808\n",
      "Epoch 13/20, Train Loss: 0.0816, Val Loss: 0.0807\n",
      "Epoch 14/20, Train Loss: 0.0812, Val Loss: 0.0804\n",
      "Epoch 15/20, Train Loss: 0.0808, Val Loss: 0.0802\n",
      "Epoch 16/20, Train Loss: 0.0805, Val Loss: 0.0801\n",
      "Epoch 17/20, Train Loss: 0.0803, Val Loss: 0.0800\n",
      "Epoch 18/20, Train Loss: 0.0800, Val Loss: 0.0797\n",
      "Epoch 19/20, Train Loss: 0.0798, Val Loss: 0.0795\n",
      "Epoch 20/20, Train Loss: 0.0796, Val Loss: 0.0792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-28 16:11:35,877] Trial 9 finished with value: 0.0792430375816582 and parameters: {'num_heads': 10, 'embedding_size': 450, 'num_layers': 2, 'dropout': 0.19859796322956355, 'learning_rate': 1.1900639463338494e-05}. Best is trial 5 with value: 0.07725657431556167.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.1326, Val Loss: 0.1294\n",
      "Epoch 2/20, Train Loss: 0.1283, Val Loss: 0.1291\n",
      "Epoch 3/20, Train Loss: 0.1281, Val Loss: 0.1289\n",
      "Epoch 4/20, Train Loss: 0.1280, Val Loss: 0.1283\n",
      "Epoch 5/20, Train Loss: 0.1279, Val Loss: 0.1282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-01-28 16:12:55,897] Trial 10 failed with parameters: {'num_heads': 16, 'embedding_size': 208, 'num_layers': 6, 'dropout': 0.3519335601598031, 'learning_rate': 0.0008672706583596746} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12345/4229465671.py\", line 34, in objective\n",
      "    model = train_model(model, train_loader, val_loader, criterion, optimizer, EPOCHS, PATIENCE)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_12345/1285253702.py\", line 34, in train_model\n",
      "    loss.backward()\n",
      "  File \"/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/torch/_tensor.py\", line 581, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 347, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-01-28 16:12:55,897] Trial 10 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Create a study object and optimize the objective function\u001b[39;00m\n\u001b[1;32m     49\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(storage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlite:///db.sqlite3\u001b[39m\u001b[38;5;124m\"\u001b[39m,direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Print the best hyperparameters\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters: \u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[20], line 34\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     31\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATIENCE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the validation set\u001b[39;00m\n\u001b[1;32m     37\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[12], line 34\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, epochs, patience)\u001b[0m\n\u001b[1;32m     32\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[1;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(predictions, y_batch)\n\u001b[0;32m---> 34\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     36\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest num_heads first\n",
    "    num_heads = trial.suggest_int(\"num_heads\", 4, 16)\n",
    "    \n",
    "    # Calculate valid embedding_size as multiples of num_heads within [128, 512]\n",
    "    min_embed = ((128 + num_heads - 1) // num_heads) * num_heads\n",
    "    max_embed = (512 // num_heads) * num_heads\n",
    "    embedding_size = trial.suggest_int(\"embedding_size\", min_embed, max_embed, step=num_heads)\n",
    "    \n",
    "    # Suggest remaining hyperparameters\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 6)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "\n",
    "    # Initialize the model with suggested hyperparameters\n",
    "    model = MultiTaskTransformer(\n",
    "        input_size=X_train_final.shape[1],\n",
    "        embedding_size=embedding_size,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=num_layers,\n",
    "        num_labels=y_train.shape[1],\n",
    "        dropout=dropout,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Define the optimizer and criterion\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Train the model\n",
    "    model = train_model(model, train_loader, val_loader, criterion, optimizer, EPOCHS, PATIENCE)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(storage=\"sqlite:///db.sqlite3\",direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "best_params = study.best_params\n",
    "model = MultiTaskTransformer(\n",
    "    input_size=X_train_final.shape[1],\n",
    "    embedding_size=best_params[\"embedding_size\"],\n",
    "    num_heads=best_params[\"num_heads\"],\n",
    "    num_layers=best_params[\"num_layers\"],\n",
    "    num_labels=y_train.shape[1],\n",
    "    dropout=best_params[\"dropout\"],\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params[\"learning_rate\"])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Train the final model\n",
    "model = train_model(model, train_loader, val_loader, criterion, optimizer, EPOCHS, PATIENCE)\n",
    "\n",
    "# Evaluate the final model\n",
    "evaluate_model(model, test_loader, criterion)\n",
    "evaluate_performance(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'num_heads': 15, 'embedding_size': 285, 'num_layers': 2, 'dropout': 0.2618574215322765, 'learning_rate': 0.00015775441092497324}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.1162, Val Loss: 0.0874\n",
      "Epoch 2/20, Train Loss: 0.0868, Val Loss: 0.0830\n",
      "Epoch 3/20, Train Loss: 0.0834, Val Loss: 0.0812\n",
      "Epoch 4/20, Train Loss: 0.0816, Val Loss: 0.0802\n",
      "Epoch 5/20, Train Loss: 0.0804, Val Loss: 0.0797\n",
      "Epoch 6/20, Train Loss: 0.0796, Val Loss: 0.0791\n",
      "Epoch 7/20, Train Loss: 0.0788, Val Loss: 0.0786\n",
      "Epoch 8/20, Train Loss: 0.0782, Val Loss: 0.0785\n",
      "Epoch 9/20, Train Loss: 0.0776, Val Loss: 0.0783\n",
      "Epoch 10/20, Train Loss: 0.0771, Val Loss: 0.0780\n",
      "Epoch 11/20, Train Loss: 0.0765, Val Loss: 0.0778\n",
      "Epoch 12/20, Train Loss: 0.0760, Val Loss: 0.0776\n",
      "Epoch 13/20, Train Loss: 0.0755, Val Loss: 0.0774\n",
      "Epoch 14/20, Train Loss: 0.0751, Val Loss: 0.0773\n",
      "Epoch 15/20, Train Loss: 0.0746, Val Loss: 0.0773\n",
      "Epoch 16/20, Train Loss: 0.0742, Val Loss: 0.0772\n",
      "Epoch 17/20, Train Loss: 0.0737, Val Loss: 0.0773\n",
      "Epoch 18/20, Train Loss: 0.0733, Val Loss: 0.0773\n",
      "Epoch 19/20, Train Loss: 0.0728, Val Loss: 0.0774\n",
      "Early stopping triggered at epoch 19. Best validation loss: 0.0772\n",
      "Test Loss: 0.07769469712842095\n",
      "Accuracy: 0.9718178425464243\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4306    0.1590    0.2322       195\n",
      "           1     0.6173    0.3985    0.4843      1202\n",
      "           2     0.5000    0.0643    0.1139       249\n",
      "           3     0.8000    0.0988    0.1758        81\n",
      "           4     0.7500    0.3134    0.4421        67\n",
      "           5     0.5385    0.1849    0.2753       265\n",
      "           6     0.5469    0.3153    0.4000       111\n",
      "           7     0.5132    0.2308    0.3184       507\n",
      "           8     0.5930    0.3712    0.4566      1056\n",
      "           9     0.6333    0.1638    0.2603       232\n",
      "          10     0.8057    0.6339    0.7096       713\n",
      "          11     0.7483    0.7065    0.7268      2041\n",
      "          12     0.6101    0.4597    0.5243       422\n",
      "          13     0.5806    0.2167    0.3156      1463\n",
      "          14     0.6033    0.2102    0.3118       861\n",
      "          15     0.5393    0.2696    0.3595       382\n",
      "          16     0.5714    0.0519    0.0952        77\n",
      "          17     0.5192    0.1709    0.2571       158\n",
      "          18     0.5102    0.1908    0.2778       131\n",
      "          19     0.5730    0.1903    0.2857       268\n",
      "          20     0.8102    0.5423    0.6498       992\n",
      "          21     0.6857    0.2857    0.4034       168\n",
      "          22     0.6981    0.2403    0.3575       154\n",
      "          23     0.6909    0.3801    0.4904       976\n",
      "          24     0.5647    0.1733    0.2652       906\n",
      "          25     0.0000    0.0000    0.0000        86\n",
      "          26     0.3827    0.0896    0.1452       346\n",
      "          27     0.6670    0.5830    0.6222      3463\n",
      "          28     0.7215    0.5926    0.6507       896\n",
      "          29     0.5909    0.3389    0.4307       422\n",
      "          30     0.5500    0.1000    0.1692       110\n",
      "          31     0.4507    0.1391    0.2126       230\n",
      "          32     0.6585    0.3553    0.4615        76\n",
      "          33     0.5549    0.4579    0.5017       629\n",
      "          34     0.6133    0.4881    0.5436       377\n",
      "          35     0.6247    0.5966    0.6103       756\n",
      "          36     0.5336    0.4184    0.4690       588\n",
      "          37     0.5115    0.3066    0.3834       362\n",
      "          38     0.5155    0.1815    0.2685       551\n",
      "          39     0.5696    0.3904    0.4633      1478\n",
      "          40     0.8000    0.0879    0.1584        91\n",
      "          41     0.5301    0.1872    0.2767       235\n",
      "          42     0.4486    0.1420    0.2157       338\n",
      "          43     0.5000    0.0826    0.1417       109\n",
      "          44     0.5303    0.1724    0.2602       203\n",
      "          45     0.6130    0.3917    0.4780       651\n",
      "          46     0.7561    0.5741    0.6526        54\n",
      "          47     0.7273    0.1702    0.2759        47\n",
      "          48     0.6731    0.5109    0.5809       137\n",
      "          49     0.8636    0.3654    0.5135        52\n",
      "          50     0.0000    0.0000    0.0000        67\n",
      "          51     0.5882    0.1124    0.1887        89\n",
      "          52     1.0000    0.1143    0.2051        35\n",
      "          53     0.8343    0.7123    0.7684       212\n",
      "          54     0.5780    0.3443    0.4315       366\n",
      "          55     0.6588    0.3564    0.4626       390\n",
      "          56     0.2500    0.0169    0.0317        59\n",
      "          57     0.5000    0.0076    0.0150       131\n",
      "          58     0.6250    0.0446    0.0833       112\n",
      "          59     0.5972    0.2575    0.3598       167\n",
      "          60     0.4737    0.0957    0.1593        94\n",
      "          61     0.5375    0.2409    0.3327       357\n",
      "          62     0.8571    0.1429    0.2449        42\n",
      "          63     0.6612    0.3376    0.4469       237\n",
      "          64     0.6481    0.2448    0.3553       143\n",
      "          65     0.5094    0.2000    0.2872       135\n",
      "          66     0.5696    0.2823    0.3775       464\n",
      "          67     0.6809    0.1702    0.2723       188\n",
      "          68     0.7246    0.3759    0.4950       133\n",
      "          69     0.6508    0.1934    0.2982       212\n",
      "          70     0.6349    0.1688    0.2667       474\n",
      "          71     0.0000    0.0000    0.0000        21\n",
      "          72     1.0000    0.0278    0.0541        36\n",
      "          73     0.6566    0.3421    0.4498       190\n",
      "          74     0.3750    0.0561    0.0976       107\n",
      "          75     0.8000    0.1622    0.2697        74\n",
      "          76     0.5385    0.1386    0.2205       101\n",
      "          77     0.5963    0.1806    0.2772       360\n",
      "          78     0.5116    0.1774    0.2635       124\n",
      "          79     0.6364    0.2593    0.3684        81\n",
      "          80     0.5566    0.1453    0.2305       406\n",
      "          81     0.7486    0.5730    0.6491      1398\n",
      "          82     0.6000    0.1330    0.2177       361\n",
      "          83     0.5750    0.3538    0.4381        65\n",
      "          84     0.5109    0.1257    0.2017       374\n",
      "          85     0.2778    0.0538    0.0901        93\n",
      "          86     0.7168    0.5882    0.6462       680\n",
      "          87     0.7131    0.3610    0.4793       964\n",
      "          88     0.8571    0.1333    0.2308        45\n",
      "          89     0.6010    0.2175    0.3194       561\n",
      "          90     0.8169    0.5918    0.6864        98\n",
      "          91     0.7786    0.6095    0.6837       548\n",
      "          92     0.7593    0.6414    0.6954      3477\n",
      "          93     0.5495    0.1984    0.2915       504\n",
      "          94     0.7153    0.3977    0.5112       259\n",
      "          95     0.7824    0.6602    0.7161      5097\n",
      "          96     0.5696    0.4737    0.5172        95\n",
      "          97     0.5412    0.2054    0.2977       224\n",
      "          98     0.9333    0.1538    0.2642        91\n",
      "          99     0.7500    0.4549    0.5663       277\n",
      "         100     0.7404    0.5611    0.6384      2907\n",
      "         101     0.7097    0.2799    0.4015       393\n",
      "         102     0.7146    0.4900    0.5814      1155\n",
      "         103     1.0000    0.2000    0.3333        25\n",
      "         104     1.0000    0.1000    0.1818        20\n",
      "         105     0.7368    0.1250    0.2137       112\n",
      "         106     0.7346    0.4595    0.5653       518\n",
      "         107     0.7262    0.3750    0.4946       488\n",
      "         108     0.5556    0.0340    0.0641       147\n",
      "         109     0.7041    0.2634    0.3833       262\n",
      "         110     0.5800    0.3085    0.4028        94\n",
      "         111     0.6353    0.1556    0.2500       347\n",
      "         112     0.8077    0.2800    0.4158        75\n",
      "         113     0.6633    0.4692    0.5496      1121\n",
      "         114     0.6000    0.0690    0.1237        87\n",
      "         115     0.7895    0.6988    0.7414      5232\n",
      "         116     0.7464    0.5880    0.6578      4459\n",
      "         117     0.5686    0.2397    0.3372       121\n",
      "         118     0.7904    0.7459    0.7675      6341\n",
      "         119     0.7342    0.1933    0.3061       300\n",
      "         120     0.7059    0.4022    0.5125       179\n",
      "         121     0.8000    0.0764    0.1395       157\n",
      "         122     0.7794    0.5871    0.6697      1378\n",
      "         123     0.4730    0.1763    0.2569       397\n",
      "         124     1.0000    0.0141    0.0278        71\n",
      "         125     0.5904    0.4277    0.4961      1176\n",
      "         126     0.7166    0.5554    0.6257       569\n",
      "         127     0.5556    0.0930    0.1594       215\n",
      "         128     0.6444    0.3112    0.4197       588\n",
      "         129     0.5561    0.2067    0.3013       600\n",
      "         130     0.6875    0.2890    0.4069       571\n",
      "         131     0.7500    0.3348    0.4630       224\n",
      "         132     0.6032    0.3731    0.4611       603\n",
      "         133     0.7313    0.3063    0.4317       160\n",
      "         134     0.6452    0.3030    0.4124        66\n",
      "         135     0.6932    0.5817    0.6326       734\n",
      "         136     0.5432    0.3964    0.4583       111\n",
      "         137     0.7532    0.4328    0.5498       134\n",
      "         138     0.6939    0.3203    0.4383       743\n",
      "         139     0.6667    0.2361    0.3487       144\n",
      "         140     0.5510    0.4355    0.4865        62\n",
      "         141     0.7654    0.6930    0.7274      1860\n",
      "         142     0.5591    0.2088    0.3041       249\n",
      "         143     0.7913    0.4333    0.5600       210\n",
      "         144     0.6785    0.3601    0.4705       586\n",
      "         145     0.6667    0.0952    0.1667        21\n",
      "         146     0.6923    0.3396    0.4557        53\n",
      "         147     0.2000    0.0263    0.0465        38\n",
      "         148     0.6260    0.3026    0.4080       271\n",
      "         149     0.7445    0.2818    0.4088       362\n",
      "         150     0.6327    0.3229    0.4276       288\n",
      "         151     0.8511    0.4878    0.6202        82\n",
      "         152     0.1818    0.0282    0.0488        71\n",
      "         153     0.6667    0.3014    0.4151       146\n",
      "         154     0.5714    0.3077    0.4000       364\n",
      "         155     0.7616    0.5684    0.6510      5413\n",
      "         156     0.7778    0.1458    0.2456        48\n",
      "         157     0.7295    0.4286    0.5399       560\n",
      "         158     0.6063    0.4519    0.5178      1755\n",
      "         159     0.6169    0.3942    0.4810       241\n",
      "         160     0.7059    0.1690    0.2727        71\n",
      "         161     0.7387    0.7323    0.7355      2335\n",
      "         162     0.7407    0.4595    0.5672      1038\n",
      "         163     0.6190    0.1070    0.1825       243\n",
      "         164     0.5166    0.1968    0.2850       869\n",
      "         165     0.6600    0.3173    0.4286       104\n",
      "         166     0.7209    0.4460    0.5511       139\n",
      "         167     0.5312    0.2125    0.3036       160\n",
      "         168     0.6857    0.2243    0.3380       214\n",
      "         169     0.8139    0.6967    0.7507       910\n",
      "         170     0.5000    0.0282    0.0533        71\n",
      "         171     0.0000    0.0000    0.0000        35\n",
      "         172     0.7028    0.6243    0.6612      3686\n",
      "         173     0.7250    0.6263    0.6720      1065\n",
      "         174     0.6543    0.5882    0.6195      1496\n",
      "         175     0.6405    0.4929    0.5571      1489\n",
      "         176     0.4881    0.0926    0.1556       443\n",
      "         177     0.7182    0.7174    0.7178      7002\n",
      "         178     0.0000    0.0000    0.0000        80\n",
      "         179     0.6034    0.3500    0.4430       100\n",
      "         180     0.5207    0.1240    0.2003       508\n",
      "         181     0.5432    0.2876    0.3761       153\n",
      "         182     0.6218    0.3499    0.4478       423\n",
      "         183     0.2500    0.0071    0.0139       140\n",
      "         184     0.5854    0.1371    0.2222       175\n",
      "         185     0.7028    0.2865    0.4071       883\n",
      "         186     0.5789    0.3837    0.4615        86\n",
      "         187     0.6667    0.0476    0.0889        84\n",
      "         188     0.5748    0.2291    0.3276       537\n",
      "         189     0.7308    0.6096    0.6647       187\n",
      "         190     0.5946    0.2066    0.3066       426\n",
      "         191     0.3684    0.0745    0.1239        94\n",
      "         192     0.6978    0.4810    0.5695       869\n",
      "         193     0.8417    0.7498    0.7931      4956\n",
      "         194     0.5645    0.1562    0.2448       224\n",
      "         195     0.0000    0.0000    0.0000        76\n",
      "         196     0.6646    0.2104    0.3196       518\n",
      "         197     0.6667    0.2574    0.3714       101\n",
      "         198     0.8687    0.5409    0.6667       159\n",
      "         199     0.6882    0.4667    0.5562       525\n",
      "         200     0.3973    0.0512    0.0908       566\n",
      "         201     0.6640    0.3051    0.4181       272\n",
      "         202     0.4737    0.1094    0.1777       905\n",
      "         203     0.5690    0.0876    0.1519       753\n",
      "         204     0.6113    0.4016    0.4847      1402\n",
      "         205     0.6516    0.3297    0.4379      2138\n",
      "         206     0.6089    0.2324    0.3364      2479\n",
      "         207     0.2778    0.0132    0.0252       379\n",
      "         208     0.5845    0.3707    0.4537      5503\n",
      "         209     0.5504    0.1094    0.1825       649\n",
      "         210     0.5918    0.0366    0.0689       793\n",
      "         211     0.5762    0.3421    0.4293      4364\n",
      "         212     0.6199    0.3303    0.4310      1096\n",
      "         213     0.4375    0.0451    0.0817       466\n",
      "         214     0.5393    0.1098    0.1825      2185\n",
      "         215     0.5263    0.1489    0.2322       470\n",
      "         216     0.5865    0.2194    0.3194      1267\n",
      "         217     0.6276    0.3855    0.4776      2877\n",
      "         218     0.6026    0.1011    0.1731       465\n",
      "         219     0.5608    0.3461    0.4281      1199\n",
      "         220     0.6202    0.1536    0.2462       840\n",
      "         221     0.4825    0.1111    0.1806       495\n",
      "         222     0.5745    0.3007    0.3948      1859\n",
      "         223     0.6382    0.2400    0.3489       904\n",
      "         224     0.6607    0.2618    0.3751      1688\n",
      "         225     0.3559    0.0541    0.0940       388\n",
      "         226     0.5000    0.0426    0.0784       658\n",
      "         227     0.5872    0.1851    0.2815      1291\n",
      "         228     0.6159    0.3825    0.4720      5236\n",
      "         229     0.5915    0.2417    0.3432      2073\n",
      "         230     0.3684    0.0857    0.1391       490\n",
      "         231     0.5015    0.1942    0.2800       860\n",
      "         232     0.8000    0.0521    0.0979       307\n",
      "         233     0.5292    0.3131    0.3934      2319\n",
      "         234     0.6264    0.5127    0.5639      4131\n",
      "         235     0.6027    0.4249    0.4984      2003\n",
      "         236     0.6087    0.3183    0.4180      1901\n",
      "         237     0.6096    0.1697    0.2655       819\n",
      "         238     0.5806    0.2299    0.3294       909\n",
      "         239     0.5072    0.0462    0.0846       758\n",
      "         240     0.6902    0.4591    0.5514      2446\n",
      "         241     0.5556    0.0223    0.0429       224\n",
      "         242     0.6188    0.2654    0.3715      3357\n",
      "         243     0.6006    0.2776    0.3797      2021\n",
      "         244     0.0000    0.0000    0.0000        87\n",
      "         245     0.5805    0.3075    0.4021      2790\n",
      "         246     0.5351    0.1362    0.2171       727\n",
      "         247     0.5826    0.3339    0.4245      3043\n",
      "\n",
      "   micro avg     0.6811    0.4235    0.5223    200012\n",
      "   macro avg     0.6119    0.2828    0.3621    200012\n",
      "weighted avg     0.6539    0.4235    0.4957    200012\n",
      " samples avg     0.6695    0.4244    0.4974    200012\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "best_params = study.best_params\n",
    "model = MultiTaskTransformer(\n",
    "    input_size=X_train_final.shape[1],\n",
    "    embedding_size=best_params[\"embedding_size\"],\n",
    "    num_heads=best_params[\"num_heads\"],\n",
    "    num_layers=best_params[\"num_layers\"],\n",
    "    num_labels=y_train.shape[1],\n",
    "    dropout=best_params[\"dropout\"],\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params[\"learning_rate\"])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Train the final model\n",
    "model = train_model(model, train_loader, val_loader, criterion, optimizer, EPOCHS, PATIENCE)\n",
    "\n",
    "# Evaluate the final model\n",
    "evaluate_model(model, test_loader, criterion)\n",
    "evaluate_performance(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9740548922715414\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5071    0.1880    0.2744       569\n",
      "           1     0.6560    0.3902    0.4893      3983\n",
      "           2     0.7778    0.1034    0.1826       812\n",
      "           3     0.8095    0.0667    0.1232       255\n",
      "           4     0.7545    0.4150    0.5355       200\n",
      "           5     0.6587    0.2345    0.3459       938\n",
      "           6     0.7072    0.3184    0.4391       402\n",
      "           7     0.5363    0.2528    0.3436      1606\n",
      "           8     0.6254    0.3883    0.4791      3237\n",
      "           9     0.6746    0.1597    0.2582       714\n",
      "          10     0.8491    0.7270    0.7833      2392\n",
      "          11     0.7634    0.7369    0.7499      6648\n",
      "          12     0.6857    0.4658    0.5547      1447\n",
      "          13     0.6480    0.2444    0.3549      4505\n",
      "          14     0.7051    0.2321    0.3493      2925\n",
      "          15     0.5903    0.3142    0.4101      1165\n",
      "          16     0.5357    0.0679    0.1205       221\n",
      "          17     0.6393    0.1696    0.2680       460\n",
      "          18     0.6607    0.2308    0.3421       481\n",
      "          19     0.6186    0.2356    0.3412       764\n",
      "          20     0.8124    0.5768    0.6746      2989\n",
      "          21     0.7000    0.3294    0.4480       510\n",
      "          22     0.6154    0.2233    0.3276       430\n",
      "          23     0.7487    0.4154    0.5343      3149\n",
      "          24     0.6327    0.1924    0.2951      2713\n",
      "          25     1.0000    0.0230    0.0449       261\n",
      "          26     0.5962    0.1329    0.2173      1189\n",
      "          27     0.7068    0.6145    0.6574     10840\n",
      "          28     0.7746    0.6496    0.7066      2825\n",
      "          29     0.5857    0.3464    0.4353      1253\n",
      "          30     0.6957    0.1397    0.2327       458\n",
      "          31     0.4745    0.1272    0.2006       731\n",
      "          32     0.7597    0.3590    0.4876       273\n",
      "          33     0.5640    0.4985    0.5292      1936\n",
      "          34     0.5903    0.4719    0.5245      1087\n",
      "          35     0.6516    0.5980    0.6237      2555\n",
      "          36     0.5787    0.4175    0.4851      1928\n",
      "          37     0.6730    0.3460    0.4570      1237\n",
      "          38     0.6645    0.2279    0.3394      1764\n",
      "          39     0.6431    0.4304    0.5157      4949\n",
      "          40     0.7447    0.1159    0.2006       302\n",
      "          41     0.6000    0.1967    0.2962       717\n",
      "          42     0.5512    0.1955    0.2886      1018\n",
      "          43     0.7273    0.0936    0.1658       342\n",
      "          44     0.6864    0.2603    0.3775       580\n",
      "          45     0.6628    0.4058    0.5034      2097\n",
      "          46     0.8507    0.5969    0.7015       191\n",
      "          47     0.7609    0.2000    0.3167       175\n",
      "          48     0.7483    0.5818    0.6546       373\n",
      "          49     0.9211    0.5357    0.6774       196\n",
      "          50     0.0000    0.0000    0.0000       188\n",
      "          51     0.5106    0.0899    0.1529       267\n",
      "          52     0.8261    0.2923    0.4318       130\n",
      "          53     0.8539    0.7223    0.7826       785\n",
      "          54     0.6434    0.3911    0.4865      1107\n",
      "          55     0.7072    0.4204    0.5273      1218\n",
      "          56     0.8571    0.0619    0.1154       194\n",
      "          57     1.0000    0.0102    0.0202       393\n",
      "          58     0.8182    0.1003    0.1787       359\n",
      "          59     0.6770    0.2802    0.3964       546\n",
      "          60     0.6000    0.1358    0.2215       265\n",
      "          61     0.6791    0.2695    0.3859      1217\n",
      "          62     0.7500    0.2124    0.3310       113\n",
      "          63     0.7355    0.3734    0.4953       782\n",
      "          64     0.7355    0.2864    0.4123       398\n",
      "          65     0.6688    0.2323    0.3448       452\n",
      "          66     0.6597    0.3325    0.4421      1510\n",
      "          67     0.7949    0.2202    0.3449       563\n",
      "          68     0.7220    0.4213    0.5321       413\n",
      "          69     0.7083    0.2563    0.3764       597\n",
      "          70     0.6162    0.1623    0.2570      1503\n",
      "          71     0.0000    0.0000    0.0000        80\n",
      "          72     0.7143    0.0431    0.0813       116\n",
      "          73     0.7368    0.4207    0.5356       599\n",
      "          74     0.7500    0.1635    0.2684       312\n",
      "          75     0.7018    0.1794    0.2857       223\n",
      "          76     0.6622    0.1775    0.2800       276\n",
      "          77     0.6282    0.2284    0.3350      1169\n",
      "          78     0.6333    0.2043    0.3089       372\n",
      "          79     0.8160    0.3579    0.4976       285\n",
      "          80     0.5978    0.1306    0.2144      1240\n",
      "          81     0.8105    0.6230    0.7045      4730\n",
      "          82     0.6390    0.1486    0.2412      1036\n",
      "          83     0.5608    0.3360    0.4203       247\n",
      "          84     0.6552    0.1459    0.2387      1172\n",
      "          85     0.6338    0.1304    0.2163       345\n",
      "          86     0.8019    0.6661    0.7277      2243\n",
      "          87     0.7948    0.4377    0.5645      3114\n",
      "          88     0.8333    0.2273    0.3571       132\n",
      "          89     0.7041    0.2506    0.3696      1776\n",
      "          90     0.8254    0.6361    0.7185       327\n",
      "          91     0.8082    0.6305    0.7084      1724\n",
      "          92     0.7677    0.6782    0.7202     10851\n",
      "          93     0.6362    0.2702    0.3793      1599\n",
      "          94     0.7140    0.4227    0.5310       821\n",
      "          95     0.8000    0.6911    0.7415     15828\n",
      "          96     0.6706    0.4763    0.5570       359\n",
      "          97     0.6753    0.2353    0.3490       663\n",
      "          98     0.7045    0.1037    0.1808       299\n",
      "          99     0.7463    0.5319    0.6211       863\n",
      "         100     0.7784    0.6015    0.6786      9132\n",
      "         101     0.7567    0.3507    0.4793      1286\n",
      "         102     0.7407    0.5523    0.6328      3730\n",
      "         103     0.7857    0.2340    0.3607        94\n",
      "         104     0.8333    0.1786    0.2941        56\n",
      "         105     0.6292    0.1538    0.2472       364\n",
      "         106     0.7784    0.5065    0.6137      1692\n",
      "         107     0.7807    0.4119    0.5393      1590\n",
      "         108     0.7647    0.0304    0.0584       428\n",
      "         109     0.7260    0.3221    0.4462       798\n",
      "         110     0.7130    0.3083    0.4304       266\n",
      "         111     0.6082    0.1632    0.2573       913\n",
      "         112     0.7129    0.3303    0.4514       218\n",
      "         113     0.7011    0.5000    0.5837      3594\n",
      "         114     0.7895    0.0952    0.1700       315\n",
      "         115     0.8137    0.7460    0.7784     16047\n",
      "         116     0.7795    0.6242    0.6933     14078\n",
      "         117     0.7000    0.3556    0.4716       374\n",
      "         118     0.8082    0.7682    0.7877     20185\n",
      "         119     0.8144    0.2739    0.4099       785\n",
      "         120     0.7517    0.3562    0.4834       612\n",
      "         121     0.8200    0.0818    0.1488       501\n",
      "         122     0.8345    0.6225    0.7131      4673\n",
      "         123     0.5830    0.2121    0.3110      1391\n",
      "         124     0.3333    0.0042    0.0083       237\n",
      "         125     0.6600    0.4776    0.5542      3645\n",
      "         126     0.8047    0.6405    0.7133      1911\n",
      "         127     0.7353    0.1580    0.2601       633\n",
      "         128     0.7081    0.3442    0.4632      1903\n",
      "         129     0.6506    0.2754    0.3870      1961\n",
      "         130     0.7254    0.3209    0.4450      1795\n",
      "         131     0.7946    0.3809    0.5149       701\n",
      "         132     0.6690    0.3994    0.5002      1908\n",
      "         133     0.8122    0.3078    0.4465       562\n",
      "         134     0.7881    0.4079    0.5376       228\n",
      "         135     0.6994    0.6449    0.6710      2140\n",
      "         136     0.5784    0.3386    0.4271       316\n",
      "         137     0.8169    0.5686    0.6705       408\n",
      "         138     0.7438    0.3606    0.4858      2343\n",
      "         139     0.7845    0.2851    0.4183       498\n",
      "         140     0.7169    0.5459    0.6198       218\n",
      "         141     0.7926    0.7235    0.7565      6148\n",
      "         142     0.7201    0.2542    0.3758       830\n",
      "         143     0.7664    0.4605    0.5753       684\n",
      "         144     0.7953    0.4277    0.5563      1826\n",
      "         145     0.6667    0.0822    0.1463        73\n",
      "         146     0.7722    0.2947    0.4266       207\n",
      "         147     0.6923    0.3782    0.4891       119\n",
      "         148     0.7005    0.3072    0.4271       830\n",
      "         149     0.7857    0.3206    0.4554      1098\n",
      "         150     0.7374    0.3726    0.4951       942\n",
      "         151     0.8581    0.5184    0.6463       245\n",
      "         152     0.6923    0.1011    0.1765       178\n",
      "         153     0.7109    0.3580    0.4762       419\n",
      "         154     0.6512    0.3382    0.4452      1165\n",
      "         155     0.7875    0.5896    0.6744     17116\n",
      "         156     0.4800    0.0727    0.1263       165\n",
      "         157     0.7518    0.4469    0.5605      1844\n",
      "         158     0.6622    0.4736    0.5523      5439\n",
      "         159     0.6977    0.4400    0.5397       834\n",
      "         160     0.8163    0.3347    0.4748       239\n",
      "         161     0.7537    0.7801    0.7667      7125\n",
      "         162     0.7631    0.4851    0.5932      3261\n",
      "         163     0.7033    0.1684    0.2718       760\n",
      "         164     0.5945    0.2285    0.3301      2836\n",
      "         165     0.7656    0.4298    0.5506       342\n",
      "         166     0.7183    0.4368    0.5433       467\n",
      "         167     0.6416    0.2126    0.3194       522\n",
      "         168     0.7520    0.2654    0.3924       697\n",
      "         169     0.8403    0.7597    0.7979      2950\n",
      "         170     0.8333    0.1157    0.2033       216\n",
      "         171     0.0000    0.0000    0.0000        92\n",
      "         172     0.7347    0.6541    0.6920     11921\n",
      "         173     0.7750    0.6430    0.7029      3493\n",
      "         174     0.7122    0.6424    0.6755      4608\n",
      "         175     0.6856    0.5262    0.5954      4504\n",
      "         176     0.5556    0.0875    0.1511      1315\n",
      "         177     0.7429    0.7386    0.7408     21933\n",
      "         178     0.2000    0.0044    0.0087       225\n",
      "         179     0.7343    0.4551    0.5619       334\n",
      "         180     0.6516    0.1894    0.2934      1748\n",
      "         181     0.6940    0.3523    0.4673       528\n",
      "         182     0.6088    0.3175    0.4173      1304\n",
      "         183     0.6818    0.0436    0.0820       344\n",
      "         184     0.7029    0.1628    0.2643       596\n",
      "         185     0.7478    0.3220    0.4502      2624\n",
      "         186     0.6703    0.3815    0.4863       325\n",
      "         187     0.6800    0.0563    0.1040       302\n",
      "         188     0.5964    0.2687    0.3705      1600\n",
      "         189     0.8024    0.6886    0.7412       578\n",
      "         190     0.6067    0.2228    0.3259      1212\n",
      "         191     0.6271    0.1104    0.1878       335\n",
      "         192     0.7641    0.5543    0.6425      3004\n",
      "         193     0.8726    0.7797    0.8235     16363\n",
      "         194     0.5980    0.1614    0.2542       756\n",
      "         195     0.0000    0.0000    0.0000       240\n",
      "         196     0.6252    0.2060    0.3100      1587\n",
      "         197     0.7848    0.3493    0.4834       355\n",
      "         198     0.8460    0.6125    0.7105       529\n",
      "         199     0.7562    0.5312    0.6241      1728\n",
      "         200     0.5473    0.0914    0.1567      1772\n",
      "         201     0.7972    0.3393    0.4760       834\n",
      "         202     0.5542    0.1279    0.2079      2759\n",
      "         203     0.6399    0.1325    0.2195      2227\n",
      "         204     0.6232    0.4087    0.4937      4294\n",
      "         205     0.6877    0.3555    0.4687      6642\n",
      "         206     0.6773    0.2680    0.3840      8139\n",
      "         207     0.5357    0.0239    0.0458      1253\n",
      "         208     0.6366    0.3935    0.4864     17756\n",
      "         209     0.5959    0.1134    0.1906      2028\n",
      "         210     0.6243    0.0461    0.0859      2449\n",
      "         211     0.6086    0.3573    0.4503     13690\n",
      "         212     0.6777    0.3589    0.4693      3480\n",
      "         213     0.5817    0.0576    0.1049      1544\n",
      "         214     0.6063    0.1266    0.2094      6645\n",
      "         215     0.5897    0.1769    0.2722      1430\n",
      "         216     0.6449    0.2590    0.3696      4073\n",
      "         217     0.7029    0.4168    0.5233      9364\n",
      "         218     0.6190    0.0924    0.1608      1548\n",
      "         219     0.6488    0.4260    0.5143      3904\n",
      "         220     0.6227    0.1726    0.2703      2705\n",
      "         221     0.6173    0.1649    0.2603      1643\n",
      "         222     0.6174    0.3140    0.4163      6019\n",
      "         223     0.7301    0.2899    0.4151      3042\n",
      "         224     0.7358    0.2925    0.4185      5618\n",
      "         225     0.5650    0.0707    0.1256      1415\n",
      "         226     0.7279    0.0503    0.0941      2126\n",
      "         227     0.6567    0.2134    0.3221      3927\n",
      "         228     0.6530    0.4050    0.4999     16618\n",
      "         229     0.6580    0.2977    0.4099      6617\n",
      "         230     0.5551    0.1683    0.2583      1794\n",
      "         231     0.5971    0.2544    0.3568      2791\n",
      "         232     0.8082    0.0605    0.1125       976\n",
      "         233     0.6006    0.3614    0.4513      7589\n",
      "         234     0.6610    0.5493    0.6000     13058\n",
      "         235     0.6492    0.4703    0.5455      6440\n",
      "         236     0.6309    0.3432    0.4446      6241\n",
      "         237     0.6113    0.1776    0.2752      2551\n",
      "         238     0.6627    0.2693    0.3830      3093\n",
      "         239     0.6683    0.0631    0.1153      2204\n",
      "         240     0.7285    0.4797    0.5785      7839\n",
      "         241     0.5278    0.0296    0.0560       642\n",
      "         242     0.6972    0.3009    0.4204     10469\n",
      "         243     0.6579    0.2992    0.4113      6454\n",
      "         244     1.0000    0.0054    0.0108       369\n",
      "         245     0.6337    0.3518    0.4524      8636\n",
      "         246     0.5712    0.1415    0.2268      2353\n",
      "         247     0.6459    0.3711    0.4714      9626\n",
      "\n",
      "   micro avg     0.7237    0.4563    0.5597    635868\n",
      "   macro avg     0.6862    0.3189    0.4081    635868\n",
      "weighted avg     0.7054    0.4563    0.5337    635868\n",
      " samples avg     0.7086    0.4548    0.5326    635868\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "evaluate_performance(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.1305, Val Loss: 0.0924\n",
      "Epoch 2/20, Train Loss: 0.0910, Val Loss: 0.0850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-01-28 16:53:59,928] Trial 24 failed with parameters: {'num_heads': 16, 'embedding_size': 176, 'num_layers': 2, 'dropout': 0.3735526181976074, 'learning_rate': 0.0001856201922305069} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_158452/3144562053.py\", line 34, in objective\n",
      "    model = train_model(model, train_loader, val_loader, criterion, optimizer, EPOCHS, PATIENCE)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_158452/1285253702.py\", line 35, in train_model\n",
      "    optimizer.step()\n",
      "  File \"/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 472, in wrapper\n",
      "    with torch.autograd.profiler.record_function(profile_name):\n",
      "  File \"/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/torch/autograd/profiler.py\", line 750, in __exit__\n",
      "    torch.ops.profiler._record_function_exit._RecordFunction(record)\n",
      "  File \"/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/torch/_ops.py\", line 953, in __call__\n",
      "    return self._op(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-01-28 16:53:59,930] Trial 24 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 55\u001b[0m\n\u001b[1;32m     49\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mload_study(\n\u001b[1;32m     50\u001b[0m     study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno-name-e82af600-5504-486b-95cb-12b9e9a1ded9\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Replace with your study name\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     storage\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqlite:///db.sqlite3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Continue optimizing the objective function\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[16], line 34\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     31\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATIENCE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the validation set\u001b[39;00m\n\u001b[1;32m     37\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[14], line 35\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, epochs, patience)\u001b[0m\n\u001b[1;32m     33\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(predictions, y_batch)\n\u001b[1;32m     34\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 35\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     37\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:472\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m cast(Optimizer, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    471\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 472\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# call optimizer step pre hooks\u001b[39;49;00m\n\u001b[1;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_global_optimizer_pre_hooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step_pre_hooks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpre_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/torch/autograd/profiler.py:750\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m--> 750\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_exit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RecordFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    752\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[0;32m~/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/torch/_ops.py:953\u001b[0m, in \u001b[0;36mTorchBindOpOverload.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_as_effectful_op_temporarily():\n\u001b[1;32m    952\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_in_python(args, kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fallthrough_keys())\n\u001b[0;32m--> 953\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest num_heads first\n",
    "    num_heads = trial.suggest_int(\"num_heads\", 4, 16)\n",
    "    \n",
    "    # Calculate valid embedding_size as multiples of num_heads within [128, 512]\n",
    "    min_embed = ((128 + num_heads - 1) // num_heads) * num_heads\n",
    "    max_embed = (512 // num_heads) * num_heads\n",
    "    embedding_size = trial.suggest_int(\"embedding_size\", min_embed, max_embed, step=num_heads)\n",
    "    \n",
    "    # Suggest remaining hyperparameters\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 6)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "\n",
    "    # Initialize the model with suggested hyperparameters\n",
    "    model = MultiTaskTransformer(\n",
    "        input_size=X_train_final.shape[1],\n",
    "        embedding_size=embedding_size,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=num_layers,\n",
    "        num_labels=y_train.shape[1],\n",
    "        dropout=dropout,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Define the optimizer and criterion\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Train the model\n",
    "    model = train_model(model, train_loader, val_loader, criterion, optimizer, EPOCHS, PATIENCE)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            predictions = model(X_batch)\n",
    "            loss = criterion(predictions, y_batch)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    return val_loss\n",
    "\n",
    "# Load the existing study from the SQLite database\n",
    "study = optuna.load_study(\n",
    "    study_name=\"no-name-e82af600-5504-486b-95cb-12b9e9a1ded9\",  # Replace with your study name\n",
    "    storage=\"sqlite:///db.sqlite3\",\n",
    ")\n",
    "\n",
    "# Continue optimizing the objective function\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'num_heads': 15, 'embedding_size': 285, 'num_layers': 2, 'dropout': 0.2618574215322765, 'learning_rate': 0.00015775441092497324}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.1162, Val Loss: 0.0874\n",
      "Epoch 2/20, Train Loss: 0.0869, Val Loss: 0.0832\n",
      "Epoch 3/20, Train Loss: 0.0835, Val Loss: 0.0813\n",
      "Epoch 4/20, Train Loss: 0.0817, Val Loss: 0.0803\n",
      "Epoch 5/20, Train Loss: 0.0805, Val Loss: 0.0796\n",
      "Epoch 6/20, Train Loss: 0.0796, Val Loss: 0.0792\n",
      "Epoch 7/20, Train Loss: 0.0789, Val Loss: 0.0788\n",
      "Epoch 8/20, Train Loss: 0.0782, Val Loss: 0.0786\n",
      "Epoch 9/20, Train Loss: 0.0776, Val Loss: 0.0781\n",
      "Epoch 10/20, Train Loss: 0.0771, Val Loss: 0.0780\n",
      "Epoch 11/20, Train Loss: 0.0766, Val Loss: 0.0777\n",
      "Epoch 12/20, Train Loss: 0.0761, Val Loss: 0.0777\n",
      "Epoch 13/20, Train Loss: 0.0756, Val Loss: 0.0775\n",
      "Epoch 14/20, Train Loss: 0.0751, Val Loss: 0.0774\n",
      "Epoch 15/20, Train Loss: 0.0747, Val Loss: 0.0773\n",
      "Epoch 16/20, Train Loss: 0.0742, Val Loss: 0.0773\n",
      "Epoch 17/20, Train Loss: 0.0737, Val Loss: 0.0771\n",
      "Epoch 18/20, Train Loss: 0.0733, Val Loss: 0.0772\n",
      "Epoch 19/20, Train Loss: 0.0729, Val Loss: 0.0773\n",
      "Epoch 20/20, Train Loss: 0.0724, Val Loss: 0.0773\n",
      "Early stopping triggered at epoch 20. Best validation loss: 0.0771\n",
      "Test Loss: 0.07762834582187944\n",
      "Accuracy: 0.971962793594073\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5217    0.1231    0.1992       195\n",
      "           1     0.6579    0.3311    0.4405      1202\n",
      "           2     0.4375    0.0562    0.0996       249\n",
      "           3     0.6667    0.0494    0.0920        81\n",
      "           4     0.5814    0.3731    0.4545        67\n",
      "           5     0.5086    0.2226    0.3097       265\n",
      "           6     0.5000    0.3333    0.4000       111\n",
      "           7     0.5632    0.1933    0.2878       507\n",
      "           8     0.6552    0.3239    0.4335      1056\n",
      "           9     0.5630    0.3276    0.4142       232\n",
      "          10     0.8277    0.6199    0.7089       713\n",
      "          11     0.7591    0.6869    0.7212      2041\n",
      "          12     0.5988    0.4668    0.5246       422\n",
      "          13     0.5928    0.1900    0.2878      1463\n",
      "          14     0.5796    0.2578    0.3569       861\n",
      "          15     0.5782    0.2225    0.3214       382\n",
      "          16     0.6667    0.0519    0.0964        77\n",
      "          17     0.4886    0.2722    0.3496       158\n",
      "          18     0.4493    0.2366    0.3100       131\n",
      "          19     0.6596    0.1157    0.1968       268\n",
      "          20     0.7734    0.5917    0.6705       992\n",
      "          21     0.6923    0.3214    0.4390       168\n",
      "          22     0.6901    0.3182    0.4356       154\n",
      "          23     0.6522    0.4170    0.5088       976\n",
      "          24     0.4991    0.3091    0.3817       906\n",
      "          25     0.0000    0.0000    0.0000        86\n",
      "          26     0.5085    0.0867    0.1481       346\n",
      "          27     0.6738    0.5845    0.6259      3463\n",
      "          28     0.7314    0.6049    0.6622       896\n",
      "          29     0.5812    0.3223    0.4146       422\n",
      "          30     0.4348    0.0909    0.1504       110\n",
      "          31     0.6842    0.0565    0.1044       230\n",
      "          32     0.6000    0.3553    0.4463        76\n",
      "          33     0.6434    0.2925    0.4022       629\n",
      "          34     0.6419    0.3899    0.4851       377\n",
      "          35     0.6745    0.4934    0.5699       756\n",
      "          36     0.5494    0.3690    0.4415       588\n",
      "          37     0.5534    0.3149    0.4014       362\n",
      "          38     0.5573    0.1325    0.2141       551\n",
      "          39     0.6274    0.2882    0.3950      1478\n",
      "          40     0.4211    0.0879    0.1455        91\n",
      "          41     0.5862    0.1447    0.2321       235\n",
      "          42     0.4681    0.0651    0.1143       338\n",
      "          43     0.5217    0.1101    0.1818       109\n",
      "          44     0.5797    0.1970    0.2941       203\n",
      "          45     0.6494    0.3472    0.4525       651\n",
      "          46     0.9143    0.5926    0.7191        54\n",
      "          47     0.6471    0.2340    0.3438        47\n",
      "          48     0.6429    0.4599    0.5362       137\n",
      "          49     0.8261    0.3654    0.5067        52\n",
      "          50     0.0000    0.0000    0.0000        67\n",
      "          51     0.5769    0.1685    0.2609        89\n",
      "          52     0.7500    0.1714    0.2791        35\n",
      "          53     0.8621    0.7075    0.7772       212\n",
      "          54     0.6992    0.2350    0.3517       366\n",
      "          55     0.6774    0.3769    0.4843       390\n",
      "          56     0.5000    0.0169    0.0328        59\n",
      "          57     0.0000    0.0000    0.0000       131\n",
      "          58     0.6000    0.0268    0.0513       112\n",
      "          59     0.5128    0.3593    0.4225       167\n",
      "          60     0.6154    0.0851    0.1495        94\n",
      "          61     0.4699    0.3501    0.4013       357\n",
      "          62     0.7000    0.1667    0.2692        42\n",
      "          63     0.6294    0.3797    0.4737       237\n",
      "          64     0.6744    0.2028    0.3118       143\n",
      "          65     0.4792    0.1704    0.2514       135\n",
      "          66     0.5697    0.3082    0.4000       464\n",
      "          67     0.6400    0.2553    0.3650       188\n",
      "          68     0.7313    0.3684    0.4900       133\n",
      "          69     0.6557    0.1887    0.2930       212\n",
      "          70     0.7045    0.1308    0.2206       474\n",
      "          71     0.0000    0.0000    0.0000        21\n",
      "          72     0.5000    0.0278    0.0526        36\n",
      "          73     0.5529    0.4947    0.5222       190\n",
      "          74     0.4783    0.1028    0.1692       107\n",
      "          75     0.7692    0.1351    0.2299        74\n",
      "          76     0.5455    0.1782    0.2687       101\n",
      "          77     0.6707    0.1528    0.2489       360\n",
      "          78     0.4688    0.1210    0.1923       124\n",
      "          79     0.5556    0.3086    0.3968        81\n",
      "          80     0.5664    0.1576    0.2466       406\n",
      "          81     0.7809    0.5150    0.6207      1398\n",
      "          82     0.5447    0.1856    0.2769       361\n",
      "          83     0.6250    0.3846    0.4762        65\n",
      "          84     0.5354    0.1417    0.2241       374\n",
      "          85     0.3333    0.0430    0.0762        93\n",
      "          86     0.7015    0.6221    0.6594       680\n",
      "          87     0.6895    0.3963    0.5033       964\n",
      "          88     0.8750    0.1556    0.2642        45\n",
      "          89     0.5513    0.2299    0.3245       561\n",
      "          90     0.8356    0.6224    0.7135        98\n",
      "          91     0.8143    0.5602    0.6638       548\n",
      "          92     0.7814    0.6273    0.6959      3477\n",
      "          93     0.6389    0.1369    0.2255       504\n",
      "          94     0.7070    0.4286    0.5337       259\n",
      "          95     0.8071    0.6229    0.7031      5097\n",
      "          96     0.6721    0.4316    0.5256        95\n",
      "          97     0.5802    0.2098    0.3082       224\n",
      "          98     0.7222    0.1429    0.2385        91\n",
      "          99     0.7443    0.4729    0.5784       277\n",
      "         100     0.7397    0.5611    0.6381      2907\n",
      "         101     0.6686    0.2926    0.4071       393\n",
      "         102     0.7315    0.4528    0.5594      1155\n",
      "         103     0.6667    0.1600    0.2581        25\n",
      "         104     0.6667    0.1000    0.1739        20\n",
      "         105     0.6471    0.0982    0.1705       112\n",
      "         106     0.7551    0.4286    0.5468       518\n",
      "         107     0.6862    0.4078    0.5116       488\n",
      "         108     0.5000    0.0612    0.1091       147\n",
      "         109     0.6837    0.2557    0.3722       262\n",
      "         110     0.6190    0.2766    0.3824        94\n",
      "         111     0.8103    0.1354    0.2321       347\n",
      "         112     0.7222    0.3467    0.4685        75\n",
      "         113     0.7470    0.3898    0.5123      1121\n",
      "         114     0.7000    0.0805    0.1443        87\n",
      "         115     0.8183    0.6653    0.7339      5232\n",
      "         116     0.6902    0.7174    0.7035      4459\n",
      "         117     0.5079    0.2645    0.3478       121\n",
      "         118     0.8238    0.7013    0.7576      6341\n",
      "         119     0.6723    0.2667    0.3819       300\n",
      "         120     0.6889    0.3464    0.4610       179\n",
      "         121     0.6552    0.1210    0.2043       157\n",
      "         122     0.7624    0.6147    0.6806      1378\n",
      "         123     0.5333    0.1612    0.2476       397\n",
      "         124     0.2500    0.0141    0.0267        71\n",
      "         125     0.6824    0.3452    0.4585      1176\n",
      "         126     0.7244    0.5220    0.6067       569\n",
      "         127     0.5000    0.0512    0.0928       215\n",
      "         128     0.6246    0.3282    0.4303       588\n",
      "         129     0.5691    0.1783    0.2716       600\n",
      "         130     0.6667    0.2942    0.4083       571\n",
      "         131     0.7167    0.3839    0.5000       224\n",
      "         132     0.6634    0.3400    0.4496       603\n",
      "         133     0.6753    0.3250    0.4388       160\n",
      "         134     0.6333    0.2879    0.3958        66\n",
      "         135     0.7405    0.5054    0.6008       734\n",
      "         136     0.8250    0.2973    0.4371       111\n",
      "         137     0.8000    0.3881    0.5226       134\n",
      "         138     0.6545    0.3365    0.4444       743\n",
      "         139     0.6071    0.2361    0.3400       144\n",
      "         140     0.5750    0.3710    0.4510        62\n",
      "         141     0.7763    0.6812    0.7257      1860\n",
      "         142     0.5524    0.2329    0.3277       249\n",
      "         143     0.7190    0.4143    0.5257       210\n",
      "         144     0.6646    0.3618    0.4685       586\n",
      "         145     0.6667    0.0952    0.1667        21\n",
      "         146     0.6667    0.3396    0.4500        53\n",
      "         147     0.2500    0.0263    0.0476        38\n",
      "         148     0.6667    0.2878    0.4021       271\n",
      "         149     0.7315    0.3011    0.4266       362\n",
      "         150     0.5714    0.3889    0.4628       288\n",
      "         151     0.8636    0.4634    0.6032        82\n",
      "         152     0.2857    0.0282    0.0513        71\n",
      "         153     0.6970    0.3151    0.4340       146\n",
      "         154     0.5838    0.2967    0.3934       364\n",
      "         155     0.7509    0.5697    0.6479      5413\n",
      "         156     0.6667    0.1667    0.2667        48\n",
      "         157     0.7557    0.3536    0.4818       560\n",
      "         158     0.6733    0.3652    0.4736      1755\n",
      "         159     0.5670    0.4564    0.5057       241\n",
      "         160     0.4828    0.1972    0.2800        71\n",
      "         161     0.7964    0.6450    0.7127      2335\n",
      "         162     0.7484    0.4412    0.5552      1038\n",
      "         163     0.5957    0.1152    0.1931       243\n",
      "         164     0.5680    0.1634    0.2538       869\n",
      "         165     0.6905    0.2788    0.3973       104\n",
      "         166     0.7176    0.4388    0.5446       139\n",
      "         167     0.5063    0.2500    0.3347       160\n",
      "         168     0.6667    0.2804    0.3947       214\n",
      "         169     0.8367    0.6813    0.7511       910\n",
      "         170     0.6000    0.0845    0.1481        71\n",
      "         171     0.0000    0.0000    0.0000        35\n",
      "         172     0.7576    0.5510    0.6380      3686\n",
      "         173     0.7361    0.6075    0.6656      1065\n",
      "         174     0.6798    0.5762    0.6237      1496\n",
      "         175     0.6848    0.4406    0.5362      1489\n",
      "         176     0.5778    0.0587    0.1066       443\n",
      "         177     0.7254    0.7128    0.7191      7002\n",
      "         178     1.0000    0.0125    0.0247        80\n",
      "         179     0.6471    0.3300    0.4371       100\n",
      "         180     0.4912    0.1654    0.2474       508\n",
      "         181     0.5692    0.2418    0.3394       153\n",
      "         182     0.6815    0.2530    0.3690       423\n",
      "         183     0.5000    0.0071    0.0141       140\n",
      "         184     0.6552    0.1086    0.1863       175\n",
      "         185     0.7040    0.2990    0.4197       883\n",
      "         186     0.6140    0.4070    0.4895        86\n",
      "         187     0.7500    0.0357    0.0682        84\n",
      "         188     0.5607    0.2235    0.3196       537\n",
      "         189     0.8148    0.5882    0.6832       187\n",
      "         190     0.7257    0.1925    0.3043       426\n",
      "         191     0.3158    0.0638    0.1062        94\n",
      "         192     0.7310    0.4534    0.5597       869\n",
      "         193     0.8370    0.7512    0.7918      4956\n",
      "         194     0.6000    0.1339    0.2190       224\n",
      "         195     0.0000    0.0000    0.0000        76\n",
      "         196     0.6640    0.1602    0.2582       518\n",
      "         197     0.6486    0.2376    0.3478       101\n",
      "         198     0.8431    0.5409    0.6590       159\n",
      "         199     0.6532    0.5238    0.5814       525\n",
      "         200     0.5556    0.0530    0.0968       566\n",
      "         201     0.6575    0.3529    0.4593       272\n",
      "         202     0.5259    0.0785    0.1365       905\n",
      "         203     0.5468    0.1939    0.2863       753\n",
      "         204     0.6214    0.3887    0.4783      1402\n",
      "         205     0.5850    0.4331    0.4977      2138\n",
      "         206     0.6110    0.2154    0.3185      2479\n",
      "         207     0.2949    0.0607    0.1007       379\n",
      "         208     0.5720    0.4049    0.4741      5503\n",
      "         209     0.5155    0.1541    0.2372       649\n",
      "         210     0.4803    0.0769    0.1326       793\n",
      "         211     0.5948    0.3064    0.4044      4364\n",
      "         212     0.6667    0.2974    0.4114      1096\n",
      "         213     0.5116    0.0472    0.0864       466\n",
      "         214     0.5489    0.1387    0.2214      2185\n",
      "         215     0.5700    0.1213    0.2000       470\n",
      "         216     0.5664    0.2052    0.3013      1267\n",
      "         217     0.6110    0.4018    0.4848      2877\n",
      "         218     0.4098    0.1613    0.2315       465\n",
      "         219     0.5575    0.3436    0.4252      1199\n",
      "         220     0.6439    0.1571    0.2526       840\n",
      "         221     0.4366    0.0626    0.1095       495\n",
      "         222     0.5930    0.2899    0.3895      1859\n",
      "         223     0.5940    0.2622    0.3638       904\n",
      "         224     0.6667    0.2642    0.3784      1688\n",
      "         225     0.3333    0.0490    0.0854       388\n",
      "         226     0.5833    0.0426    0.0793       658\n",
      "         227     0.5761    0.2200    0.3184      1291\n",
      "         228     0.6023    0.4093    0.4874      5236\n",
      "         229     0.6097    0.2132    0.3159      2073\n",
      "         230     0.3902    0.0980    0.1566       490\n",
      "         231     0.6140    0.1535    0.2456       860\n",
      "         232     0.5614    0.1042    0.1758       307\n",
      "         233     0.5601    0.2893    0.3816      2319\n",
      "         234     0.6503    0.4812    0.5531      4131\n",
      "         235     0.6266    0.3929    0.4830      2003\n",
      "         236     0.6033    0.3119    0.4112      1901\n",
      "         237     0.5227    0.1404    0.2214       819\n",
      "         238     0.6130    0.1969    0.2981       909\n",
      "         239     0.5385    0.0462    0.0851       758\n",
      "         240     0.6447    0.5482    0.5926      2446\n",
      "         241     0.3030    0.0446    0.0778       224\n",
      "         242     0.6315    0.2782    0.3863      3357\n",
      "         243     0.6035    0.2870    0.3890      2021\n",
      "         244     0.0000    0.0000    0.0000        87\n",
      "         245     0.6130    0.2731    0.3779      2790\n",
      "         246     0.5946    0.0908    0.1575       727\n",
      "         247     0.6383    0.2639    0.3734      3043\n",
      "\n",
      "   micro avg     0.6904    0.4156    0.5189    200012\n",
      "   macro avg     0.6097    0.2806    0.3621    200012\n",
      "weighted avg     0.6639    0.4156    0.4929    200012\n",
      " samples avg     0.6760    0.4179    0.4945    200012\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/elouarn/Documents/Projet_IA/challenge-data-music-catalogs/music-catalogs-classifier/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters: \", study.best_params)\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "best_params = study.best_params\n",
    "model = MultiTaskTransformer(\n",
    "    input_size=X_train_final.shape[1],\n",
    "    embedding_size=best_params[\"embedding_size\"],\n",
    "    num_heads=best_params[\"num_heads\"],\n",
    "    num_layers=best_params[\"num_layers\"],\n",
    "    num_labels=y_train.shape[1],\n",
    "    dropout=best_params[\"dropout\"],\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params[\"learning_rate\"])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Train the final model\n",
    "model = train_model(model, train_loader, val_loader, criterion, optimizer, EPOCHS, PATIENCE)\n",
    "\n",
    "# Evaluate the final model\n",
    "evaluate_model(model, test_loader, criterion)\n",
    "evaluate_performance(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer et ESNs sauvegardés avec succès !\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarder le modèle et les ESNs dans le dossier models et un sous dossier qui s'incrémente\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Créer un dossier models s'il n'existe pas\n",
    "if not os.path.exists(\"../models\"):\n",
    "    os.makedirs(\"../models\")\n",
    "\n",
    "# Créer un sous-dossier pour les modèles\n",
    "sub_folder = 0\n",
    "while os.path.exists(f\"../models/model_{sub_folder}\"):\n",
    "    sub_folder += 1\n",
    "os.makedirs(f\"../models/model_{sub_folder}\")\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "torch.save(model.state_dict(), f\"../models/model_{sub_folder}/transformer_weights.pth\")\n",
    "\n",
    "with open(f\"../models/model_{sub_folder}/transformer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Sauvegarder les ESNs\n",
    "with open(f\"../models/model_{sub_folder}/esn_Genre.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_Genre, f)\n",
    "\n",
    "with open(f\"../models/model_{sub_folder}/esn_Instrument.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_Instrument, f)\n",
    "\n",
    "with open(f\"../models/model_{sub_folder}/esn_Mood.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_Mood, f)\n",
    "\n",
    "# with open(f\"../models/model_{sub_folder}/esn_Genre_Instrument.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model_Genre_Instrument, f)\n",
    "\n",
    "# with open(f\"../models/model_{sub_folder}/esn_Genre_Mood.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model_Genre_Mood, f)\n",
    "\n",
    "# with open(f\"../models/model_{sub_folder}/esn_Instrument_Mood.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(model_Instrument_Mood, f)\n",
    "\n",
    "\n",
    "print(\"Transformer et ESNs sauvegardés avec succès !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
